%% $Id$
%% tr.tex

\documentclass{article}
\usepackage{epic,eepic}

\oddsidemargin  0.25 in
\evensidemargin  0.25 in
\textwidth   6 in
\topmargin -0.125 in
\textheight 8.5 in
\setlength{\parindent}{0.0in}
\setlength{\parskip}{7pt plus 1pt}


\newcommand{\strength}{\sf}
\newcommand{\ignore}[1]{}
\newcommand{\code}{\small\sf}
\newcommand{\finish}[1]{{\bf #1}\marginpar{\Large $\bullet$}}
\input{abb1.tex}


\title{The Cassowary Lineary Arithmetic Constraint Solving Algorithm:
  Interface and Implementation}

\author{Greg J. Badros \and Alan Borning}

\date{Technical Report UW-CSE-98-DRAFT \\
Department of Computer Science and Engineering \\
University of Washington \\
Box 352350, Seattle, WA  98195-2350  USA \\
{\small \{{\tt gjb},{\tt borning}\}{\tt @cs.washington.edu}} \\
26 June 1998}  

\begin{document}
\maketitle

\begin{abstract}
Linear equality and inequality constraints arise naturally in specifying
many aspects of user interfaces, such as requiring that one window be to
the left of another, requiring that a pane occupy the leftmost 1/3 of a
window, or preferring that an object be contained within a rectangle if
possible.  Current constraint solvers designed for UI applications cannot
efficiently handle simultaneous linear equations and inequalities.  This is
a major limitation.  We describe incremental algorithms based on the dual
simplex and active set methods that can solve such systems of constraints
efficiently.  Both algorithms have been implemented and tested.

This informal technical report is details the latest version of the
Cassowary algorithm.  An earlier version of this technical report also
discussed QOCA and the similarities between Cassowary and that
algorithm~cite{borning-simplex-tr}.
\end{abstract}

% suppress the page number on this first page
%\thispagestyle{empty}
%\setcounter{page}{0}

\bigskip

\section{Introduction}

Linear equality and inequality constraints arise naturally in specifying
many aspects of user interfaces, in particular layout and other geometric
relations.  Inequality constraints, in particular, are needed to express
relationships such as ``inside,'' ``above,'' ``below,'' ``left-of,''
``right-of,'' and ``overlaps.''  For example, if we are designing a
Web document we can express the
requirement that {\code figure1} be to the left of {\code figure2} as the
constraint \mbox{\code figure1.rightSide $\leq$ figure2.leftSide}.

It is important to be able to express preferences as well as requirements
in a constraint system.  One use is to express a desire for stability
when moving parts of an image: things should stay where they were unless
there is some reason for them to move.  A second use is to process
potentially invalid user inputs in a graceful way.  For example, if the
user tries to move a figure outside of its bounding window, it is
reasonable for the figure just to bump up against the side of the window
and stop, rather than given an error.  A
third use is to balance conflicting desires, for example in laying out a
graph.

Efficient techniques are available for solving such constraints if the
constraint network is acyclic.  However, in trying to apply constraint
solvers to real-world problems, we found that the collection of constraints
was often cyclic.  This sometimes arose when the programmer
added redundant constraints --- the cycles {\em could} have been avoided by
careful analysis.  However, this is an added burden on the programmer.
Further, it is clearly contrary to the spirit of the whole enterprise to
require programmers to be constantly on guard to avoid cycles and redundant
constraints; after all, one of the goals in providing constraints is to
allow programmers to state what relations they want to hold in a
declarative fashion, leaving it to the underlying system to enforce these
relations.  For other applications, such as complex layout problems with
conflicting goals,  cycles seem unavoidable.

\subsection{Constraint Hierarchies and Comparators}

Since we want to be able to express preferences as well as requirements in
the constraint system, we need a specification for how conflicting
preferences are to be traded off.  {\em Constraint hierarchies}
\cite{borning-lisp-symbolic-computation-92} provide a general theory for
this.  In a constraint hierarchy each constraint has a strength.  The
{\strength required} strength is special, in that {\strength required}
constraints must be satisfied.  The other strengths all label non-required
constraints.  A constraint of a given strength completely dominates any
constraint with a weaker strength.  In the theory, a {\em comparator} is
used to compare different possible solutions to the constraints and select
among them.

Within this framework a number of variations are possible.  One decision is
whether we only compare solutions on a constraint-by-constraint basis (a
{\em local} comparator), or whether we take some aggregate measure of the
unsatisfied constraints of a given strength (a {\em global} comparator).  A
second choice is whether we are concerned only whether a constraint is
satisfied or not (a {\em predicate} comparator), or whether we also want to
know how nearly satisfied it is (a {\em metric} comparator.  (Constraints
whose domain is a metric space, for example the reals, can have an
associated error function.  The error in satisfying a constraint {\em cn}
is 0 iff the constraint is satisfied, and becomes larger the less nearly
satisfied is the constraint.)

As recognized for the Indigo solver~\cite{borning-uist-96},
for inequality constraints it is important to use a metric rather than a
predicate comparator.  Thus, plausible comparators for use with linear
equality and inequality constraints are {\em locally-error-better}, 
{\em weighted-sum-better}, and {\em least-squares-better}.  
For a given
collection of constraints, Cassowary finds a locally-error-better or a
weighted-sum-better solution
(In constrast, QOCA finds a least-squares-better solution.
The least-squares-better comparator strongly
penalizes outlying values when trading off constraints of the same
strength.  It is particularly suited to tasks such as laying out a tree, a
graph, or a collection of windows, where there are inherently conflicting
preferences~\cite{borning-simplex-tr}.)
Locally-error-better is a more permissive
comparator, in that it admits more solutions to the constraints.  (In fact
any least-squares-better or weighted-sum-better solution is also a
locally-error-better solution \cite{borning-lisp-symbolic-computation-92}.)
It is thus easier to implement algorithms to find a locally-error-better
solution, and in particular to design hybrid algorithms that include
subsolvers for simultaneous equations and inequalities and also subsolvers
for nonnumeric constraints \cite{borning-cp-95}.
Since each of these different comparators is preferable in certain
situations we  give algorithms for each.

\subsection{Adapting the Simplex Algorithm}

Linear programming is concerned with solving the following problem.  Consider
a collection of $n$ real-valued variables $x_1, \ldots, x_n$, each
of which  is constrained to be non-negative: 
$x_i \geq 0$ for $1 \leq i \leq n$.  There are  $m$
linear equality or inequality constraints over the $x_i$, each of the form:

\hspace*{5mm}\mbox{$a_1 x_1 + \ldots + a_n x_n = b$},\\
\hspace*{5mm}\mbox{$a_1 x_1 + \ldots + a_n x_n \leq b$},  or\\
\hspace*{5mm}\mbox{$a_1 x_1 + \ldots + a_n x_n \geq b$}.

Given these constraints, we wish to find values for the $x_i$ that minimizes 
(or maximizes) the value of the {\em objective function} \\
\vspace*{2mm}
\hspace*{5mm}$c + d_1 x_1 + \ldots + d_n x_n$.

This problem has been heavily studied for the past 50 years.  The most
commonly used algorithm for solving it is the simplex algorithm, developed
by Dantzig in the 1940s, and there are now numerous variations of it. 
Unfortunately, however, existing implementations of the simplex
are not really suitable for UI applications.

The principal issue is incrementality.  For interactive graphical
applications, we need to solve similar problems repeatedly, rather than
solving a single problem once, and to achieve interactive response times,
very fast incremental algorithms are needed.  There are two cases.  First, when
moving an object with a mouse or other input device, we typically represent
this interaction as a one-way constraint relating the mouse position to the
desired $x$ and $y$ coordinates of a part of the figure.  For this case we
must re-satisfy the same collection of constraints, differing only in the
mouse location, each time the screen is refreshed.  Second, when editing an
object we may add or remove constraints and other parts, and we would like
to make these operations fast, by reusing as much of the previous solution
as possible.  The performance requirements are considerably more stringent
for the first case than for the second.  
\ignore{In Sections \ref{resolving} and
\ref{quadratic} we describe how to update an existing solution rapidly
given new inputs (e.g.\ a new mouse position), while in Sections
\ref{adding-constraints} and \ref{removing-constraints} we describe how to
add or delete a constraint incrementally.}

Another issue is defining a suitable objective function.  The objective
function in the standard simplex algorithm must be a linear expression; but
the objective functions for the locally-error-better,
weighted-sum-better, and least-squares-better comparators are all
non-linear.  Fortunately techniques have been developed in the operations
research community for handling these cases, which we adopt here.  For the
first two comparators, the objective functions are ``almost linear,''
while the third comparator gives rise to a quadratic
optimization problem.
\ignore{leading to the quasi-linear optimization technique described in Section
\ref{quasi-linear}.  Least-squares-better results in a quadratic
optimization problem, which is solved using the technique described in
Section \ref{quadratic}. } 

Finally, a third issue is accommodating variables that may take on both
positive and negative values, which in general is the case in UI
applications.  (The standard simplex algorithm requires all variables to be
non-negative.)  Here we adopt efficient techniques developed for
implementing constraint logic programming languages.

\subsection{Overview}

We present algorithms for incrementally solving linear equality and
inequality constraints for the three different comparators described
above. In Section~\ref{augmented-simplex-form} we give algorithms for
incrementally adding and deleting required constraints with restricted and
unrestricted variables from a system of constraints kept in {\em augmented
simplex form}, a type of solved form.  In Section \ref{resolving} we give
an algorithm, Cassowary, based on the dual simplex, for incrementally
solving hierarchies of constraints using the locally-error-better or
weighted-sum-better comparators when a constraint is added or an object is
moved.

Cassowary has been implemented in Smalltalk, C++, and Java. It performs
surprisingly well, and a summary of our results is given in Section
\ref{empirical-evaluation}.  The algorithm is straightforward, and a
reimplementation based on this paper is more reasonable, given a
knowledge of the simplex algorithm.  The various implementations with
example applications are available from the authors.

\subsection{Related Work}

There is a long history of using constraints in user interfaces and
interactive systems, beginning with Ivan Sutherland's pioneering Sketchpad
system \cite{sutherland-ifips-63}.  Most of the current systems use one-way
constraints (e.g.\ \cite{hudson-subarctic-manual,myers-chi-96}), or local
propagation algorithms for acyclic collections of multi-way constraints
(e.g.\ \cite{sannella-spe-93,vander-zanden-toplas-96}).
Indigo \cite{borning-uist-96} handles acyclic collections of inequality
constraints, but not cycles (simultaneous equations and inequalities).  UI
systems that handle simultaneous linear equations include \mbox{DETAIL}
\cite{hosobe-cp-96} and Ultraviolet \cite{borning-cp-95}.  A number of
researchers (including the first author) have experimented with a
straightforward use of a simplex package in a UI constraint solver,
but the speed was not satisfactory for interactive use.

Baraff \cite{baraff-siggraph-94} describes a quadratic optimization
algorithm for solving linear constraints that arise in modelling physical
systems.  Finally, much of the work on constraint solvers
has been in the logic programming and constraint logic programming
communities.  Current constraint logic programming languages such as
CLP($\cal R$) \cite{jaffar-toplas-92} include efficient solvers for linear
equalities and inequalities.  (See \cite{marriott-stuckey-book} for a
survey.)  However, these solvers use a refinement model of computation, in
which the values determined for variables are successively refined as the
computation progresses, but there is no notion as such of state and change.
As a result, these systems are not so well suited for building interactive
graphical applications.

Borning, Marriot, Stuckey, and Xiao discuss both an early version of
Cassowary and the related QOCA algorithm.  QOCA uses the same solving
technique as Cassowary, but uses a least-squares-better comparator
during the optimization from basic feasible solved
form~\cite{borning-uist-97,borning-simplex-tr}.  An earlier version of
QOCA is described in references \cite{helm-gi-92} and
\cite{helm-eurographics-92}.  These earlier descriptions, however, do
not give any details of the algorithm, although the incremental deletion
algorithm is described in \cite{huynh-marriott-96}.


\section{Incremental Simplex}
\label{inc-simplex}

\begin{quotation}
  As you see, the subject of linear programming is surrounded by
  notational and terminological thickets.  Both of these thorny defenses
  are lovingly cultivated by a coterie of stern acolytes who have
  devoted themselves to the field.  Actually, the basic ideas of linear
  programming are quite simple.  -- {\em Numerical Recipes}, \cite[page
  424]{press-89}
\end{quotation}

We now describe an incremental version of the simplex algorithm, adapted
to the task at hand.  In the description we use a running example,
illustrated by the diagram in Figure~\ref{fig:pict}.

\begin{figure}[htb]
\begin{center}
\input constraints.eepic
\end{center}
\caption{Simple constrained picture\label{fig:pict}}
\end{figure}

The constraints on the variables in Figure~\ref{fig:pict} are as follows:
$x_m$ is constrained to be the midpoint of the line from $x_l$ to $x_r$,
and $x_l$ is constrained to be at least 10 to the left of $x_r$.  All
variables must lie in the range 0 to 100.  (To keep the presentation
manageable, we deal only with the $x$ coordinates.  Adding analogous
constraints on the $y$ coordinates would be simple but would double the
number of the constraints in our example.)  Since $x_l < x_m < x_r$ in any
solution, we simplify the problem by removing the redundant bounds
constraints.  However,
even with these simplifications the resulting constraints have a cyclic
constraint graph, and cannot be handled by methods such as Indigo.

We can represent this using the constraints
$$\begin{array}{rcl}
2 x_m &=& x_l + x_r \\
x_l + 10 &\leq &x_r \\
x_r &\leq& 100 \\
0 &\leq& x_l
\end{array}$$
Now suppose  we wish to minimize the distance between
$x_m$ and $x_l$ or in other words, minimize $x_m - x_l$.

\subsection{Augmented Simplex Form}
\label{augmented-simplex-form}

An optimization problem is in \emph{augmented simplex form} if constraint
$C$ has the form $C_U \wedge C_S \wedge C_I$ where $C_U$ and $C_S$ are
conjunctions of linear arithmetic equations and $C_I$ is $\bigwedge \{ x
\geq 0 \mid x \in vars(C_S)\}$ and the objective function $f$ is a linear
expression over variables in $C_S$\@.  The simplex algorithm does not itself
handle variables that may take negative values (so-called {\em
unrestricted variables}), and imposes a constraint $x \geq 0$ on all
variables occurring in its equations.  Augmented simplex form allows us to
handle unrestricted variables efficiently and simply; it was developed for
implementing constraint logic programming languages
\cite{marriott-stuckey-book}, and we have adopted it here.  Essentially it
uses {\em two} tableaux rather than one.  All of the unrestricted
variables will be placed in $C_U$, the unrestricted variable tableau.
$C_S$, the simplex tableau, contains only variables constrained to be
non-negative.  The simplex algorithm is used to determine an optimal
solution for the equations in the simplex tableau, ignoring the
unrestricted variable tableau for purposes of optimization.  The equations
in the unrestricted variable tableau are then used to determine values for
its variables.

{\bf Implementation Note.}  In the paper we describe $C_U$ and $C_S$ as
two separate tableaux.  In the implementation, however, it turns out to be
simpler to have just one tableau, since most operations are applied to both
$C_U$ and $C_S$.  Unrestricted and restricted variables are instances of
different classes, and in the code
we differentiate when necessary by sending the {\sf
isRestricted} message to the variable for each row.  See Section
\ref{cassowary-details}.

It is not difficult to write an arbitrary optimization problem over linear
real equations and inequalities into augmented simplex form.  The first
step is to convert inequalities to equations.  Each inequality of the form
$e \leq r$, where $e$ is a linear real expression and $r$ is a number, can be
replaced with $e + s = r \wedge s \geq 0$ where $s$ is a new non-negative
\emph{slack} variable.

For example, the constraints for Figure~\ref{fig:pict} can be written as
\begin{quote}\vspace*{-1ex}
minimize $x_m - x_l$ 
subject to 
$$\begin{array}{rcl}
2 x_m & = & x_l + x_r \\
x_l + 10 + s_1& = &x_r \\
x_r + s_2 &= &100 \\
0 &\leq & x_l, s_1, s_2
\end{array}$$
\end{quote}\vspace{-0.9ex}

We now separate the equalities into $C_U$ and $C_S$\@.
Initially all equations are in $C_S$\@.  We separate out the
unrestricted variables into $C_U$ using Gauss-Jordan elimination.  To do
this, we select an equation in $C_S$ containing an unrestricted variable
$u$ and remove the equation
from $C_S$\@.  We then solve the equation for $u$, yielding
a new equation $u=e$ for some expression $e$.  We then substitute $e$ for
all remaining occurrences of $u$ in $C_S$, $C_U$, and $f$,
and place the equation $u=e$ in
$C_U$\@.  The process is repeated until there are no more unrestricted
variables in $C_S$\@.  In our example the third equation can be used to
substitute $100 - s_2$ for $x_r$ 
obtaining
\begin{quote}\vspace*{-1ex}
minimize $x_m - x_l$ 
$$
\begin{array}{rcl}
x_r &= & 100 - s_2 \\ \hline
2 x_m & = & x_l + 100 - s_2 \\
x_l + 10 + s_1& = & 100 - s_2 \\
0 &\leq & x_l, s_1, s_2
\end{array}
$$
\end{quote}\vspace{-0.9ex}
Next, 
and the first equation can be used to 
substitute $50 + \frac{1}{2}x_l - \frac{1}{2} s_2$ 
for $x_m$, giving 
\begin{quote}\vspace*{-1ex}
minimize $50 - \frac{1}{2}x_l - \frac{1}{2} s_2$ 
subject to 
$$
\begin{array}{rcl}
x_m & = & 50 + \frac{1}{2} x_l - \frac{1}{2} s_2 \\ 
x_r &= & 100 - s_2 \\ \hline
x_l + 10 + s_1& = &100 - s_2 \\
0 &\leq & x_l, s_1, s_2
\end{array}
$$
\end{quote}\vspace{-0.9ex}
The tableau shows $C_U$ above the horizontal line,
and $C_S$ and $C_I$ below the horizontal line.  From now 
on $C_I$ will be omitted --- any variable occurring below the horizontal
line is implicitly constrained to be non-negative.
The simplex method works by taking an optimization problem in ``basic
feasible solved form'' (a type of normal form) and repeatedly applying
matrix operations to obtain new basic feasible solved forms.  Once we
have split the equations into $C_U$ and $C_S$ we can ignore $C_U$ for
purposes of optimization. 

{\bf One Detail.}  The example includes the constraint $x_l \geq 0$.  To
simplify the example, we just make $x_l$ be a restricted variable to
capture this constraint.  In the Cassowary implementation, however, all
variables that may be accessed from outside the solver as well as within it
are unrestricted.  Only error or slack variables are represented as
restricted variables, and these variables occur only within the solver.
See Section \ref{cassowary-details}.  The primary benefit of this is
that the programmer using the solver always uses just the one kind of
variable.  A minor benefit is that only the external, unrestricted
variables actually store their values as a field in the variable object;
the values of restricted variables are just given by the tableau.  A minor
drawback is that the constraint $v \geq 0$ must be represented explicitly.
(For any other constant $c \neq 0$, $v \geq c$ must be represented
explicitly in any event.)

{\bf Another Detail.}  The operations are shown as modifying $C_U$ as well
as $C_S$\@.  It would be possible to modify just $C_S$ and leave $C_U$
unchanged, using $C_U$ only to define values for the variables on the left
hand side of its equations.  This would speed up pivoting byt it would make the incremental updates of the constants in edit
constraints slower; and since this is a much more frequent operation, in
the implementation we do actually modify both $C_U$ and $C_S$\@.

An augmented simplex form optimization problem is in 
\emph{basic feasible solved form} if the equations are of the form 
$$x_0 = c + a_1 x_1 + \ldots + a_n x_n$$ where the variable $x_0$ does not
occur in any other equation or in the objective function.  If the equation
is in $C_S$, $c$ must be non-negative.  However, there is no
such restriction on the constants for the equations in $C_U$\@.  In either
case the variable $x_0$ is said to be \emph{basic} and the other
variables in the equation are \emph{parameters}.  A problem in basic
feasible solved form defines a \emph{basic feasible solution}, which is
obtained by setting each parametric variable to 0 and each basic variable
to the value of the constant in the right-hand side.

For instance, the following constraint
is in basic feasible solved form and is equivalent to the 
problem above.
\begin{quote}\vspace*{-1ex}
minimize $50 - \frac{1}{2} x_l + \frac{1}{2} s_2 $ 
subject to 
$$
\begin{array}{rlrrr} 
x_m & = &50 & + \frac{1}{2} x_l & - \frac{1}{2} s_2 \\
x_r & = &100 &  & - s_2 \\ \hline
s_1 & = &90 & - x_l &  - s_2 
\end{array}
$$
\end{quote}\vspace{-0.9ex}
The basic feasible solution corresponding to this
basic feasible solved form is 
$$\{x_m \mapsto 50, x_r \mapsto 100, s_1 \mapsto 90, x_l \mapsto 0, 
s_2 \mapsto 0\}.$$
The value of the objective function with this solution is 50.

\subsection{Simplex Optimization}
\label{simplex-optimization}

We now describe how to find an optimum solution to a constraint in basic
feasible solved form.  Except for the operations on the additional
unrestricted variable tableau $C_U$, the material presented in this
subsection is simply Phase II of the standard two-phase simplex algorithm.

The simplex algorithm finds the optimum by repeatedly looking for an
``adjacent'' basic feasible solved form whose basic feasible solution
decreases the value of the objective function.  When no such adjacent basic
feasible solved form can be found, the optimum has been found.  The
underlying operation is called {\em pivoting}, and involves exchanging a
basic and a parametric variable using matrix operations.  Thus by
``adjacent'' we mean the new basic feasible solved form can be reached by
performing a single pivot.

In our example, increasing $x_l$ from $0$ will decrease the value of the
objective function.  We must be careful as we cannot increase the value of
$x_l$ indefinitely as this may cause the value of some other
basic non-negative variable to become negative.  We must examine the
equations in $C_S$\@.  The equation $s_1 = 90 - x_l - s_2$ allows $x_l$ to
take at most a value of $90$, as if $x_l$ becomes larger than this, then
$s_1$ would become negative.  The equations above the horizontal line do not
restrict $x_l$, since whatever value $x_l$ takes the unrestricted variables
$x_m$ and $x_r$ can take a value to satisfy the equation.  In general,
we choose the most restrictive equation in $C_S$, and use it to eliminate
$x_l$.  In the case of ties we arbitrarily break the tie. In this example
the most restrictive equation is $s_1 = 90 - x_l - s_2$.  Writing $x_l$ as
the subject we obtain $x_l = 90 - s_1 - s_2$.  We replace $x_l$ everywhere
by $90 - s_1 - s_2$ and obtain
\begin{quote}\vspace*{-1ex}
minimize $5 + \frac{1}{2} s_1 + s_2$ 
subject to 
$$
\begin{array}{rlrrr} 
x_m & = &95 & - \frac{1}{2} s_1 & - s_2 \\
x_r & = &100 &  & - s_2 \\ \hline
x_l & = &90 & - s_1 & - s_2 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

We have just performed a pivot, having moved $s_1$ out of the set of basic
variables and replaced it by $x_l$.

We continue this process.  Increasing the value of $s_1$ will increase
the objective (that we are trying to minimize).  Note that decreasing $s_1$ will also decrease the
objective function value, but as $s_1$ is constrained to be non-negative,
it already takes its minimum value of $0$ in the associated basic feasible
solution.  Hence we are at an optimal solution.  

(If we were to have an unrestricted variable in the objective function, the
optimization would be unbounded.  This is not an issue for our algorithm 
since the objective function in those cases always only contains
restricted variables, i.e.\ variables implicitly constrained to be
non-negative.)

In general, the simplex algorithm applied to $C_S$ 
is described as follows. 
We are given a problem in basic feasible solved form in which 
the variables $x_1, \ldots ,x_n$ are basic and the variables
$y_1, \ldots ,y_m$ are parameters.
\begin{quote}\vspace*{-1ex}
minimize $e + \sum_{j=1}^{m} d_j y_j$
subject to 
$$
\begin{array}{rcr}
        \bigwedge_{i=1}^{n} x_i & = & c_i + \sum_{j=1}^{m} a_{ij} y_j ~\wedge\\
                  \multicolumn{3}{c}{\bigwedge_{i=1}^{n} x_i \ge 0 \wedge
                  \bigwedge_{j=1}^{m} y_j \ge 0. }
            \end{array}
$$
\end{quote}\vspace{-0.9ex}
Select an entry variable $y_J$ such that $d_J < 0$.  (An entry variable is
one that will enter the basis, i.e.\ it is currently parametric and we want
to make it basic.)
Pivoting on such a variable can only decrease the value of the objective
function.
If no such variable exists, the optimum has been reached.
Now determine the exit variable $x_I$\@.  We must choose this variable so that
it maintains  basic feasible solved form by ensuring 
that the new $c_i$'s are still positive after pivoting. 
That is, we must choose an $x_I$ 
so that $- c_I/ a_{IJ}$ is a minimum element
of the set
$$
\{ -c_i/a_{iJ} \mid a_{iJ} < 0 \mbox{ and } 1 \le i \le n \}.
$$
If there were no $i$ for which $a_{iJ} < 0$ then we could stop since the
optimization problem would be unbounded, and so would not have
a minimum.  This is because we could choose $y_J$ to take an arbitrarily
large value, and so make the objective function arbitrarily small.
However, this is not an issue in our context since our
optimization problems will always have a lower bound of 0.

We proceed to choose $x_I$, and pivot $x_I$ out and replace it with $y_J$
to obtain the new basic feasible solution.
We continue this process until an optimum is reached.
The algorithm is specified in Figure~\ref{fig:simplex-opt},
and takes as inputs the simplex tableau $C_S$ and the objective function $f$.


\begin{figure}[tb]
\begin{center}
\fbox{
\begin{minipage}{10cm}
\begin{tabbing}
xx \= xx \= xx \= xx \= \kill
%Boolean
\textsf{simplex\_opt}($C_S$,$f$) \\
\> \textbf{repeat} \\
\> \> \% Choose variable $y_J$ to become basic\\
\> \> \textbf{if} for each $j \in \{ 1, \ldots ,m \}$ $d_j \ge 0$ \textbf{then} \\
\> \> \> \textbf{return} \% an optimal solution has been found\\
\> \> \textbf{endif} \\
\> \> \textbf{choose} $J \in \{ 1, \ldots ,m \}$ such that $d_J < 0$\\
\> \> \% Choose variable $x_I$ to become non-basic\\
\> \> \textbf{choose} $I \in \{ 1, \ldots ,n \}$ such that\\
\> \> \> $-c_I / a_{IJ} =
    \min_{i \in \{1,\ldots, n\}} \{ -c_i / a_{iJ} \mid a_{iJ} < 0 \}$\\
\> \> $e$ := $( x_I - c_I - \sum_{j=1, j\ne J}^m a_{Ij} y_j)/ a_{IJ}$\\
\> \> $C_S \left[ I \right]$ := $(Y_J = e)$\\
\> \> replace $Y_J$ by $e$ in $f$\\
\> \> \textbf{for} each $i \in \{1, \ldots ,n\}$\\
\> \> \> \textbf{if} $i \ne I$ \textbf{then}
       replace $Y_J$ by $e$ in $C_S \left[ I \right]$ \textbf{endif}\\
\> \> \textbf{endfor}\\
\> \textbf{endrepeat}
\end{tabbing}
\end{minipage}
}
\end{center}
\caption{Simplex optimization\label{fig:simplex-opt}}
\end{figure}


\ignore{ %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{quote}\vspace*{-1ex}
\textbf{Simplex algorithm} 
\\ {\sc input:} An  optimization problem $(C_S \wedge C_P,f)$ 
in basic feasible solved form.
\\ {\sc output:} Either  $false$ indicating that $(C_S \wedge C_P,f)$
does not have an optimal solution or else an optimal solution
to  $(C_S \wedge C_P,f)$.
\\ {\sc method:} 
Call , and let $\tuple{ F, C', f'}$ be the result.
If $F$ is $false$, output $false$, otherwise output the basic feasible
solution corresponding to $(C',f')$.
\end{quote}\vspace{-0.9ex}
} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Incrementality: Adding a Constraint}
\label{adding-constraints}

We now describe how to add the equation for a new constraint incrementally.
This technique is also used in our implementations to find an initial basic
feasible solved form for the original simplex problem, by starting from an
empty constraint set and adding the constraints one at a time.

As an example, suppose we wish to ensure that the midpoint
sits in the centre of the screen.  
This is represented by the constraint
$x_m = 50$.  If we substitute for each of the basic variables
(only $x_m$) in this constraint we obtain the equation
$45 - \frac{1}{2} s_1 - s_2 = 0$.  
In order to add this constraint
straightforwardly to the tableau we create a new 
non-negative variable $a$ called an \emph{artificial variable}.
(This is simply an incremental version of the operation used in
Phase I of the two-phase simplex algorithm.)
We let $a = 45 - \frac{1}{2} s_1 - s_2$ be added to the tableau
(clearly this gives a tableau in basic feasible solved form)
and then minimize the value of $a$.
If $a$ takes the value $0$
then we have obtained a solution to the problem
with the added constraint, and
we can then eliminate the artificial variable altogether since it is 
a parameter (and hence takes the value 0).  This is the case for our example;
the resulting tableau is
\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x_m & = &50   \\
x_r & = &100 & - s_2 \\ \hline
x_l & = &0 & + s_2 \\
s_1 & = &90 & -2 s_2
\end{array}
$$
\end{quote}\vspace{-0.9ex}

In general, to add a new required constraint to the tableau we first
convert it to an augmented simplex form equation by adding slack variables
if it is an inequality.  Next, we use the current tableau to substitute out
all the basic variables.  This gives an equation $e = c$ where $e$ is a
linear expression.  If $c$ is negative, we multiply both sides by $-1$ so
that the constant becomes non-negative.  If $e$ contains an unrestricted
variable we use it to substitute for that variable and add the equation to
the tableau above the line (i.e.\ to $C_U$).  Otherwise we create a
restricted artificial variable $a$ and add the equation $a = c - e$ to the
tableau below the line (i.e.\ to $C_S$), and minimize $c - e$. If the
resulting minimum is not zero then the constraints are unsatisfiable.
Otherwise $a$ is either parametric or basic.  If $a$ is parametric, the
column for it can be simply removed from the tableau.  If it is basic, the
row must have constant 0 (since we were able to achieve a value of 0 for
our objective function, which is equal to $a$).  If the row is just $a =
0$, it can be removed.  Otherwise, $a = 0 + b x + e$ where $b \neq 0$.  We
can then pivot $x$ into the basis using this row and remove the column for
$a$.

{\bf Implementation Note.}  In some cases we can add an equation to the
tableau without using an artificial variable, even when the equation
contains only restricted variables, and for efficiency should do so when
it is easy to detect that this can be done.  See Section
\ref{cassowary-adding-constraints}.

\subsection{Incrementality: Removing a Constraint}
\label{removing-constraints}

We also want a method for incrementally removing a constraint from the
tableaux.  After a series of pivots have been performed, the information
represented by the constraint may not be contained in a single row, so we
need a way to identify the constraint's influence in the tableaux.  To do
this, we use a ``marker'' variable that is originally present only in the
equation representing the constraint.  We can then identify the
constraint's influence in the tableaux by looking for occurrences of that
marker variable.  For inequality constraints, the slack variable $s$ added
to make it an equality serves as the marker, since $s$ will originally
occur only in that equation.  The equation representing a nonrequired
equality constraint will have an error variable that can serve as a marker
--- see Section \ref{non-requireds}.  For required equality constraints, we
add a ``dummy'' restricted variable to the original equation to serve as a
marker, which we never allow to enter the basis (so that it always has
value 0).  In our running example, then, to allow the constraint $2 x_m =
x_l + x_r$ to be deleted incrementally we would add a dummy variable $d_3$,
resulting in $2 x_m = x_l + x_r + d_3$.  The simplex optimization routine
checks for these dummy variables in choosing an entry variable, and does
not allow one to be selected.  (We did not include this variable in the
tableaux presented earlier to keep things simpler.)

(Note: these dummy variables must be restricted, not unrestricted, since we
might need to have some of them in the equations for restricted basic
variables.)

Consider removing the constraint that $x_l$ is 10 to the left of $x_r$.
The slack variable $s_1$, which we added to the inequality to make it
an equation, records exactly how this equation has been used to modify the
tableau.  We can remove the inequality by pivoting the tableau until 
$s_1$ is basic and then simply drop the row in which it is basic.

In the tableau above $s_1$ is already basic, and so removing it simply means
dropping the row in which it is basic, obtaining
\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x_m & = &50   \\
x_r & = &100 & - s_2 \\ \hline
x_l & = &0 & + s_2 \\

\end{array}
$$
\end{quote}\vspace{-0.9ex}

If we wanted to remove this constraint
from the tableau before adding $x_m = 50$ (i.e. the final tableau given in
Section \ref{simplex-optimization}), $s_1$ is a parameter.
We make $s_1$ basic by treating it as an entry variable and
(as usual) determining the most restrictive equation and using that 
to pivot $s_1$ into the basis, and then remove the row.

There is such a restrictive equation in this example.  However, if no
equation restricts the size of the marker variable, that is, its
coefficients are all non-negative, then either the marker variable has a
positive coefficient in all equations, or it only occurs in equations for
unrestricted variables.  If it does occur in an equation for a restricted
variable, pick the equation that gives the smallest ratio.  (The row with
the marker variable will become infeasible, but all the other rows will
still be feasible, and we will be dropping the row with the marker
variable.  In effect we are removing the non-negativity restriction on the
marker variable.)  Finally, if it only occurs in equations for unrestricted
variables, we can choose any equation in which it occurs.

In the case above, the row $x_l  = 90  - s_1  - s_2$
is the most constraining equation.
Pivoting to let $s_1$ enter the
basis, and then removing the row in which it is basic, we obtain
\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x_m & = &50 & + \frac{1}{2} x_l & - \frac{1}{2} s_2 \\
x_r & = &100 &  & - s_2 \\ \hline
\end{array}
$$
\end{quote}\vspace{-0.9ex}

In the preceding example the marker variable had a negative coefficient.
Here is an example in which it just has positive coefficients.  
% (This is an example just for the tech report.)
The original constraints are:

\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x & \geq & 10   \\
x & \geq & 20   \\
x & \geq & 30 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

In basic feasible solved form this is:

\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x & = & 30 & + d_3   \\ \hline
s_1 & = & 20 & + d_3 \\
s_2 & = & 10 & + d_3 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

where $s_1$, $s_2$, and $d_3$ are the marker variables for 
$x \geq 10$, $x \geq 20$, and $x \geq 30$ respectively.

Suppose we want to remove the $x \geq 30$ constraint.  We need to pivot to
make $d_3$ basic.  The equation that gives the smallest ratio is 
$s_2  = 10  + d_3$, so the entry variable is $d_3$ and the exit variable is
$s_2$, giving:

\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x & = & 20 & + s_2   \\ \hline
s_1 & = & 10 & + s_2 \\
d_3 & = & -10 & + s_2 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

This is now infeasible, but we drop the row with $d_3$ giving 

\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x & = & 20 & + s_2   \\ \hline
s_1 & = & 10 & + s_2 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

which is of course feasible.

As another fine point, note that there is no problem with redundant
constraints.  Consider:

\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x & \geq & 10   \\
x & \geq & 10 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

When converted to basic feasible solved form, each $x \geq 10$ constraint
gets a separate slack variable, which is used as the marker variable for
that constraint.


\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x & = & 10 & + s_1   \\ \hline
s_2 & = & 0 & + s_1 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

To delete the second $x \geq 10$ constraint we would simply drop
the $s_2 = 0 + s_1$ row.  To delete the first  $x \geq 10$ constraint we
would pivot, making $s_1$ basic and $s_2$ parametric:

\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr}
x & = & 10 & + s_2   \\ \hline
s_1 & = & 0 & + s_2
\end{array}
$$
\end{quote}\vspace{-0.9ex}

and then drop the  $s_1 = 0 + s_2$ row.

A consequence of this is that if there are two redundant constraints, both
of them must be removed to eliminate their effect.  (This seems to be a
more desirable behaviour for the solver than removing redundant constraints
automatically, although if the latter were desired the solver could be
modified to do this.)  Another consequence is that when adding a new
constraint, we would never decide that it was redundant and not add it to
the tableau.  (If there were no dummy marker variables, we {\em would} do this
for redundant required equality constraints.)

\subsection{Handling Non-Required Constraints}
\label{non-requireds}

Suppose the user wishes to edit $x_m$ in the diagram and have $x_l$ and
$x_r$ weakly stay where they are.  This adds the non-required constraints
$x_m$ {\em edit}, $x_l$ {\em stay}, and $x_r$ {\em stay}.  Suppose further
that we are trying to move $x_m$ to position 50, and that $x_l$ and $x_r$ are
currently at 30 and 60 respectively.  We are thus imposing the constraints
{\strength strong} $x_m = 50$, {\strength weak} $x_l = 30$, and 
{\strength weak} $x_r = 60$.
There are two possible translations of these non-required constraints
to an objective function, depending on the comparator used.

For locally-error-better or weighted-sum-better, we can
simply add the errors of the each constraint to form an objective function.
Consider the constraint $x_m = 50$.  We define the error as $|x_m-50|$\@.  We
need to combine the errors for each non-required constraint with a weight
so we obtain the objective function 
$$s |x_m - 50| + w |x_l - 30| + w |x_r - 60|$$
where $s$ and $w$ are weights so that the strong constraint is always
strictly more important than solving any combination of weak constraints,
so that we find a locally-error-better or weighted-sum-better solution.
For the least-squares-better comparator the objective function is 
$$s (x_m - 50)^2 + w (x_l - 30)^2 + w (x_r - 60)^2.$$  
In the presentation, we will use $s = 1000$ and $w = 1$.

Cassowary actually uses symbolic weights and a lexicographic ordering,
which ensures that strong constraints are always satisfied in preference to
weak ones (see Section \ref{cassowary-details}).

Unfortunately neither of these objective functions is linear and hence the
simplex method is not applicable directly.  We now show how we can 
solve the problem using optimization algorithms
based on the two alternate objective functions: \emph{quasi-linear
optimization} and \emph{quadratic optimization}.

\section{Cassowary's Quasi-linear Optimization}
\label{quasi-linear}

Cassowary finds either locally-error-better or weighted-sum-better
solutions.  Since every weighted-sum-better solution is also a
locally-error-better solution \cite{borning-lisp-symbolic-computation-92},
the weighted-sum part of the optimization comes automatically from the
manner in which the objective function is constructed.

Both the edit and the stay constraints will be represented as equations of
the form  
$$v = \alpha + \delta_v^{+} - \delta_v^{-}$$
where $\delta_v^{+}$ and $\delta_v^{-}$ are non-negative variables
representing the
deviation of $v$ from the desired value $\alpha$.  If the constraint is
satisfied both $\delta_v^{+}$ and $\delta_v^{-}$ will be 0.  
Otherwise $\delta_v^{+}$ will be
positive and $\delta_v^{-}$ will be 0 if $v$ is too big, 
or vice versa if $v$ is
too small.  
Since we want $\delta_v^{+}$ and $\delta_v^{-}$ to be 0 if
possible, we make them part of the objective function, with larger
coefficients for the error variables for stronger constraints.
(We need to use the pair of variables to satisfy simplex's
non-negativity restriction, since these variables  $\delta_v^{+}$ and
$\delta_v^{-}$ will be part of the objective function.)  

Translating the constraints
{\strength strong} $x_m = 50$, 
{\strength weak} $x_l = 30$,
and {\strength weak} $x_r = 60$
which arise from the 
edit and stay constraints we obtain:
$$\begin{array}{rcl}
x_m & = &50 + \delta_{x_m}^+ -  \delta_{x_m}^- \\
x_l & = &30 + \delta_{x_l}^+ -  \delta_{x_l}^- \\
x_r &= &60 + \delta_{x_r}^+ -  \delta_{x_r}^- \\
0 &\leq& \delta_{x_m}^+, \delta_{x_m}^-, \delta_{x_l}^+, \delta_{x_l}^-, 
        \delta_{x_r}^+, \delta_{x_r}^-
\end{array}$$
The objective function to satisfy the
non-required constraints 
can now be restated as

\begin{quote}\vspace*{-1ex}
minimize $1000 \delta_{x_m}^+ + 1000  \delta_{x_m}^- + \delta_{x_l}^+ +  
        \delta_{x_l}^- + \delta_{x_r}^+ +  \delta_{x_r}^-$.
\end{quote}\vspace{-0.9ex}

An optimal solution of this problem can be found using the simplex algorithm,
and results in a tableau
\begin{trivlist}\item
minimize $10 + 1002 \delta_{x_m}^+ + 998 \delta_{x_m}^-  + 2
\delta_{x_l}^- + 2\delta_{x_r}^- $ 
subject to 
$$
\begin{array}{rlrrrrrr} 
x_m & = &50 &  + \delta_{x_m}^+ & - \delta_{x_m}^- \\
x_r & = &70 & + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                - \delta_{x_l}^+ & + \delta_{x_l}^- \\ \hline
x_l & = & 30  & & & + \delta_{x_l}^+ & - \delta_{x_l}^- \\
s_1 & = &30 &  + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                -2 \delta_{x_l}^+ & +2 \delta_{x_l}^- \\
s_2 & = &30 &   - 2 \delta_{x_m}^+ &+2\delta_{x_m}^- &
                + \delta_{x_l}^+ & - \delta_{x_l}^- \\
\delta_{x_r}^+ & = & 10 & + 2 \delta_{x_m}^+  & - 2\delta_{x_m}^- &
        - \delta_{x_l}^+ & + \delta_{x_l}^- & +\delta_{x_r}^-
\end{array}
$$
\end{trivlist}
This corresponds to the solution
$\{x_m \mapsto 50, x_l \mapsto 30, x_r \mapsto 70\}$
illustrated in Figure~\ref{fig:pict}.
Notice that the weak stay constraint on $x_r$ is not satisfied (read
directly from last line of the above tableau).

\subsection{Incrementality: Resolving the Optimization Problem}
\label{resolving}

Now suppose the user moves the mouse (which
is editing $x_m$) to $x=60$.
We wish to solve a new problem, with
constraints {\strength strong} $x_m = 60$, and
{\strength weak} $x_l = 30$ and {\strength weak} $x_r = 70$
(so that $x_l$ and $x_r$ should stay where they are if possible).

% WAS: 
% {\strength weak} $x_l = 10$ and {\strength weak} $x_r = 90$
% ... hope this is right

There are two steps.  First, we modify the tableau to reflect the new
constraints we wish to solve.  Second, we resolve the optimization problem
for this modified tableau.

Let us first examine how to modify the tableau to reflect the new values of
the stay constraints.  This will not require reoptimizing the tableau,
since we know that the new stay constraints are satisfied exactly.
Suppose the previous stay value for variable $v$ was $\alpha$, and in the
current solution $v$ takes value $\beta$.  The current tableau contains the
information that 
$$v = \alpha + \delta_v^+ - \delta_v^-$$
and we need to
modify this so that instead 
$$v = \beta + \delta_v^+ - \delta_v^-$$
There
are two cases to consider: (a) both $\delta_v^+$ and $\delta_v^-$ are
parameters, or (b) one of them is basic.

In case (a) $v$ must take the value $\alpha$
in the current solution since both $\delta_v^+$ and 
$\delta_v^-$ take the value $0$ and 
$$v = \alpha + \delta_v^+ - \delta_v^-$$
Hence $\beta = \alpha$ and no changes need to be made.

In case (b) assume without loss of generality that $\delta_v^+$ is
basic.  In the original equation representing the stay constraint, the
coefficient for $\delta_v^+$ is the negative of the coefficient for
$\delta_v^-$.  Since these variables occur in no other constraints, this
relation between the coefficients will continue to hold as we perform
pivots.  In other words, $\delta_v^+$ and $\delta_v^-$ come in pairs: any
equation that contains $\delta_v^+$ will also contain $\delta_v^-$ and vice
versa.  Since $\delta_v^+$ is assumed to be basic, it occurs exactly once
in an equation with constant $c$, and further this equation also contains
the only occurrence of $\delta_v^-$.  In the current solution 
$$\{v \mapsto \beta, \delta_v^+ \mapsto c, \delta_v^- \mapsto 0\}$$
and since the
equation 
$$v = \alpha + \delta_v^+ - \delta_v^-$$
holds, $\beta = \alpha + c$.  To replace the equation 
$$v = \alpha + \delta_v^+ - \delta_v^-$$
by  
$$v = \beta + \delta_v^+ - \delta_v^-$$
we simply need to replace the constant $c$
in the row for $\delta_v^+$ by $0$.  Since there are no other 
occurrences of $\delta_v^+$
and $\delta_v^-$ we have replaced the old equation with the new.

For our example, to update the tableau for the new values for the stay
constraints on $x_l$ and $x_r$ we simply set the constant of last equation
(the equation for $\delta_{x_r}^+$) to 0.

Now let us consider the edit constraints.  Suppose the previous edit value
for $v$ was $\alpha$, and the new edit value for $v$ is $\beta$.  The
current tableau contains the information that 
$$v = \alpha + \delta_v^+ - \delta_v^-$$
and again we need to modify this so that instead 
$$v = \beta + \delta_v^+ - \delta_v^-$$
To do so we must replace every occurrence of 
$$\delta_v^+ - \delta_v^-$$
by 
$$\beta - \alpha + \delta_v^+ - \delta_v^-$$
taking proper account of the coefficients of $\delta_v^+$ and $\delta_v^-$.
(Again, remember that $\delta_v^+$ and $\delta_v^-$ come in pairs.)

If either of $\delta_v^+$ and $\delta_v^-$ is basic, this simply involves
appropriately modifying the equation in which they are basic.  Otherwise, if
both are non-basic, then we need to change every equation of the form
$$
x_i = c_i + a'_v \delta_v^+ - a'_v \delta_v^- + e
$$
to
$$
x_i = c_i + a'_v (\beta - \alpha) + a'_v \delta_v^+ - a'_v \delta_v^- + e
$$
Hence modifying the tableau to reflect the new values of edit and stay
constraints involves only changing the constant values in some equations.
The modifications for stay constraints always result in a tableau in basic
feasible solved form, since it never makes a constant become negative.
In contrast the modifications for edit constraints may not.

To return to our example, suppose we pick up $x_m$ with the mouse and
move it to 60.  Then we have that $\alpha = 50$ and $\beta = 60$,
so we need to add 10 times the 
coefficient of $\delta_{x_m}^+$ to the constant part of every row.
The modified tableau, after the updates for both the stays and edits, is 
\begin{trivlist}\item
minimize $20 + 1002 \delta_{x_m}^+ + 998 \delta_{x_m}^-  + 2
\delta_{x_l}^- + 2\delta_{x_r}^- $ 
subject to 
$$
\begin{array}{rlrrrrrr} 
x_m & = &60 &  + \delta_{x_m}^+ & - \delta_{x_m}^- \\
x_r & = &90 & + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                - \delta_{x_l}^+ & + \delta_{x_l}^- \\ \hline
x_l & = & 30  & & & + \delta_{x_l}^+ & - \delta_{x_l}^- \\
s_1 & = &50 &  + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                -2 \delta_{x_l}^+ & +2 \delta_{x_l}^- \\
s_2 & = &10 &   - 2 \delta_{x_m}^+ &+2\delta_{x_m}^- &
                + \delta_{x_l}^+ & - \delta_{x_l}^- \\
\delta_{x_r}^+ & = & 20 & + 2 \delta_{x_m}^+  & - 2\delta_{x_m}^- &
        - \delta_{x_l}^+ & + \delta_{x_l}^- & +\delta_{x_r}^-
\end{array}
$$
\end{trivlist}
Clearly it is feasible and already in optimal form, and so we have
incrementally resolved the problem by simply modifying constants in the
tableaux. The new tableaux give the solution
$\{x_m \mapsto 60, x_l \mapsto 30, x_r \mapsto 90\}$.
So sliding the midpoint rightwards 
has caused the right point to slide rightwards as well, but twice as far.
The resulting diagram is shown at the top of Figure~\ref{fig:quasi}.

\begin{figure}[htb]
\begin{center}
\input quasi.eepic
\end{center}
\caption{Resolving the constraints\label{fig:quasi}}
\end{figure}

Suppose we now move $x_m$ from 60 to 90.  
The modified tableau is 
\begin{trivlist}\item
minimize $60 + 1002 \delta_{x_m}^+ + 998 \delta_{x_m}^-  + 2
\delta_{x_l}^- + 2\delta_{x_r}^- $ 
subject to 
$$
\begin{array}{rlrrrrrr} 
x_m & = &90 &  + \delta_{x_m}^+ & - \delta_{x_m}^- \\
x_r & = &150 & + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                - \delta_{x_l}^+ & + \delta_{x_l}^- \\ \hline
x_l & = & 30  & & & + \delta_{x_l}^+ & - \delta_{x_l}^- \\
s_1 & = &110 &  + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                -2 \delta_{x_l}^+ & +2 \delta_{x_l}^- \\
s_2 & = &-50 &   - 2 \delta_{x_m}^+ &+2\delta_{x_m}^- &
                + \delta_{x_l}^+ & - \delta_{x_l}^- \\
\delta_{x_r}^+ & = & 60 & + 2 \delta_{x_m}^+  & - 2\delta_{x_m}^- &
        - \delta_{x_l}^+ & + \delta_{x_l}^- & +\delta_{x_r}^-
\end{array}
$$
\end{trivlist}
The tableau is no longer in basic feasible solved form,
since the constant of the row 
for $s_2$ is negative, even though $s_2$ is supposed to be non-negative.

Thus, in general, after updating the constants for the edit constraints,
the simplex tableau $C_S$ may no longer be in basic feasible solved form,
since some of the constants may be negative.  However, the tableau is still
in basic form, so we can still read a solution directly from it as before.
And since no coefficient has changed, in particular in the optimization
function, the resulting tableau reflects an optimal but not feasible
solution.

We need to find a feasible and optimal solution.  We could do so by adding
artificial variables (as we did when adding a constraint), optimizing the
sum of the artificial variables to find an initial feasible solution, and
then reoptimizing the original problem.

But we can do much better.  The process of moving from an optimal and
infeasible solution to an optimal and feasible solution is exactly the dual
of normal simplex algorithm, where we progress from a feasible and
non-optimal solution to feasible and optimal solution.  Hence we can use
the \emph{dual simplex algorithm} to find a feasible solution while staying
optimal.

Solving the dual optimization problem starts from
an infeasible optimal tableau of the form
\begin{quote}\vspace*{-1ex}
minimize $e + \Sigma_{j=1}^m d_j y_j$ subject to
$$\begin{array}{rcl}
\bigwedge_{i=1}^{n} x_i = c_i + \Sigma_{j=i}^m a_{ij} y_j
\end{array}$$
\end{quote}\vspace{-0.9ex}
where some $c_i$ may be negative 
for rows with non-negative basic variables (infeasibility) 
and each $d_j$ is non-negative (optimality).

The dual simplex algorithm selects an exit variable
by finding a row $I$ with non-negative basic variable
$x_I$ and negative constant $c_I$\@.  
The entry variable is the variable $y_J$
such that
the ratio $d_J/a_{IJ}$ is the minimum of all $d_j/a_{Ij}$
where $a_{Ij}$ is positive. This ensures that when pivoting we stay at an
optimal solution.
The pivot replaces $y_j$ by
$$-1/a_{Ij} (-x_I + c_I + \Sigma_{j=1, j\neq J }^m a_{Ij} y_j)$$
and is performed as in the (primal) simplex algorithm.
The algorithm is shown in Figure~\ref{fig:simplex-dual}.

\begin{figure}[tb]
\begin{center}
\fbox{
\begin{minipage}{10cm}
\begin{tabbing}
xx \= xx \= xx \= xx \= \kill
%Boolean
\textsf{re\_opt}($C_S$,$f$) \\
\> \textbf{foreach} $stay:v \in C$ \\
\> \> \textbf{if} $\delta_v^+$ or $\delta_v^-$ is basic in row $i$ \textbf{then} $c_i$ :=
0 \textbf{endif} \\
\>  \textbf{endfor} \\
\> \textbf{foreach} $edit:v \in C$ \\
\> \> \textbf{let} $\alpha$ and $\beta$ be the previous and current edit
values for $v$\\
\> \> \textbf{let} $\delta_v^+$ be $y_j$ \\
\> \> \textbf{foreach} $i \in \{1, \ldots ,n\}$ \\
\> \> \>  $c_i$ := $c_i + a_{ij} (\beta - \alpha)$ \\
\> \> \textbf{endfor} \\
\> \textbf{endfor} \\
\> \textbf{repeat} \\
%\> \> \textbf{if} for each $i \in \{ 1, \ldots , n\}$ 
%       $c_i \ge 0$ or $x_I \geq 0 \not\in C_I$  \textbf{then} \\
\> \> \% Choose variable $x_I$ to become non-basic\\
\> \> \textbf{choose} $I$ where $c_I < 0$ \\
\> \> \textbf{if} there is no such $I$ \\
\> \> \> \textbf{return} $true$  \\
\> \> \textbf{endif} \\

\> \> \% Choose variable $y_J$ to become basic\\
\> \> \textbf{if} for each $j \in \{ 1, \ldots , m\}$ $a_{Ij} \leq 0$ \textbf{then} \\
\> \> \> \textbf{return} $false$\\
\> \> \textbf{endif} \\
\> \> \textbf{choose} $J \in \{1, \ldots , m\}$ such that\\
\> \> \> $d_J / a_{IJ} =
    \min_{j \in \{1, \ldots , m\}} \{ d_j / a_{Ij} \mid a_{Ij} > 0 \}$\\
\> \> $e$ := $( x_I - c_I - \sum_{j=1, j\ne J}^m a_{Ij} y_j)/ a_{IJ}$\\
\> \> replace $y_J$ by $e$ in $f$\\
\> \> \textbf{for} each $i \in \{1, \ldots ,n\}$\\
\> \> \> \textbf{if} $i \ne I$ \textbf{then}
       replace $y_J$ by $e$ in row $i$ \textbf{endif}\\
\> \> \textbf{endfor}\\
\> \> replace the $I^{th}$ row by $y_J = e$ \\
\> \textbf{until} $false$ \\
\end{tabbing}
\end{minipage}
}
\end{center}
\caption{Dual Simplex Re-optimization\label{fig:simplex-dual}}
\end{figure}

Continuing the example above
we select the exit variable $s_2$, 
the only non-negative basic variable for a row with negative constant.  
We find that $\delta_{x_l}^+$
has the minimum ratio since its coefficient in the optimization function
is 0, so it will be the entry variable.
Replacing $\delta_{x_l}^+$ everywhere by 
$50 + s_2 + 2 \delta_{x_m}^+ - 2 \delta_{x_m}^- + \delta_{x_l}^+$
we obtain the tableau

\begin{trivlist}\item
minimize $30060 + 1002 \delta_{x_m}^+ + 998 \delta_{x_m}^-  + 2
\delta_{x_l}^- + 2\delta_{x_r}^- $ 
subject to 
$$
\begin{array}{rlrrrrrr} 
x_m & = &90 & & + \delta_{x_m}^+ & - \delta_{x_m}^- \\
x_r & = &100 & - s_2 \\ \hline
x_l & = & 80 & + s_2 & + 2 \delta_{x_m}^+ & - 2 \delta_{x_m}^- \\
s_1 & = &110 & - 2 s_2 & + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- \\
\delta_{x_l}^+ & = & 50 & + s_2 & + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                + \delta_{x_l}^- \\
\delta_{x_r}^+ & = & 40 & - s_2 &&&&  +\delta_{x_r}^-
\end{array}
$$
\end{trivlist}
The tableau is feasible (and of course still
optimal) and represents the solution
$\{x_m \mapsto 90, x_r \mapsto 100, x_l \mapsto 80\}$.
So by sliding the midpoint further right, the rightmost point hits the wall
and the left point slides right to satisfy the constraints.
The resulting diagram is shown at the bottom of Figure~\ref{fig:quasi}.

To summarize, incrementally finding a new solution for new input variables
involves updating the constants in the tableaux to reflect the updated stay
constraints, then updating the constants to reflect the updated edit
constraints, and finally reoptimizing if needed.  In an interactive
graphical application, when using the dual optimization method typically a
pivot is only required when one part first hits a barrier, or first moves
away from a barrier.  The intuition behind this is that
when a constraint
first becomes unsatisfied, the value of one of its error variables will
become non-zero, and hence the variable will have to enter the basis;
when a constraint first becomes satisfied,
we can move one of its error variables out of the basis.

In the example, pivoting occurred when the right point $x_r$ came up against a
barrier.  Thus, if we picked up the midpoint $x_m$ with the mouse and
smoothly slid it rightwards, 1 pixel every screen refresh, only one pivot
would be required in moving from 50 to 95.  This illustrates why the dual
optimization is well suited to this problem and leads to efficient
resolving of the hierarchical constraints.

\section{Implementation Details}
\label{cassowary-details}

This section explains the details of the various Cassowary
implementations. There is also a subsection on some fine points
regarding the comparator.

\subsection{Solver Protocol}
\label{solver-protocol}

The solver itself is represented as an instance of {\sf ClSimplexSolver}.
The public message protocol is as follows.

\begin{description}

\item[{\sf addConstraint(ClConstraint cn)}] \ \\
Incrementally add the linear constraint {\sf cn} to the tableau.  The
constraint object contains its strength.

\item[{\sf removeConstraint(ClConstraint cn)}] \ \\
Remove the constraint {\sf cn} from the tableau.  Also remove 
any error variables associated with {\sf cn} from the objective function.


\item[{\sf addEditVar(ClVariable v, ClStrength s)}] \ \\
      Add an edit constraint of strength {\sf s} on variable {\sf v} to
      the tableau so that {\sf suggestValue} (see below) can be used on
      that variable after a {\sf beginEdit()}.
      

\item[{\sf removeEditVar(ClVariable v)}] \ \\
      Remove the previously added edit constraint on variable {\sf v}.
      The {\sf endEdit} message automatically removes all the edit
      variables as part of terminating an edit manipulation.
      

\item[{\sf beginEdit()}] \ \\
      Prepare the tableau for new values to be given to the
      currently-edited variables.  The {\sf addEditVar} message should
      be used before calling {\sf beginEdit}, and {\sf suggestValue} and
      {\sf resolve} should be used only after {\sf beginEdit} has been
      invoked (but before the matching {\sf endEdit}.
      

\item[{\sf suggestValue(ClVariable v, double n)}] \ \\
      Specify a new desired value, {\sf n}, for the variable {\sf v}.
      Before this call, {\sf v} needs to have been added as a variable
      of an edit constraint (either by {\sf addConstraint} of a
      hand-built {\sf EditConstraint} object or more simply using {\sf
        addEditVar}).
      

\item[{\sf endEdit()}] \ \\
      Denote the end of an edit manipulation, thus removing all edit
      constraints from the tableau.  Each {\sf beginEdit} call must be
      matched with a corresponding {\sf endEdit} invocation.

\item[{\sf resolve()}] \ \\
Try to re-solve the tableau given the newly specified desired values.
Calls to resolve should be sandwiched between a {\sf beginEdit()} and a
{\sf endEdit()}, and should occur after new values for edit variables
are set using {\sf suggestValue}.

\item[{\sf addPointStays(Vector points)}] \ \\
This is kind of a kludge, and addresses the desire to satisfy the stays
on both the {\sf x} and {\sf y} components of a given point rather than
on the {\sf x} component of one point and the {\sf y} component of another.
{\sf points} is an array of points, whose {\sf x} and {\sf y} components
are constrainable variables.  Add a weak stay constraint to the {\sf x}
and {\sf y} variables of each point.  The weights for the {\sf x} and
{\sf y} components of a given point are the same.  However, the weights
for successive points are each smaller than those for the previous
point (1/2 of the previous weight).  The effect of this is to encourage the
solver to satisfy the stays on both the {\sf x} and {\sf y} of a given
point rather than the {\sf x} stay on one point and the {\sf y} stay on
another.  See Subsection \ref{comparator-details} for more on this issue.

\item[{\sf reset}] \ \\
Re-initialize the solver from the original constraints,
thus getting rid of any accumulated numerical problems.  (It is not clear
how often such problems arise, but here is the method anyway.)

\end{description}

\subsubsection{Possible Revisions to Solver Protocol}

One thing that might be worth changing is the way that stay constraints
are handled.  Currently, each variable that is to stay at an old value
needs an explicit stay constraint.  These stay constraints need to be
added before any other constraints, since otherwise the variable's value
is likely to be changed inappropriately to satisfy the other
constraints while initially building the tableau.

Instead, stay constraints could be implicit for each variable, and thus
be in effect before any other constraints.

\subsection{Principal Classes}

Here is a listing of the principal classes.  (In the current implementation
all the classes start with ``{\sf Cl}''.)  All of the classes are of course
direct or indirect sublclasses of {\sf Object}.

\begin{tabbing}
XXX\=XXX\=XXX\=XXX\=XXX\=XXX\=   \kill
\> {\sf Object} \\
\> \> {\sf ClAbstractVariable} \\
\> \> \> {\sf ClDummyVariable} \\
\> \> \> {\sf ClObjectiveVariable} \\
\> \> \> {\sf ClSlackVariable} \\
\> \> \> {\sf ClVariable} \\
\> \> {\sf ClConstraint} \\
\> \> \> {\sf ClEditOrStayConstraint} \\
\> \> \> \> {\sf ClEditConstraint} \\
\> \> \> \> {\sf ClStayConstraint} \\
\> \> \> {\sf ClLinearConstraint} \\
\> \> \> \> {\sf ClLinearEqualityConstraint} \\
\> \> \> \> {\sf ClLinearInequalityConstraint} \\
\> \> {\sf ClLinearExpression} \\
\> \> {\sf ClTableau} \\
\> \> \> {\sf ClSimplexSolver} \\
\> \> {\sf ClStrength} \\
\> \> {\sf ClSymbolicWeight}
\end{tabbing}

Following is a description of the classes.  Some of these classes make
use of the Dictionary Abstract Data Type: Dictionaries have keys and
values and permit efficiently finding the value
for a given key, and adding or deleting key/value pairs.  One can also
iterate through all keys, all values, or all key/value pairs.

The solver itself is represented as an instance of {\sf ClSimplexSolver},
with public message protocol as described above.  There is more on the
implementation of this class in Subsection 
\ref{cl-simplex-solver-implementation}.

\subsubsection{Variables}

{\sf ClAbstractVariable} and its subclasses represent various kinds of
constrained variables.  {\sf ClAbstractVariable} is an abstract class, that
is, it is just used as a superclass of other classes; one does't make
instances of {\sf ClAbstractVariable} itself.  {\sf ClAbstractVariable}
defines the message protocol for constrainable variables.  Its only
instance variable is {\sf name}, which is a string name for the variable.
(This is used for debugging and constraint understanding tasks.)

Instances of the concrete {\sf ClVariable} sub-class of {\sf
  ClAbstractVariable} are what the user of the solver sees (hence
it was given a nicer class name).  This class has an instance variable
{\sf value} that holds the value of this variable.  Users of the solver can
send one of these variables the message {\sf value} to get its value.

The other subclasses of {\sf ClAbstractVariable} are used only within the
solver.  They do not hold their own values --- rather, the value is just
given by the current tableau.  None of them have any additional instance
variables.

Instances of {\sf ClSlackVariable} are restricted to be non-negative.  They
are used as the slack variable when converting an inequality constraint to
an equation, and for the error variables to represent non-required constraints.

Instances of {\sf ClDummyVariable} is used as a marker variable to allow
required equality constraints to be deleted.  (For inequalities or
non-required constraints, the slack or error variable is used as the
marker.)  These dummy variables are never pivoted into the basis.

An instance of {\sf ClObjectiveVariable} is used to index the objective row
in the tableau.  (Conventionally this variable is named $z$.)  This kind of
variable is just for convenience --- the tableau is represented as a
dictionary (with some additional cross-references).  Each row is
represented as an entry in the dictionary; the key is a basic variable and
the value is an expression.  So an instance of {\sf ClObjectiveVariable} is
the key for the objective row.  The objective row is unique in that the
coefficients of its expression are {\sf ClSymbolicWeight}s not just
ordinary real numbers. (The current C++ and Java implementations convert {\sf
  ClSymbolicWeight}s to real numbers to avoid dealing with {\sf
  ClLinearExpression}s parameterized on the type of the coefficient.
See section \ref{symweights} for more details.)

All variables understand the following messages: {\sf isDummy}, {\sf
isExternal}, {\sf isPivotable}, and {\sf isRestricted}.  They also
understand messages to get and set the variable's name.

\begin{figure}[htb]
\begin{center}
\begin{tabular}{|l|c|c|c|c|} \hline
Class               & isDummy & isExternal & isPivotable & isRestricted \\\hline\hline
ClDummyVariable     & True    & False      & False       & True \\\hline
ClVariable          & False   & True       & False       & False \\\hline
ClSlackVariable     & False   & False      & True        & True \\\hline
ClObjectiveVariable & False   & False      & False       & False \\\hline
\end{tabular}
\end{center}
\caption{Subclasses of {\sf ClAbstractVariable}\label{fig:absVarSubclasses}}
\end{figure}

For {\sf isDummy}, instances of {\sf ClDummyVariable} return true and
everone else returns false.  The solver uses this message to test for dummy
variables.  It will not choose a dummy variable as the subject for a new
equation, unless all the variables in the equation are dummy variables.
(The solver also will not pivot on dummy variables, but this is handled by the
{\sf isPivotable} message.)

For {\sf isExternal}, instances of {\sf ClVariable} return true and
everyone else returns false.  If a variable responds true to this message,
it means that it is known outside the solver, and so the solver needs to
give it a value after solving is complete.

For {\sf isPivotable}, instances of {\sf ClSlackVariable} returns true
and everyone else returns false.  The solver uses this message to decide
whether it can pivot on a variable.

For {\sf isRestricted}, instances of {\sf ClSlackVariable} and
of {\sf ClDummyVariable} return true, and instances of
{\sf ClVariable} and {\sf ClObjectiveVariable} return false.  Returning
true means that this variable is restricted to being non-negative.


So variables do not hold state, except for a name for debugging, and a value
for instances of {\sf ClVariable} --- mostly their significance is just
their identity.  The only other messages that variables understand are some
messages to {\sf ClVariable} for creating constraints --- see Subsection
\ref{constraint-creation}.

\subsubsection{Linear Expressions}

Instances of the class {\sf ClLinearExpression} hold a linear expression,
and are used in building and representing
constraints, and in representing the tableau.  A linear expression holds a
dictionary of variables and coefficients (the keys are variables and the
values are the corresponding coefficients).  Only variables
with non-zero coefficients are included in the dictionary; 
if a variable is not in this
dictionary its coefficient is assumed to be zero.  The other instance
variable is a constant.  So to represent the linear expression 
$a_1 x_1 + \cdots + a_n x_n + c$, the dictionary would hold the key $x_1$
with value $a_1$, etc., and the constant $c$.  This representation was
convenient in Smalltalk given the built-in class dictionary, and allows one
to find the coefficient for a given variable without searching.  It has
some space overhead for the dictionary.  An alternative representation
would be to use a linked list for the coefficients and variables --- with
this representation one would need to search the list for a given variable,
but the representation would be more compact.  If expressions typically had
only a small number of non-zero coefficients this representation may be
preferable.

Linear expressions understand  a large number of messages.  Some of these
are for constraint creation (see Section \ref{constraint-creation}).  The
others are to substitute an expression for a variable in the constraint, to
add an expression, to find the coefficient for a variable, and so forth.

\subsubsection{Constraints}
\label{constraint-classes}

There is an abstract class {\sf ClConstraint} that serves as the superclass
for other concrete classes.  It defines two instance variables: {\sf
strength} and {\sf weight}.  The variable {\sf strength} is the strength of
this constraint in the constraint hierarchy (and should be an instance of
{\sf ClStrength}), while {\sf weight} is a float indicating the weight of
the constraint, or nil if it does not have a weight.  (Weights are only
relevant for the weighted-sum-better comparator, not for
locally-error-better.) 

Constraints understand various message that return true or false regarding
some aspect of the constraint, such as {\sf isRequired}, {\sf
isEditConstraint}, {\sf isStayConstraint}, and {\sf isInequality}.

{\sf ClLinearConstraint} is an abstract subclass of {\sf ClConstraint}.  It
adds an instance variable {\sf expression}, which holds an instance of
{\sf ClLinearExpression}.  It has two concrete subclasses.  An instance of
{\sf ClLinearEquation} represents the linear equality constraint  \\
\hspace*{1cm} {\sf expression = 0}. \\
An instance of {\sf  ClLinearInequality} represents
the constraint \\
\hspace*{1cm} {\sf expression} $\geq$ 0.

The other part of the hierarchy is for edit and stay constraints (both of
which are represented explicitly in the current implementation).  {\sf
ClEditOrStayConstraint} has an instance variable {\sf variable}, which is
the variable with the edit or stay.  Otherwise all they do is respond
appropriately to the messages {\sf isEditConstraint} and {\sf
isStayConstraint}. 

This constraint hierarchy is also intended to allow extension to include
local propagation constraints (which would be another subclass of {\sf
  ClConstraint}) -- otherwise we could have made everything be a linear
constraint, and eliminated the abstract class {\sf ClConstraint}
entirely.

\subsubsection{Constraint Creation}
\label{constraint-creation}

This subsection describes a mechanism to allow constraints to be defined
easily by programmers.  The convenience afforded by Cassowary varies
among languages.  Smalltalk's dynamic nature makes it the most
expressive.  C++'s operator overloading still permits using natural
infix notation.  Java, however, requires using regular methods, and
leaves us with the single option of prefix expressions when building
constraints.

In Smalltalk, the messages +, -, *, and / are defined for {\sf
  ClVariable} and {\sf ClLinearExpression} to allow convenient creation
of constraints by programmers.  Also, {\sf ClVariable} and {\sf
  ClLinearExpression}, as well as {\sf Number}, define {\sf cnEqual:},
{\sf cnGEQ:}, and {\sf cnLEQ:} to return linear equality or inequality
constraints.  Thus, the Smalltalk expression

\hspace*{1cm} {\sf 3*x+5 cnLEQ: y}

returns an instance of {\sf ClLinearEquality} representing the
constraint $3 x + 5 \leq y$.  This works as follows.  The number 3 gets
the message {\sf * x}.  Since {\sf x} is not a number, 3 sends the
message {\sf * 3} to {\sf x}.  {\sf x} is an instance of {\sf
  ClVariable}, which understands * to return a new linear expression
with a single term, namely itself times the argument.  (If the argument
is not a number it raises an exception that the expression is
non-linear.)  The linear expression representing $3x$ gets the message +
with the argument 5, and returns a new linear expression representing $3
x + 5$.  This linear expression gets the message {\sf cnLEQ:} with the
argument {\sf y}.  It computes a new linear expression representing $3 x
+ 5 - y$, and then returns an instance of {\sf ClLinearInequality} with
this expression.

(It is tempting to make this nicer by using the $=$, $<=$, and $>=$
messages, so that one could write

\hspace*{1 cm} {\sf 3*x+5 $<=$ y}

instead but since the rest of Smalltalk expects $=$, $<=$, and $>=$ to
perform a test and return a boolean, rather than to return a constraint,
this would not be a good idea.)

Similarly, in C++ the arithmetic operators are overloaded to build {\sf
  ClLinearExpression}s from {\sf ClVariable}s and other {\sf
  ClLinearExpression}s.  Actual constraints are built using various
constructors for {\sf ClLinearEquation} or {\sf ClLinearInequality}.  An
enumeration defines the symbolic constants {\sf cnLEQ} and {\sf cnGEQ}
to approximate the Smalltalk interface.  For example:

\hspace*{1 cm}{\sf ClLinearInequality cn(3*x+5, cnLEQ,y); ~ ~ ~ // C++}

build the constraint {\sf cn} representing $3 x + 5 \leq y$.

In Java, the same constraint would be built as follows:

\hspace*{1 cm}{\sf ClLinearInequality cn = new
  ClLinearInequality(CL.Plus(CL.Times(x,3),5), CL.LEQ, y);}

Though the Java implementation makes it difficult to express hard-coded
constraints, use of the implementation in conjunction with a
user-interface for specifying the constraints has shown that the
inconvenience is relatively unimportant.

\subsubsection{Symbolic Weights and Strengths}
\label{symweights}

The constraint hierarchy theory allows an arbitrary (although finite)
number of strengths of constraint.  In practice, however, programmers use a
small number of strengths in a stylized way.  The current implementation
therefore includes a small number of pre-defined strengths, and the maximum
number of strengths is defined as a constant.  (This constant can be
changed --- see below --- but we would not expect to do so frequently.)

The strengths are currently defined as follows.

\begin{description}

\item[{\sf required}]  Required constraints must be satisfied.  This
strength is used for most programmer-defined constraints.

\item[{\sf strong}] This strength is used for edit constraints.

\item[{\sf medium}] Currently unused.  

\item[{\sf weak}] This strength is used for stay constraints.

\end{description}

These are represented as four instances of {\sf ClStrength}.

The other relevant class is {\sf ClSymbolicWeight}.  As mentioned in
Section \ref{non-requireds}, the objective function is formed as the
weighted sum of the positive and negative errors for the non-required
constraints.  The weights should be such that the stronger constraints
totally dominate the weaker ones.  In general to pick a real number for the
weight we need to know how big the values of the variables can be.  To
avoid this problem altogether, rather than real numbers as weights we use
symbolic weights and a lexicographic ordering, which ensures that strong
constraints are always satisfied in preference to weak ones.  

Instances of {\sf ClSymbolicWeight} are used to represent these symbolic
weights.  These instances have an array of floating point numbers, whose
length is the number of non-required strengths (so 3 at the moment).  Each
element of the array represents the value at that strength, so $(1.0, 0.0,
10.0)$ represents a weight of 1.0 {\sf strong}, 0.0 {\sf medium}, and 10.0
{\sf weak}.  (In Smalltalk {\sf ClSymbolicWeight} is a variable length
subclass; we could have had an instance variable with an array of length 3
instead.)   Symbolic weights understand various arithmetic messages, as
follows (in C++, these are implemented using operator overloading):

\begin{description}

\item[{\sf + w}]  \ \\
{\sf w} is also a symbolic weight.
Return the result of adding {\sf self} to {\sf w}.  

\item[{\sf -- w}] \ \\
{\sf w} is also a symbolic weight.
Return the result of subtracting {\sf w} from {\sf self}

\item[{\sf * n}] \ \\
{\sf n} is a number.
Return the result of multiplying {\sf self} by {\sf n}.  

\item[{\sf / n}] \ \\
{\sf n} is a number.
Return the result of dividing {\sf self} by {\sf n}.  

\item[$<=$ {\sf  n}, ~ $>=$ {\sf n}, ~ $<$ {\sf n}, ~ $>$ {\sf n}, ~ $=$ {\sf n}] \ \\
{\sf w} is a symbolic weight.
Return true if {\sf self} is related to {\sf n} as the operator normally
queries.

\item[{\sf negative}] \ \\
Return true if this symbolic weight is negative
(i.e.\ it does not consist of all zeros and the first non-zero number is
negative). 

\end{description}

Instances of {\sf ClStrength} represent a strength in the constraint
hierarchy.  The instance variables are {\sf name} (for printing
purposes) and {\sf symbolicWeight}, which is the unit symbolic weight
for this strength.  Thus, with the 3 strengths as above, {\sf strong} is
$(1.0, 0.0, 0.0)$, {\sf medium} is $(0.0, 1.0, 0.0)$, and {\sf weak} is
$(0.0, 0.0, 1.0)$.

The above arithmetic messages let the Smalltalk implementation of the
solver use symbolic weights just like numbers in expressions.  This is
important because the objective row in the tableau has coefficients
which are {\sf ClSymbolicWeight}s but are subject to the same
manipulation as the other tableau rows whose expressions have
coefficients which are just real numbers.

In both C++ and Java, an additional message {\sf asDouble()} is understood
by {\sf ClSymbolicWeight}s.  This converts the representation to a real
number that approximates the total ordering suggested by the more
general vector of real numbers.  It is these real numbers that are used
as the coefficients in the objective row of the tableau (in lieu of {\sf
  ClSymbolicWeight}s, which the coefficients conceptually are).  This
avoids the obvious complexities such genericity introduces to the static
type systems of C++ and Java. (An improved C++ implementation using
templates is underway).

Also, since Java lacks operator overloading, the above operations are
invoked using suggestive alphabetic method names such as {\sf add}, {\sf
  subtract}, {\sf times}, {\sf lessThan}, etc.


\subsection{{\sf ClSimplexSolver} Implementation}
\label{cl-simplex-solver-implementation}

Here are the instance variables of {\sf ClSimplexSolver} (some fields
are inherited from {\sf ClTableau}, the base class of {\sf
  ClSimplexSolver} which provides the basic sparse-matrix interface ---
see section \ref{cl-tableau-implementation}).

\begin{description}

\item[{\sf rows}] \ \\
A dictionary with keys {\sf ClAbstractVariable} and
values {\sf ClLinearExpression}.  This holds the tableau.  Note that the
keys can be either restricted or unrestricted variables, i.e.\ both $C_U$
and $C_S$ are actually merged into one tableau.  This simplified the code
considerably, since many operations are applied to both restricted and
unrestricted rows.

\item[{\sf columns}] \ \\
A dictionary with keys {\sf ClAbstractVariable} and
values {\sf Set of ClAbstractVariable}.  These are the column
cross-indices.  Each parametric variable {\sf p} should be a key in this
dictionary.  The corresponding set should include exactly those basic
variables whose linear expression includes {\sf p}
({\sf p} will of course have a non-zero coefficient).  The keys can be 
either unrestricted or restricted variables.

\item[{\sf objective}] \ \\
Return an instance of {\sf ClObjectiveVariable}
(named {\sf z}) that is the key for the objective row in the tableau.

\item[{\sf infeasibleRows}] \ \\
Return a set of basic variables that have
infeasible rows.  (This is used when re-optimizing with the dual simplex
method.)  

\item[{\sf prevEditConstants}] \ \\
An array of constants (floats) for the edit
constraints on the previous iteration.  The elements in this array must be
in the same order as {\sf editPlusErrorVars} and {\sf editMinusErrorVars},
and the argument to the public {\sf resolve:} message.

\item[{\sf stayPlusErrorVars}] \ \\
An array of plus error variables (instances
of {\sf ClSlackVariable}) for the stay constraints.  The corresponding
negative error variable must have the same index in {\sf stayMinusErrorVars}.

\item[{\sf stayMinusErrorVars}] \ \\
See {\sf stayPlusErrorVars}.

\item[{\sf editPlusErrorVars}] \ \\
An array of plus error variables (instances
of {\sf ClSlackVariable}) for the edit constraints.  The corresponding
negative error variable must have the same index in {\sf editMinusErrorVars}.

\item[{\sf editMinusErrorVars}] \ \\
See {\sf editPlusErrorVars}.

\item[{\sf markerVars}] \ \\
A dictionary whose keys are constraints and whose
values are instances of a subclass of {\sf ClAbstractVariable}.  This
dictionary is used to find the marker variable for a constraint when
deleting that constraint.  A secondary use is that iterating through the
keys will give all of the original constraints (useful for {\sf reset}).

\item[{\sf errorVars}] \ \\
A dictionary whose keys are constraints and whose
values are arrays of {\sf ClSlackVariable}.  This dictionary gives the
error variable (or variables) for a given non-required constraint.  We need
this if the constraint is deleted, since the corresponding error variables
must be deleted from the objective function.

\item[{\sf slackCounter}] \ \\
Used for debugging.  An integer used to generate
names for slack variables, which are useful when printing out expressions.
(Thus we get slack variables named {\sf s1}, {\sf s2}, etc.)

\item[{\sf artificialCounter}] \ \\
Similar to {\sf slackCounter} but for
artificial variables.

\item[{\sf dummyCounter}]  \ \\
Similar to {\sf slackCounter} but for
dummy variables (ie.\ marker variables for required equality constraints).

\end{description}

\subsubsection{{\sf ClTableau} (Sparse Matrix) Operations }
\label{cl-tableau-implementation}

The basic requirements for the tableau representation are that one should
be able to perform the following operations efficiently:

\begin{itemize}

\item determine whether a variable is basic

\item determine whether a variable is parametric

\item find the corresponding expression for a basic variable

\item iterate through all the parametric variables with non-zero
coefficients in a given row

\item find all the rows that contain a given parametric variable with a
non-zero coefficient

\item add a row

\item remove a row

\item remove a parametric variable

\item substitute out a variable (i.e.\ replace all occurrences of a
variable with an expression, updating the tableau as appropriate).

\end{itemize}

The representation of the tableau as a dictionary of rows, with column
cross-indices, supports these operations.  Keeping the cross indices
up-to-date is a bit tricky, and so the solver actually accesses the rows
and columns only via the below interface of {\sf ClTableau}, to avoid
getting the two representations out of sync.

\begin{description}
  

\item[{\sf addRow(ClAbstractVariable var, ClLinearExpression expr)}]  \ \\
      Add the constraint {\sf var=expr} to the tableau.  {\sf var} will
      become a basic variable.  Update the column cross indices.
      

\item[{\sf noteAddedVariable(ClAbstractVariable var, ClAbstractVariable subject)}]  \ \\
      Variable {\sf var} has been added to the linear expression for
      {\sf subject}.  Update the column cross indices.
      

\item[{\sf noteRemovedVariable(ClAbstractVariable var, ClAbstractVariable subject)}]  \ \\
      Variable {\sf var} has been removed from the linear expression for
      {\sf subject}.  Update the column cross indices.
      

\item[{\sf removeColumn(ClAbstractVariable var)}] \ \\
      Remove the parametric variable {\sf var} from the tableau.  This
      involves removing the column cross index for {\sf var} and
      removing {\sf var} from every expression in {\sf rows} in which it
      occurs.
      

\item[{\sf removeRow(ClAbstractVariable var)}] \ \\
      Remove the basic variable {\sf var} from the tableau.  Since {\sf
        var} is basic, there should be a row {\sf var=expr}.  Remove
      this row, and also update the column cross indices.
      

\item[{\sf substituteOut(ClAbstractVariable var, ClLinearExpression expr)}] \ \\
      Replace all occurences of {\sf var} with {\sf expr} and update the
      column cross indices.

\end{description}

\subsubsection{Adding a Constraint}
\label{cassowary-adding-constraints}

Section \ref{adding-constraints} discussed how to add constraints
incrementally.  If the equation contains any unrestricted variables, we can
not use an artificial variable because we can not put an equation in $C_S$
that contains an unrestricted variable.  In some other cases we can avoid
using an artificial variable for efficiency.  We can avoid using an
artificial variable if we can choose a subject for the equation from among
its current variables.  Here are the rules for choosing a subject.  (These
are to be used after replacing any basic variables with their defining
expressions.)

We start with an expression {\sf expr} (which is an instance of {\sf
ClLinearExpression}).  If necessary, normalize {\sf expr} by multiplying by
$-1$ so that its constant part is non-negative.  We are adding the
constraint {\sf expr=0} to the tableau.  To do this we want to pick a
variable in {\sf expr} to be the subject of an equation, so that we can add
the row {\sf var=expr2}, where {\sf expr2} is the result of solving {\sf
expr=0} for {\sf var}.

\begin{itemize}

\item If {\sf expr} contains any unrestricted variables, we must choose an
unrestricted variable as the subject.

\item If the subject is new to the solver, we will not have to do any
substitutions, so we prefer new variables to ones that are currently noted
as parametric.  

\item If {\sf expr} contains only restricted variables, if there is a
(restricted) variable in {\sf expr} that has a negative coefficient and
that is new to the solver, we can pick that variable as the subject.

\item Otherwise use an artificial variable.

\end{itemize}

A consequence of these rules is that we can always add a non-required
constraint to the tableau without using an artificial variable, since the
equation will contain a positive and a negative error 
or slack variable, both of
which are new to the solver, and which occur with opposite signs.  
(Constraints that are originally equations will have a positive and
a negative error variable, while constraints that are originally inequalities
will have one error variable and one slack variable, with opposite signs.)  This is
good because a common operation is adding a non-required edit.

\subsubsection{Removing a Constraint}

Here are a few additional remarks in addition to the material presented in
Section \ref{removing-constraints}.  

First, before we remove the constraint, there may be some stay constraints
that were unsatisfied previously --- if we just removed the constraint
these could come into play.  Instead, reset all of the stays so that all
variables are constrained to stay at their current values.

Also, if the constraint being removed is not required we need to remove the
error variables for it from the objective function.  To do this we add the
following to the expression for the objective function:

$$ -1 \times e \times s \times w $$

where $e$ is the error variable if it is parametric, or else $e$ is its
defining expression if it is basic, $s$ is the unit symbolic weight for the
constraint's strength, and $w$ is its weight.  ($s$ is an instance of 
{\sf ClSymbolicWeight} and $w$ is a float.)

If we allow non-required constraints other than stays and edits, we also
need to re-optimize after deleting a constraint, since a non-required
constraint might have become satisfiable (or more nearly satisfiable).

\subsection{Omissions}

The solver should implement Bland's anti-cycling
rule~\cite{marriott-stuckey-book}, but it does not at the moment.
Adding this should be straightforward.

\subsection{Comparator Details}  
\label{comparator-details}  

Our implementation of Cassowary favors solutions that satisfies some of the
constraints completely, rather than ones that partially satisfy e.g.\ each
of two conflicting equalities.  These are still legitimate
locally-error-better solutions.  Cassowary's behaviour is analogous to that
of the simplex algorithm, which always finds solutions at a vertex of the
polytope even if all the solutions on an edge or face are equally good.
(And of course Cassowary behaves this way because simplex does.)

Such solutions are also produced by greedy constraint satisfaction
algorithms, such as local propagation algorithms like DeltaBlue and Indigo,
since these algorithms try to satisfy constraints one at a time, and in
effect the constraints considered first are given a stronger strength than
those considered later.

However, there is an issue regarding comparators and Cassowary, which has
not yet been resolved in an entirely clean way.  One of the public methods
for Cassowary is {\sf addPointStays: points}, as discussed in Subsection
\ref{solver-protocol}.  This method addresses the desire to satisfy the
stays on both the {\sf x} and {\sf y} components of a given point rather
than on the {\sf x} component of one point and the {\sf y} component of
another.

As an example of why this is useful, consider a line with endpoints 
{\sf p1} and {\sf p2} and a midpoint {\sf m}.  There are constraints 
{\sf (p1.x+p2.x)/2 = m.x} and {\sf (p1.y+p2.y)/2 = m.y}.  Suppose we are
editing {\sf m}.  It would look strange to satisfy the stay constraints on
{\sf p1.x} and  {\sf p2.y}, rather than both stays on {\sf p1} or both
stays on {\sf p2}.  (This claim has been verified
empirically --- in earlier implementations of Cassowary this
happened, and indeed it looked strange.)

The current implementation of {\sf addPointStays: points} uses different
weights for the stay constraints for successive elements of {\sf points},
which is a kludge but which seems to work well in practice.

We had some trouble coming up with an example where it would give a bad
answer --- here is a kind of contrived one.  Suppose we have a line with
endpoints {\sf p1} and {\sf p2} and a midpoint {\sf m}.  Suppose also we
have constraints {\sf p2.x = 2*p3.x} and {\sf p2.y = 2*p3.y}.  (This is a
bit strange since here we are using {\sf p3} as a distance from the origin
rather than as a location --- otherwise multiplying it by 2 is
problematic.)  If we give these points to {\sf addPointStays:} in the order
{\sf p1}, {\sf p2}, and {\sf p3}, then the stays on {\sf p1} will have
weight 1, those on {\sf p2} will have weight 0.5, and those on {\sf p3}
will have weight 0.25.  Then, a one legitimate WSB solution would satisfy
the stays on {\sf p1.x} and {\sf p1.y}, but another legitimate WSB solution
would satisfy the stays on {\sf p1.x}, {\sf p2.y}, and {\sf p3.y}.

%% ASKALAN: Better fix -- use another dimension of ClSymbolicWeight?

Here is a cleaner way to handle this situation.  We first introduce a new
comparator with the dubious name of {\em tilted-locally-error-better}.  The
set of TLEB solutions can be defined by taking a given hierarchy, forming
all possible hierarchies by breaking strength ties in all possible ways to
form a totally ordered set of constraints, and taking the union of the sets
of solutions to each of these totally ordered hierarchies.

For example, consider the two constraints {\strength weak} $x=0$ and
{\strength weak} $x=10$.  The set of LEB solutions is the infinite set
of mappings from $x$ to each number in $\left[ 0, 10 \right]$.  Assuming 
equal weights on the constraints, the (single)
least-squares solution is $\left\{ x \mapsto 5 \right\}$.  The TLEB 
solutions are defined by
producing all the totally ordered hierarchies and taking the union of their
solutions.  In this case the two possible total orderings are:

\hspace*{1cm} {\strength weak} $x=0$, {\strength slightly\_weaker} $x=10$ \\
\hspace*{1cm} {\strength slightly\_weaker} $x=0$, {\strength weak} $x=10$

These have solutions $\left\{ x \mapsto 0 \right\}$ and 
$\left\{ x \mapsto 10 \right\}$ respectively, so the set of TLEB 
solutions to the original hierarchy is 
$\left\{ \left\{ x \mapsto 0 \right\} , 
    \left\{ x \mapsto 10 \right\} \right\}$.  

As an aside, we hypothesize that the only psychologically plausible
solutions to the example are $\left\{ x \mapsto 0\right\}$, 
$\left\{ x \mapsto 5\right\} $, and $\left\{ x \mapsto 10\right\}$, 
but not e.g.\ $\left\{ x \mapsto 3.8\right\} $ --- although this hypothesis
has not been tested.  Another relevant question is whether users prefer any
of these solutions over others (for a given application domain).

Next, we introduce a notion of a \emph{compound constraint}, a conjunction
of primitive constraints, in this case linear equalities or inequalities.
For compound constraints, when we break the strength ties in defining the
set of tilted-locally-error-better solutions, we insist on mapping each
linear equality or inequality in a compound constraint to an adjacent
strength.  (We have been a bit imprecise in the use of the term
``constraint'' in this paper, sometimes using it to denote a primitive
constraint and sometimes to denote a conjunction of primitive constraints.
For the present definition, however, we need to distinguish compound
constraints that have been specificially identified as such by the user
from conjunctions of primitive constraints more generally, such as the
constraints $C_S$ and $C_U$ discussed in Section \ref{augmented-simplex-form}.)

Now, to define {\sf addPointStays:} in a more clean way, we could make each
point stay a compound constraint.  To illustrate why this works, consider
the midpoint example again.  We have two endpoints {\sf p1} and {\sf p2},
and a midpoint {\sf m}.  There are constraints {\sf (p1.x+p2.x)/2 = m.x}
and {\sf (p1.y+p2.y)/2 = m.y}, and we are editing {\sf m}.  Then the stays
on {\sf p1} and {\sf p2} will each be compound constraints:

\hspace*{1cm} {\strength weak} ($p1.x$ {\em stay} \& $p1.y$ {\em stay}) \\
\hspace*{1cm} {\strength weak} ($p2.x$ {\em stay} \& $p2.y$ {\em stay})

In defining the set of tilted-locally-error-better solutions, the total
orderings of these constraints that we will consider have the stays on
$p1.x$ and $p1.y$ both stronger than those on $p2.x$ and $p2.y$, or both
weaker.  This produces the desired result.

Note that it is not sufficient just to define a notion of ``compound
constraint'' without adding the notion of tilting --- otherwise if we were
using locally-error-better, we would just sum the errors of the 
primitive constraints, which would allow us to trade off the errors
arbitrarily and hence satisfy the stay on the $x$ component of one point
and the $y$ component of another.

Note also that all of this is not a problem for least-squares-better
comparators such as the one that the QOCA algorithm uses --- that
comparator distributes the error to the $x$ and $y$ components of all
the points with stays of the same strength~\cite{borning-simplex-tr}.

\section{Empirical Evaluation}
\label{empirical-evaluation}

Cassowary has been implemented in Smalltalk, C++, and Java.
Running the Smalltalk implementation of Cassowary on the same problems, 
for the 300 constraint problem,
adding a constraint takes on average $38$~msec (including the initial
solve), deleting a constraint $46$~msec, and resolving as the point moves
$15$~msec.  (Stay and edit constraints are represented explicitly in this
implementation, so there were also stay constraints on each variable, plus
two edit constraints, for a total of 602 constraints.)
For the 900 constraint problem, adding a constraint takes on
average $98$~msec (again including the initial solve), deleting a
constraint $151$~msec, and resolving as the point moves $45$~msec.  These
tests were run using an implementation in OTI Smalltalk Version 4.0 running
on a IBM Thinkpad 760EL laptop computer.

For the C++ implementation on the problem with 900
constraints and variables, adding a constraint takes $40$~msec, deleting 
a constraint $8$~msec, and resolving as the point moves $8$~msec.  By
default for Cassowary the solver optimizes after each constraint
addition or removal, so there is no timing for the initial ``solve''
because it is implicit with each change to the tableau.  The Java
implementation under the basic Sun JDK 1.1 (no JIT compiler) is about
6 to 10 times slower than the C++ implementation.  These tests were run
on a Pentium 200 running Linux 2.0.29.

As these measurements are for implementations in different languages,
running on different machines, they should not be viewed as any kind of
head-to-head comparison.  Nevertheless, they indicate that both
algorithms are eminently practical for use with interactive graphical
applications.

The various implementations of Cassowary are being used actively.
One author is currently embedding the C++ implementation in a X11 system
window manager based on a Scheme configuration language. A demonstration
Constraint Drawing Application using the Java implementation is
available from the authors.  A third Cassowary application currently
being developed using a different Java implementation is a web authoring
tool \cite{borning-multimedia-97}, in which the appearance of a page is
determined by the combination of constraints from both the web author
and the viewer.

\subsection*{Acknowledgments}

Thanks to Kim Marriott, Peter Stuckey and Yi Xiao who were instrumental
in devloping the incremental simplex-based algorithm on which Cassowary
and QOCA~\cite{borning-simplex-tr} are based.

This project has been funded in part by the National Science Foundation
under Grants \mbox{IRI-9302249} and \mbox{CCR-9402551} and in part by Object
Technology International.  Alan Borning's visit to Monash University and
the University of Melbourne was sponsored in part by the
Australian-American Educational Foundation (Fulbright Commission).

Additionally, Greg Badros is supported by a National Science Foundation
Graduate Research Fellowship.  Parts of this material are based upon
work supported under that fellowship.  Any opinions, findings,
conclusions, or recommendations expressed in this publication are those
of the author, and do not necessarily reflect the views of the National
Science Foundation.

\bibliography{constraints}
\bibliographystyle{plain}

\end{document}
