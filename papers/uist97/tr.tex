%% $Id$
%% tr.tex

\documentclass{article}
\usepackage{epic,eepic}

\oddsidemargin  0.25 in
\evensidemargin  0.25 in
\textwidth   6 in
\topmargin -0.125 in
\textheight 8.5 in
\setlength{\parindent}{0.0in}
\setlength{\parskip}{7pt plus 1pt}


\newcommand{\strength}{\sf}
\newcommand{\ignore}[1]{}
\newcommand{\code}{\small\sf}
\newcommand{\finish}[1]{{\bf #1}\marginpar{\Large $\bullet$}}
\input{abb1.tex}

% suppress the page number on this first page
\thispagestyle{empty}
\setcounter{page}{0}

\begin{document}

\vspace*{27 mm}

\begin{center}
{\Large\bf Solving Linear Arithmetic Constraints \\
       for User Interface Applications: \\
\vspace*{1mm}
       Algorithm Details} \\ 
\vspace*{5 mm}

Greg Badros, Alan Borning, Kim Marriott, Peter Stuckey, and Yi Xiao  \\

\vspace*{5 mm}

Technical Report DRAFT \\
Department of Computer Science \& Engineering \\
University of Washington \\
June 1998  \\
\end{center}

\vspace*{7 mm}

{\large {\bf Abstract.}} 
Linear equality and inequality constraints arise naturally in specifying
many aspects of user interfaces, such as requiring that one window be to
the left of another, requiring that a pane occupy the leftmost 1/3 of a
window, or preferring that an object be contained within a rectangle if
possible.  Current constraint solvers designed for UI applications cannot
efficiently handle simultaneous linear equations and inequalities.  This is
a major limitation.  We describe incremental algorithms based on the dual
simplex and active set methods that can solve such systems of constraints
efficiently.  Both algorithms have been implemented and tested.

This informal technical report is adapted from the paper ``Solving Linear
Arithmetic Constraints for User Interface Applications,'' which will appear
in the Proceedings of UIST'97 (The ACM User Interface and Software
Technology Symposium).  It contains additional details, in particular of
the Cassowary and QOCA algorithms and their implementations.


\vspace*{2 mm}

Authors' addresses:

\begin{center}
\begin{tabular}{ll}
Greg Badros, Alan Borning            & Kim Marriott   \\
Dept.\ of Computer Science \& Engr.  & Dept.\ of Computer Science \\
University of Washington             & Monash University \\
Box 352350                           & Wellington Road  \\
Seattle, Washington 98195, USA       & Clayton, Victoria 3168, Australia   \\
\{gjb,borning\}@cs.washington.edu      & marriott@cs.monash.edu.au \\
\ \\
Peter Stuckey                        & Yi Xiao  \\
Dept.\ of Computer Science           & Dept.\ of Mathematics \& Statistics \\
University of Melbourne              & University of Melbourne \\
Parkville, Victoria 3052, Australia  &  Parkville, Victoria 3052, Australia \\
pjs@cs.mu.oz.au                      &  yxiao@maths.mu.oz.au
\end{tabular}
\end{center}

\newpage

\section{Introduction}

Linear equality and inequality constraints arise naturally in specifying
many aspects of user interfaces, in particular layout and other geometric
relations.  Inequality constraints, in particular, are needed to express
relationships such as ``inside,'' ``above,'' ``below,'' ``left-of,''
``right-of,'' and ``overlaps.''  For example, if we are designing a
Web document we can express the
requirement that {\code figure1} be to the left of {\code figure2} as the
constraint \mbox{\code figure1.rightSide $\leq$ figure2.leftSide}.

It is important to be able to express preferences as well as requirements
in a constraint system.  One use is to express a desire for stability
when moving parts of an image: things should stay where they were unless
there is some reason for them to move.  A second use is to process
potentially invalid user inputs in a graceful way.  For example, if the
user tries to move a figure outside of its bounding window, it is
reasonable for the figure just to bump up against the side of the window
and stop, rather than given an error.  A
third use is to balance conflicting desires, for example in laying out a
graph.

Efficient techniques are available for solving such constraints if the
constraint network is acyclic.  However, in trying to apply constraint
solvers to real-world problems, we found that the collection of constraints
was often cyclic.  This sometimes arose when the programmer
added redundant constraints --- the cycles {\em could} have been avoided by
careful analysis.  However, this is an added burden on the programmer.
Further, it is clearly contrary to the spirit of the whole enterprise to
require programmers to be constantly on guard to avoid cycles and redundant
constraints; after all, one of the goals in providing constraints is to
allow programmers to state what relations they want to hold in a
declarative fashion, leaving it to the underlying system to enforce these
relations.  For other applications, such as complex layout problems with
conflicting goals,  cycles seem unavoidable.

\subsection{Constraint Hierarchies and Comparators}

Since we want to be able to express preferences as well as requirements in
the constraint system, we need a specification for how conflicting
preferences are to be traded off.  {\em Constraint hierarchies}
\cite{borning-lisp-symbolic-computation-92} provide a general theory for
this.  In a constraint hierarchy each constraint has a strength.  The
{\strength required} strength is special, in that {\strength required}
constraints must be satisfied.  The other strengths all label non-required
constraints.  A constraint of a given strength completely dominates any
constraint with a weaker strength.  In the theory, a {\em comparator} is
used to compare different possible solutions to the constraints and select
among them.

Within this framework a number of variations are possible.  One decision is
whether we only compare solutions on a constraint-by-constraint basis (a
{\em local} comparator), or whether we take some aggregate measure of the
unsatisfied constraints of a given strength (a {\em global} comparator).  A
second choice is whether we are concerned only whether a constraint is
satisfied or not (a {\em predicate} comparator), or whether we also want to
know how nearly satisfied it is (a {\em metric} comparator.  (Constraints
whose domain is a metric space, for example the reals, can have an
associated error function.  The error in satisfying a constraint {\em cn}
is 0 iff the constraint is satisfied, and becomes larger the less nearly
satisfied is the constraint.)

As recognized for the Indigo solver~\cite{borning-uist-96},
for inequality constraints it is important to use a metric rather than a
predicate comparator.  Thus, plausible comparators for use with linear
equality and inequality constraints are {\em locally-error-better}, 
{\em weighted-sum-better}, and {\em least-squares-better}.  
For a given
collection of constraints, Cassowary finds a locally-error-better or a
weighted-sum-better solution; QOCA finds a least-squares-better solution.
The least-squares-better comparator strongly
penalizes outlying values when trading off constraints of the same
strength.  It is particularly suited to tasks such as laying out a tree, a
graph, or a collection of windows, where there are inherently conflicting
preferences (for example, that all the nodes in the depiction of a graph
have some minimum spacing and that edge lengths be minimized).
Locally-error-better, on the other hand, is a more permissive
comparator, in that it admits more solutions to the constraints.  (In fact
any least-squares-better or weighted-sum-better solution is also a
locally-error-better solution \cite{borning-lisp-symbolic-computation-92}.)
It is thus easier to implement algorithms to find a locally-error-better
solution, and in particular to design hybrid algorithms that include
subsolvers for simultaneous equations and inequalities and also subsolvers
for nonnumeric constraints \cite{borning-cp-95}.
Since each of these different comparators is preferable in certain
situations we  give algorithms for each.

\subsection{Adapting the Simplex Algorithm}

Linear programming is concerned with solving the following problem.  Consider
a collection of $n$ real-valued variables $x_1, \ldots, x_n$, each
of which  is constrained to be non-negative: 
$x_i \geq 0$ for $1 \leq i \leq n$.  There are  $m$
linear equality or inequality constraints over the $x_i$, each of the form:

\hspace*{5mm}\mbox{$a_1 x_1 + \ldots + a_n x_n = b$},\\
\hspace*{5mm}\mbox{$a_1 x_1 + \ldots + a_n x_n \leq b$},  or\\
\hspace*{5mm}\mbox{$a_1 x_1 + \ldots + a_n x_n \geq b$}.

Given these constraints, we wish to find values for the $x_i$ that minimizes 
(or maximizes) the value of the {\em objective function} \\
\vspace*{2mm}
\hspace*{5mm}$c + d_1 x_1 + \ldots + d_n x_n$.

This problem has been heavily studied for the past 50 years.  The most
commonly used algorithm for solving it is the simplex algorithm, developed
by Dantzig in the 1940s, and there are now numerous variations of it. 
Unfortunately, however, existing implementations of the simplex
are not really suitable for UI applications.

The principal issue is incrementality.  For interactive graphical
applications, we need to solve similar problems repeatedly, rather than
solving a single problem once, and to achieve interactive response times,
very fast incremental algorithms are needed.  There are two cases.  First, when
moving an object with a mouse or other input device, we typically represent
this interaction as a one-way constraint relating the mouse position to the
desired $x$ and $y$ coordinates of a part of the figure.  For this case we
must re-satisfy the same collection of constraints, differing only in the
mouse location, each time the screen is refreshed.  Second, when editing an
object we may add or remove constraints and other parts, and we would like
to make these operations fast, by reusing as much of the previous solution
as possible.  The performance requirements are considerably more stringent
for the first case than for the second.  
\ignore{In Sections \ref{resolving} and
\ref{quadratic} we describe how to update an existing solution rapidly
given new inputs (e.g.\ a new mouse position), while in Sections
\ref{adding-constraints} and \ref{removing-constraints} we describe how to
add or delete a constraint incrementally.}

Another issue is defining a suitable objective function.  The objective
function in the standard simplex algorithm must be a linear expression; but
the objective functions for the locally-error-better,
weighted-sum-better, and least-squares-better comparators are all
non-linear.  Fortunately techniques have been developed in the operations
research community for handling these cases, which we adopt here.  For the
first two comparators, the objective functions are ``almost linear,''
while the third comparator gives rise to a quadratic
optimization problem.
\ignore{leading to the quasi-linear optimization technique described in Section
\ref{quasi-linear}.  Least-squares-better results in a quadratic
optimization problem, which is solved using the technique described in
Section \ref{quadratic}. } 

Finally, a third issue is accommodating variables that may take on both
positive and negative values, which in general is the case in UI
applications.  (The standard simplex algorithm requires all variables to be
non-negative.)  Here we adopt efficient techniques developed for
implementing constraint logic programming languages.

\subsection{Overview}

We present algorithms for incrementally solving linear equality and
inequality constraints for the three different comparators described
above. In Section~\ref{augmented-simplex-form} we give algorithms for
incrementally adding and deleting required constraints with restricted and
unrestricted variables from a system of constraints kept in {\em augmented
simplex form}, a type of solved form.  In Section \ref{resolving} we give
an algorithm, Cassowary, based on the dual simplex, for incrementally
solving hierarchies of constraints using the locally-error-better or
weighted-sum-better comparators when a constraint is added or an object is
moved, while in Section \ref{quadratic} we give an algorithm, QOCA, based
on the active set method, for incrementally solving hierarchies of
constraints using the least-squares-better comparator.

Both of our algorithms have been implemented, Cassowary in
Smalltalk, C++, and Java and QOCA in C++\@.  They perform surprisingly well, and a summary
of our results is given in Section \ref{empirical-evaluation}.  The QOCA
implementation is considerably more sophisticated 
than the current version of Cassowary.  However, QOCA is
inherently a more complex algorithm, and re-implementing it with a
comparable level of performance would be a daunting task.  In contrast,
Cassowary is straightforward, and a reimplementation based on this paper is
more reasonable, given a knowledge of the simplex algorithm.  The
various implementations with example applications are available from the 
authors.

\subsection{Related Work}

There is a long history of using constraints in user interfaces and
interactive systems, beginning with Ivan Sutherland's pioneering Sketchpad
system \cite{sutherland-ifips-63}.  Most of the current systems use one-way
constraints (e.g.\ \cite{hudson-subarctic-manual,myers-chi-96}), or local
propagation algorithms for acyclic collections of multi-way constraints
(e.g.\ \cite{sannella-spe-93,vander-zanden-toplas-96}).
Indigo \cite{borning-uist-96} handles acyclic collections of inequality
constraints, but not cycles (simultaneous equations and inequalities).  UI
systems that handle simultaneous linear equations include \mbox{DETAIL}
\cite{hosobe-cp-96} and Ultraviolet \cite{borning-cp-95}.  A number of
researchers (including the first author) have experimented with a
straightforward use of a simplex package in a UI constraint solver,
but the speed was not satisfactory for interactive use.
An earlier version of QOCA is described in references \cite{helm-gi-92} and
\cite{helm-eurographics-92}.  These earlier descriptions, however, do not
give any details of the algorithm, although the incremental deletion
algorithm is described in \cite{huynh-marriott-96}.  
The current implementation is
much improved, in particular through the use of the active set method
described in Section \ref{active-sets}.

Baraff \cite{baraff-siggraph-94} describes a quadratic optimization
algorithm for solving linear constraints that arise in modelling physical
systems.  Finally, much of the work on constraint solvers
has been in the logic programming and constraint logic programming
communities.  Current constraint logic programming languages such as
CLP($\cal R$) \cite{jaffar-toplas-92} include efficient solvers for linear
equalities and inequalities.  (See \cite{marriott-stuckey-book} for a
survey.)  However, these solvers use a refinement model of computation, in
which the values determined for variables are successively refined as the
computation progresses, but there is no notion as such of state and change.
As a result, these systems are not so well suited for building interactive
graphical applications.

\section{Incremental Simplex}
\label{inc-simplex}

\begin{quotation}
As you see, the subject of linear programming is surrounded by notational
and terminological thickets.  Both of these thorny defenses are lovingly
cultivated by a coterie of stern acolytes who have devoted themselves to
the field.  
Actually, the basic ideas of linear programming are quite simple. 
-- {\em Numerical Recipes}, \cite[page 424]{press-89}
\end{quotation}

We now describe an incremental version of the simplex algorithm, adapted to
the task at hand.
The material presented in this section is common to both
Cassowary and QOCA\@.  
The two algorithms use different optimization
techniques, however, which are described in Sections \ref{quasi-linear} and
\ref{quadratic} respectively.  
In the description we use a running example,
illustrated by the diagram in Figure~\ref{fig:pict}.

\begin{figure}[htb]
\begin{center}
\input constraints.eepic
\end{center}
\caption{Simple constrained picture\label{fig:pict}}
\end{figure}

The constraints on the variables in Figure~\ref{fig:pict} are as follows:
$x_m$ is constrained to be the midpoint of the line from $x_l$ to $x_r$,
and $x_l$ is constrained to be at least 10 to the left of $x_r$.  All
variables must lie in the range 0 to 100.  (To keep the presentation
manageable, we deal only with the $x$ coordinates.  Adding analogous
constraints on the $y$ coordinates would be simple but would double the
number of the constraints in our example.)  Since $x_l < x_m < x_r$ in any
solution, we simplify the problem by removing the redundant bounds
constraints.  However,
even with these simplifications the resulting constraints have a cyclic
constraint graph, and cannot be handled by methods such as Indigo.

We can represent this using the constraints
$$\begin{array}{rcl}
2 x_m &=& x_l + x_r \\
x_l + 10 &\leq &x_r \\
x_r &\leq& 100 \\
0 &\leq& x_l
\end{array}$$
Now suppose  we wish to minimize the distance between
$x_m$ and $x_l$ or in other words, minimize $x_m - x_l$.

\subsection{Augmented Simplex Form}
\label{augmented-simplex-form}

An optimization problem is in \emph{augmented simplex form} if constraint
$C$ has the form $C_U \wedge C_S \wedge C_I$ where $C_U$ and $C_S$ are
conjunctions of linear arithmetic equations and $C_I$ is $\bigwedge \{ x
\geq 0 \mid x \in vars(C_S)\}$ and the objective function $f$ is a linear
expression over variables in $C_S$\@.  The simplex algorithm does not itself
handle variables that may take negative values (so-called {\em
unrestricted variables}), and imposes a constraint $x \geq 0$ on all
variables occurring in its equations.  Augmented simplex form allows us to
handle unrestricted variables efficiently and simply; it was developed for
implementing constraint logic programming languages
\cite{marriott-stuckey-book}, and we have adopted it here.  Essentially it
uses {\em two} tableaux rather than one.  All of the unrestricted
variables will be placed in $C_U$, the unrestricted variable tableau.
$C_S$, the simplex tableau, contains only variables constrained to be
non-negative.  The simplex algorithm is used to determine an optimal
solution for the equations in the simplex tableau, ignoring the
unrestricted variable tableau for purposes of optimization.  The equations
in the unrestricted variable tableau are then used to determine values for
its variables.

{\bf Implementation Note.}  In the paper we describe $C_U$ and $C_S$ as
two separate tableaux.  In the implementation, however, it turns out to be
simpler to have just one tableau, since most operations are applied to both
$C_U$ and $C_S$.  Unrestricted and restricted variables are instances of
different classes, and in the code
we differentiate when necessary by sending the {\sf
isRestricted} message to the variable for each row.  See Section
\ref{cassowary-details}.

It is not difficult to write an arbitrary optimization problem over linear
real equations and inequalities into augmented simplex form.  The first
step is to convert inequalities to equations.  Each inequality of the form
$e \leq r$, where $e$ is a linear real expression and $r$ is a number, can be
replaced with $e + s = r \wedge s \geq 0$ where $s$ is a new non-negative
\emph{slack} variable.

For example, the constraints for Figure~\ref{fig:pict} can be written as
\begin{quote}\vspace*{-1ex}
minimize $x_m - x_l$ 
subject to 
$$\begin{array}{rcl}
2 x_m & = & x_l + x_r \\
x_l + 10 + s_1& = &x_r \\
x_r + s_2 &= &100 \\
0 &\leq & x_l, s_1, s_2
\end{array}$$
\end{quote}\vspace{-0.9ex}

We now separate the equalities into $C_U$ and $C_S$\@.
Initially all equations are in $C_S$\@.  We separate out the
unrestricted variables into $C_U$ using Gauss-Jordan elimination.  To do
this, we select an equation in $C_S$ containing an unrestricted variable
$u$ and remove the equation
from $C_S$\@.  We then solve the equation for $u$, yielding
a new equation $u=e$ for some expression $e$.  We then substitute $e$ for
all remaining occurrences of $u$ in $C_S$, $C_U$, and $f$,
and place the equation $u=e$ in
$C_U$\@.  The process is repeated until there are no more unrestricted
variables in $C_S$\@.  In our example the third equation can be used to
substitute $100 - s_2$ for $x_r$ 
obtaining
\begin{quote}\vspace*{-1ex}
minimize $x_m - x_l$ 
$$
\begin{array}{rcl}
x_r &= & 100 - s_2 \\ \hline
2 x_m & = & x_l + 100 - s_2 \\
x_l + 10 + s_1& = & 100 - s_2 \\
0 &\leq & x_l, s_1, s_2
\end{array}
$$
\end{quote}\vspace{-0.9ex}
Next, 
and the first equation can be used to 
substitute $50 + \frac{1}{2}x_l - \frac{1}{2} s_2$ 
for $x_m$, giving 
\begin{quote}\vspace*{-1ex}
minimize $50 - \frac{1}{2}x_l - \frac{1}{2} s_2$ 
subject to 
$$
\begin{array}{rcl}
x_m & = & 50 + \frac{1}{2} x_l - \frac{1}{2} s_2 \\ 
x_r &= & 100 - s_2 \\ \hline
x_l + 10 + s_1& = &100 - s_2 \\
0 &\leq & x_l, s_1, s_2
\end{array}
$$
\end{quote}\vspace{-0.9ex}
The tableau shows $C_U$ above the horizontal line,
and $C_S$ and $C_I$ below the horizontal line.  From now 
on $C_I$ will be omitted --- any variable occurring below the horizontal
line is implicitly constrained to be non-negative.
The simplex method works by taking an optimization problem in ``basic
feasible solved form'' (a type of normal form) and repeatedly applying
matrix operations to obtain new basic feasible solved forms.  Once we
have split the equations into $C_U$ and $C_S$ we can ignore $C_U$ for
purposes of optimization. 

{\bf One Detail.}  The example includes the constraint $x_l \geq 0$.  To
simplify the example, we just make $x_l$ be a restricted variable to
capture this constraint.  In the Cassowary implementation, however, all
variables that may be accessed from outside the solver as well as within it
are unrestricted.  Only error or slack variables are represented as
restricted variables, and these variables occur only within the solver.
See Section \ref{cassowary-details}.  The primary benefit of this is
that the programmer using the solver always uses just the one kind of
variable.  A minor benefit is that only the external, unrestricted
variables actually store their values as a field in the variable object;
the values of restricted variables are just given by the tableau.  A minor
drawback is that the constraint $v \geq 0$ must be represented explicitly.
(For any other constant $c \neq 0$, $v \geq c$ must be represented
explicitly in any event.)

{\bf Another Detail.}  The operations are shown as modifying $C_U$ as well
as $C_S$\@.  It would be possible to modify just $C_S$ and leave $C_U$
unchanged, using $C_U$ only to define values for the variables on the left
hand side of its equations.  This would speed up pivoting, but at least for
Cassowary it would make the incremental updates of the constants in edit
constraints slower; and since this is a much more frequent operation, in
the Cassowary implementation we do actually modify both $C_U$ and $C_S$\@.

An augmented simplex form optimization problem is in 
\emph{basic feasible solved form} if the equations are of the form 
$$x_0 = c + a_1 x_1 + \ldots + a_n x_n$$ where the variable $x_0$ does not
occur in any other equation or in the objective function.  If the equation
is in $C_S$, $c$ must be non-negative.  However, there is no
such restriction on the constants for the equations in $C_U$\@.  In either
case the variable $x_0$ is said to be \emph{basic} and the other
variables in the equation are \emph{parameters}.  A problem in basic
feasible solved form defines a \emph{basic feasible solution}, which is
obtained by setting each parametric variable to 0 and each basic variable
to the value of the constant in the right-hand side.

For instance, the following constraint
is in basic feasible solved form and is equivalent to the 
problem above.
\begin{quote}\vspace*{-1ex}
minimize $50 - \frac{1}{2} x_l + \frac{1}{2} s_2 $ 
subject to 
$$
\begin{array}{rlrrr} 
x_m & = &50 & + \frac{1}{2} x_l & - \frac{1}{2} s_2 \\
x_r & = &100 &  & - s_2 \\ \hline
s_1 & = &90 & - x_l &  - s_2 
\end{array}
$$
\end{quote}\vspace{-0.9ex}
The basic feasible solution corresponding to this
basic feasible solved form is 
$$\{x_m \mapsto 50, x_r \mapsto 100, s_1 \mapsto 90, x_l \mapsto 0, 
s_2 \mapsto 0\}.$$
The value of the objective function with this solution is 50.

\subsection{Simplex Optimization}
\label{simplex-optimization}

We now describe how to find an optimum solution to a constraint in basic
feasible solved form.  Except for the operations on the additional
unrestricted variable tableau $C_U$, the material presented in this
subsection is simply Phase II of the standard two-phase simplex algorithm.

The simplex algorithm finds the optimum by repeatedly looking for an
``adjacent'' basic feasible solved form whose basic feasible solution
decreases the value of the objective function.  When no such adjacent basic
feasible solved form can be found, the optimum has been found.  The
underlying operation is called {\em pivoting}, and involves exchanging a
basic and a parametric variable using matrix operations.  Thus by
``adjacent'' we mean the new basic feasible solved form can be reached by
performing a single pivot.

In our example, increasing $x_l$ from $0$ will decrease the value of the
objective function.  We must be careful as we cannot increase the value of
$x_l$ indefinitely as this may cause the value of some other
basic non-negative variable to become negative.  We must examine the
equations in $C_S$\@.  The equation $s_1 = 90 - x_l - s_2$ allows $x_l$ to
take at most a value of $90$, as if $x_l$ becomes larger than this, then
$s_1$ would become negative.  The equations above the horizontal line do not
restrict $x_l$, since whatever value $x_l$ takes the unrestricted variables
$x_m$ and $x_r$ can take a value to satisfy the equation.  In general,
we choose the most restrictive equation in $C_S$, and use it to eliminate
$x_l$.  In the case of ties we arbitrarily break the tie. In this example
the most restrictive equation is $s_1 = 90 - x_l - s_2$.  Writing $x_l$ as
the subject we obtain $x_l = 90 - s_1 - s_2$.  We replace $x_l$ everywhere
by $90 - s_1 - s_2$ and obtain
\begin{quote}\vspace*{-1ex}
minimize $5 + \frac{1}{2} s_1 + s_2$ 
subject to 
$$
\begin{array}{rlrrr} 
x_m & = &95 & - \frac{1}{2} s_1 & - s_2 \\
x_r & = &100 &  & - s_2 \\ \hline
x_l & = &90 & - s_1 & - s_2 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

We have just performed a pivot, having moved $s_1$ out of the set of basic
variables and replaced it by $x_l$.

We continue this process.  Increasing the value of $s_1$ will increase
the objective (that we are trying to minimize).  Note that decreasing $s_1$ will also decrease the
objective function value, but as $s_1$ is constrained to be non-negative,
it already takes its minimum value of $0$ in the associated basic feasible
solution.  Hence we are at an optimal solution.  

(If we were to have an unrestricted variable in the objective function, the
optimization would be unbounded.  This is not an issue for Cassowary or
QOCA, since the objective function in those cases always only contains
restricted variables, i.e.\ variables implicitly constrained to be
non-negative.)

In general, the simplex algorithm applied to $C_S$ 
is described as follows. 
We are given a problem in basic feasible solved form in which 
the variables $x_1, \ldots ,x_n$ are basic and the variables
$y_1, \ldots ,y_m$ are parameters.
\begin{quote}\vspace*{-1ex}
minimize $e + \sum_{j=1}^{m} d_j y_j$
subject to 
$$
\begin{array}{rcr}
        \bigwedge_{i=1}^{n} x_i & = & c_i + \sum_{j=1}^{m} a_{ij} y_j ~\wedge\\
                  \multicolumn{3}{c}{\bigwedge_{i=1}^{n} x_i \ge 0 \wedge
                  \bigwedge_{j=1}^{m} y_j \ge 0. }
            \end{array}
$$
\end{quote}\vspace{-0.9ex}
Select an entry variable $y_J$ such that $d_J < 0$.  (An entry variable is
one that will enter the basis, i.e.\ it is currently parametric and we want
to make it basic.)
Pivoting on such a variable can only decrease the value of the objective
function.
If no such variable exists, the optimum has been reached.
Now determine the exit variable $x_I$\@.  We must choose this variable so that
it maintains  basic feasible solved form by ensuring 
that the new $c_i$'s are still positive after pivoting. 
That is, we must choose an $x_I$ 
so that $- c_I/ a_{IJ}$ is a minimum element
of the set
$$
\{ -c_i/a_{iJ} \mid a_{iJ} < 0 \mbox{ and } 1 \le i \le n \}.
$$
If there were no $i$ for which $a_{iJ} < 0$ then we could stop since the
optimization problem would be unbounded, and so would not have
a minimum.  This is because we could choose $y_J$ to take an arbitrarily
large value, and so make the objective function arbitrarily small.
However, this is not an issue in our context since our
optimization problems will always have a lower bound of 0.

We proceed to choose $x_I$, and pivot $x_I$ out and replace it with $y_J$
to obtain the new basic feasible solution.
We continue this process until an optimum is reached.
The algorithm is specified in Figure~\ref{fig:simplex-opt},
and takes as inputs the simplex tableau $C_S$ and the objective function $f$.


\begin{figure}[tb]
\begin{center}
\fbox{
\begin{minipage}{10cm}
\begin{tabbing}
xx \= xx \= xx \= xx \= \kill
%Boolean
\textsf{simplex\_opt}($C_S$,$f$) \\
\> \textbf{repeat} \\
\> \> \% Choose variable $y_J$ to become basic\\
\> \> \textbf{if} for each $j \in \{ 1, \ldots ,m \}$ $d_j \ge 0$ \textbf{then} \\
\> \> \> \textbf{return} \% an optimal solution has been found\\
\> \> \textbf{endif} \\
\> \> \textbf{choose} $J \in \{ 1, \ldots ,m \}$ such that $d_J < 0$\\
\> \> \% Choose variable $x_I$ to become non-basic\\
\> \> \textbf{choose} $I \in \{ 1, \ldots ,n \}$ such that\\
\> \> \> $-c_I / a_{IJ} =
    \min_{i \in \{1,\ldots, n\}} \{ -c_i / a_{iJ} \mid a_{iJ} < 0 \}$\\
\> \> $e$ := $( x_I - c_I - \sum_{j=1, j\ne J}^m a_{Ij} y_j)/ a_{IJ}$\\
\> \> $C_S \left[ I \right]$ := $(Y_J = e)$\\
\> \> replace $Y_J$ by $e$ in $f$\\
\> \> \textbf{for} each $i \in \{1, \ldots ,n\}$\\
\> \> \> \textbf{if} $i \ne I$ \textbf{then}
       replace $Y_J$ by $e$ in $C_S \left[ I \right]$ \textbf{endif}\\
\> \> \textbf{endfor}\\
\> \textbf{endrepeat}
\end{tabbing}
\end{minipage}
}
\end{center}
\caption{Simplex optimization\label{fig:simplex-opt}}
\end{figure}


\ignore{ %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{quote}\vspace*{-1ex}
\textbf{Simplex algorithm} 
\\ {\sc input:} An  optimization problem $(C_S \wedge C_P,f)$ 
in basic feasible solved form.
\\ {\sc output:} Either  $false$ indicating that $(C_S \wedge C_P,f)$
does not have an optimal solution or else an optimal solution
to  $(C_S \wedge C_P,f)$.
\\ {\sc method:} 
Call , and let $\tuple{ F, C', f'}$ be the result.
If $F$ is $false$, output $false$, otherwise output the basic feasible
solution corresponding to $(C',f')$.
\end{quote}\vspace{-0.9ex}
} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Incrementality: Adding a Constraint}
\label{adding-constraints}

We now describe how to add the equation for a new constraint incrementally.
This technique is also used in our implementations to find an initial basic
feasible solved form for the original simplex problem, by starting from an
empty constraint set and adding the constraints one at a time.

As an example, suppose we wish to ensure that the midpoint
sits in the centre of the screen.  
This is represented by the constraint
$x_m = 50$.  If we substitute for each of the basic variables
(only $x_m$) in this constraint we obtain the equation
$45 - \frac{1}{2} s_1 - s_2 = 0$.  
In order to add this constraint
straightforwardly to the tableau we create a new 
non-negative variable $a$ called an \emph{artificial variable}.
(This is simply an incremental version of the operation used in
Phase I of the two-phase simplex algorithm.)
We let $a = 45 - \frac{1}{2} s_1 - s_2$ be added to the tableau
(clearly this gives a tableau in basic feasible solved form)
and then minimize the value of $a$.
If $a$ takes the value $0$
then we have obtained a solution to the problem
with the added constraint, and
we can then eliminate the artificial variable altogether since it is 
a parameter (and hence takes the value 0).  This is the case for our example;
the resulting tableau is
\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x_m & = &50   \\
x_r & = &100 & - s_2 \\ \hline
x_l & = &0 & + s_2 \\
s_1 & = &90 & -2 s_2
\end{array}
$$
\end{quote}\vspace{-0.9ex}

In general, to add a new required constraint to the tableau we first
convert it to an augmented simplex form equation by adding slack variables
if it is an inequality.  Next, we use the current tableau to substitute out
all the basic variables.  This gives an equation $e = c$ where $e$ is a
linear expression.  If $c$ is negative, we multiply both sides by $-1$ so
that the constant becomes non-negative.  If $e$ contains an unrestricted
variable we use it to substitute for that variable and add the equation to
the tableau above the line (i.e.\ to $C_U$).  Otherwise we create a
restricted artificial variable $a$ and add the equation $a = c - e$ to the
tableau below the line (i.e.\ to $C_S$), and minimize $c - e$. If the
resulting minimum is not zero then the constraints are unsatisfiable.
Otherwise $a$ is either parametric or basic.  If $a$ is parametric, the
column for it can be simply removed from the tableau.  If it is basic, the
row must have constant 0 (since we were able to achieve a value of 0 for
our objective function, which is equal to $a$).  If the row is just $a =
0$, it can be removed.  Otherwise, $a = 0 + b x + e$ where $b \neq 0$.  We
can then pivot $x$ into the basis using this row and remove the column for
$a$.

%%% ASKALAN Not just for efficiency-- it failed when we always used 
%%% an artificial variable
%{\bf Implementation Note.}  In some cases we can add an equation to the
%tableau without using an artificial variable, and for efficiency should do
%so when it is easy to detect that this can be done.  See Section
%\ref{cassowary-adding-constraints}.

\subsection{Incrementality: Removing a Constraint}
\label{removing-constraints}

We also want a method for incrementally removing a constraint from the
tableaux.  After a series of pivots have been performed, the information
represented by the constraint may not be contained in a single row, so we
need a way to identify the constraint's influence in the tableaux.  To do
this, we use a ``marker'' variable that is originally present only in the
equation representing the constraint.  We can then identify the
constraint's influence in the tableaux by looking for occurrences of that
marker variable.  For inequality constraints, the slack variable $s$ added
to make it an equality serves as the marker, since $s$ will originally
occur only in that equation.  The equation representing a nonrequired
equality constraint will have an error variable that can serve as a marker
--- see Section \ref{non-requireds}.  For required equality constraints, we
add a ``dummy'' restricted variable to the original equation to serve as a
marker, which we never allow to enter the basis (so that it always has
value 0).  In our running example, then, to allow the constraint $2 x_m =
x_l + x_r$ to be deleted incrementally we would add a dummy variable $d_3$,
resulting in $2 x_m = x_l + x_r + d_3$.  The simplex optimization routine
checks for these dummy variables in choosing an entry variable, and does
not allow one to be selected.  (We did not include this variable in the
tableaux presented earlier to keep things simpler.)

(Note: these dummy variables must be restricted, not unrestricted, since we
might need to have some of them in the equations for restricted basic
variables.)

Consider removing the constraint that $x_l$ is 10 to the left of $x_r$.
The slack variable $s_1$, which we added to the inequality to make it
an equation, records exactly how this equation has been used to modify the
tableau.  We can remove the inequality by pivoting the tableau until 
$s_1$ is basic and then simply drop the row in which it is basic.

In the tableau above $s_1$ is already basic, and so removing it simply means
dropping the row in which it is basic, obtaining
\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x_m & = &50   \\
x_r & = &100 & - s_2 \\ \hline
x_l & = &0 & + s_2 \\

\end{array}
$$
\end{quote}\vspace{-0.9ex}

If we wanted to remove this constraint
from the tableau before adding $x_m = 50$ (i.e. the final tableau given in
Section \ref{simplex-optimization}), $s_1$ is a parameter.
We make $s_1$ basic by treating it as an entry variable and
(as usual) determining the most restrictive equation and using that 
to pivot $s_1$ into the basis, and then remove the row.

There is such a restrictive equation in this example.  However, if no
equation restricts the size of the marker variable, that is, its
coefficients are all non-negative, then either the marker variable has a
positive coefficient in all equations, or it only occurs in equations for
unrestricted variables.  If it does occur in an equation for a restricted
variable, pick the equation that gives the smallest ratio.  (The row with
the marker variable will become infeasible, but all the other rows will
still be feasible, and we will be dropping the row with the marker
variable.  In effect we are removing the non-negativity restriction on the
marker variable.)  Finally, if it only occurs in equations for unrestricted
variables, we can choose any equation in which it occurs.

In the case above, the row $x_l  = 90  - s_1  - s_2$
is the most constraining equation.
Pivoting to let $s_1$ enter the
basis, and then removing the row in which it is basic, we obtain
\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x_m & = &50 & + \frac{1}{2} x_l & - \frac{1}{2} s_2 \\
x_r & = &100 &  & - s_2 \\ \hline
\end{array}
$$
\end{quote}\vspace{-0.9ex}

In the preceding example the marker variable had a negative coefficient.
Here is an example in which it just has positive coefficients.  
% (This is an example just for the tech report.)
The original constraints are:

\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x & \geq & 10   \\
x & \geq & 20   \\
x & \geq & 30 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

In basic feasible solved form this is:

\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x & = & 30 & + d_3   \\ \hline
s_1 & = & 20 & + d_3 \\
s_2 & = & 10 & + d_3 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

where $s_1$, $s_2$, and $d_3$ are the marker variables for 
$x \geq 10$, $x \geq 20$, and $x \geq 30$ respectively.

Suppose we want to remove the $x \geq 30$ constraint.  We need to pivot to
make $d_3$ basic.  The equation that gives the smallest ratio is 
$s_2  = 10  + d_3$, so the entry variable is $d_3$ and the exit variable is
$s_2$, giving:

\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x & = & 20 & + s_2   \\ \hline
s_1 & = & 10 & + s_2 \\
d_3 & = & -10 & + s_2 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

This is now infeasible, but we drop the row with $d_3$ giving 

\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x & = & 20 & + s_2   \\ \hline
s_1 & = & 10 & + s_2 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

which is of course feasible.

As another fine point, note that there is no problem with redundant
constraints.  Consider:

\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x & \geq & 10   \\
x & \geq & 10 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

When converted to basic feasible solved form, each $x \geq 10$ constraint
gets a separate slack variable, which is used as the marker variable for
that constraint.


\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr} 
x & = & 10 & + s_1   \\ \hline
s_2 & = & 0 & + s_1 
\end{array}
$$
\end{quote}\vspace{-0.9ex}

To delete the second $x \geq 10$ constraint we would simply drop
the $s_2 = 0 + s_1$ row.  To delete the first  $x \geq 10$ constraint we
would pivot, making $s_1$ basic and $s_2$ parametric:

\begin{quote}\vspace*{-1ex}
$$
\begin{array}{rlrrr}
x & = & 10 & + s_2   \\ \hline
s_1 & = & 0 & + s_2
\end{array}
$$
\end{quote}\vspace{-0.9ex}

and then drop the  $s_1 = 0 + s_2$ row.

A consequence of this is that if there are two redundant constraints, both
of them must be removed to eliminate their effect.  (This seems to be a
more desirable behaviour for the solver than removing redundant constraints
automatically, although if the latter were desired the solver could be
modified to do this.)  Another consequence is that when adding a new
constraint, we would never decide that it was redundant and not add it to
the tableau.  (If there were no dummy marker variables, we {\em would} do this
for redundant required equality constraints.)

\subsection{Handling Non-Required Constraints}
\label{non-requireds}

Suppose the user wishes to edit $x_m$ in the diagram and have $x_l$ and
$x_r$ weakly stay where they are.  This adds the non-required constraints
$x_m$ {\em edit}, $x_l$ {\em stay}, and $x_r$ {\em stay}.  Suppose further
that we are trying to move $x_m$ to position 50, and that $x_l$ and $x_r$ are
currently at 30 and 60 respectively.  We are thus imposing the constraints
{\strength strong} $x_m = 50$, {\strength weak} $x_l = 30$, and 
{\strength weak} $x_r = 60$.
There are two possible translations of these non-required constraints
to an objective function, depending on the comparator used.

For locally-error-better or weighted-sum-better, we can
simply add the errors of the each constraint to form an objective function.
Consider the constraint $x_m = 50$.  We define the error as $|x_m-50|$\@.  We
need to combine the errors for each non-required constraint with a weight
so we obtain the objective function 
$$s |x_m - 50| + w |x_l - 30| + w |x_r - 60|$$
where $s$ and $w$ are weights so that the strong constraint is always
strictly more important than solving any combination of weak constraints,
so that we find a locally-error-better or weighted-sum-better solution.
For the least-squares-better comparator the objective function is 
$$s (x_m - 50)^2 + w (x_l - 30)^2 + w (x_r - 60)^2.$$  
In the presentation, we will use $s = 1000$ and $w = 1$.

Cassowary actually uses symbolic weights and a lexicographic ordering,
which ensures that strong constraints are always satisfied in preference to
weak ones (see Section \ref{cassowary-details}).  However, QOCA 
does not employ symbolic weights.

Unfortunately neither of these objective functions is linear and hence the
simplex method is not applicable directly.  We now show how we can 
solve the problem using optimization algorithms
based on the two alternate objective functions: \emph{quasi-linear
optimization} and \emph{quadratic optimization}.

\section{Cassowary: Quasi-linear Optimization}
\label{quasi-linear}

Cassowary finds either locally-error-better or weighted-sum-better
solutions.  Since every weighted-sum-better solution is also a
locally-error-better solution \cite{borning-lisp-symbolic-computation-92},
the weighted-sum part of the optimization comes automatically from the
manner in which the objective function is constructed.

For Cassowary
both the edit and the stay constraints will be represented as equations of
the form  
$$v = \alpha + \delta_v^{+} - \delta_v^{-}$$
where $\delta_v^{+}$ and $\delta_v^{-}$ are non-negative variables
representing the
deviation of $v$ from the desired value $\alpha$.  If the constraint is
satisfied both $\delta_v^{+}$ and $\delta_v^{-}$ will be 0.  
Otherwise $\delta_v^{+}$ will be
positive and $\delta_v^{-}$ will be 0 if $v$ is too big, 
or vice versa if $v$ is
too small.  
Since we want $\delta_v^{+}$ and $\delta_v^{-}$ to be 0 if
possible, we make them part of the objective function, with larger
coefficients for the error variables for stronger constraints.
(We need to use the pair of variables to satisfy simplex's
non-negativity restriction, since these variables  $\delta_v^{+}$ and
$\delta_v^{-}$ will be part of the objective function.)  

Translating the constraints
{\strength strong} $x_m = 50$, 
{\strength weak} $x_l = 30$,
and {\strength weak} $x_r = 60$
which arise from the 
edit and stay constraints we obtain:
$$\begin{array}{rcl}
x_m & = &50 + \delta_{x_m}^+ -  \delta_{x_m}^- \\
x_l & = &30 + \delta_{x_l}^+ -  \delta_{x_l}^- \\
x_r &= &60 + \delta_{x_r}^+ -  \delta_{x_r}^- \\
0 &\leq& \delta_{x_m}^+, \delta_{x_m}^-, \delta_{x_l}^+, \delta_{x_l}^-, 
        \delta_{x_r}^+, \delta_{x_r}^-
\end{array}$$
The objective function to satisfy the
non-required constraints 
can now be restated as

\begin{quote}\vspace*{-1ex}
minimize $1000 \delta_{x_m}^+ + 1000  \delta_{x_m}^- + \delta_{x_l}^+ +  
        \delta_{x_l}^- + \delta_{x_r}^+ +  \delta_{x_r}^-$.
\end{quote}\vspace{-0.9ex}

An optimal solution of this problem can be found using the simplex algorithm,
and results in a tableau
\begin{trivlist}\item
minimize $10 + 1002 \delta_{x_m}^+ + 998 \delta_{x_m}^-  + 2
\delta_{x_l}^- + 2\delta_{x_r}^- $ 
subject to 
$$
\begin{array}{rlrrrrrr} 
x_m & = &50 &  + \delta_{x_m}^+ & - \delta_{x_m}^- \\
x_r & = &70 & + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                - \delta_{x_l}^+ & + \delta_{x_l}^- \\ \hline
x_l & = & 30  & & & + \delta_{x_l}^+ & - \delta_{x_l}^- \\
s_1 & = &30 &  + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                -2 \delta_{x_l}^+ & +2 \delta_{x_l}^- \\
s_2 & = &30 &   - 2 \delta_{x_m}^+ &+2\delta_{x_m}^- &
                + \delta_{x_l}^+ & - \delta_{x_l}^- \\
\delta_{x_r}^+ & = & 10 & + 2 \delta_{x_m}^+  & - 2\delta_{x_m}^- &
        - \delta_{x_l}^+ & + \delta_{x_l}^- & +\delta_{x_r}^-
\end{array}
$$
\end{trivlist}
This corresponds to the solution
$\{x_m \mapsto 50, x_l \mapsto 30, x_r \mapsto 70\}$
illustrated in Figure~\ref{fig:pict}.
Notice that the weak stay constraint on $x_r$ is not satisfied (read
directly from last line of the above tableau).

\subsection{Incrementality: Resolving the Optimization Problem}
\label{resolving}

Now suppose the user moves the mouse (which
is editing $x_m$) to $x=60$.
We wish to solve a new problem, with
constraints {\strength strong} $x_m = 60$, and
{\strength weak} $x_l = 30$ and {\strength weak} $x_r = 70$
(so that $x_l$ and $x_r$ should stay where they are if possible).

% WAS: 
% {\strength weak} $x_l = 10$ and {\strength weak} $x_r = 90$
% ... hope this is right

There are two steps.  First, we modify the tableau to reflect the new
constraints we wish to solve.  Second, we resolve the optimization problem
for this modified tableau.

Let us first examine how to modify the tableau to reflect the new values of
the stay constraints.  This will not require reoptimizing the tableau,
since we know that the new stay constraints are satisfied exactly.
Suppose the previous stay value for variable $v$ was $\alpha$, and in the
current solution $v$ takes value $\beta$.  The current tableau contains the
information that 
$$v = \alpha + \delta_v^+ - \delta_v^-$$
and we need to
modify this so that instead 
$$v = \beta + \delta_v^+ - \delta_v^-$$
There
are two cases to consider: (a) both $\delta_v^+$ and $\delta_v^-$ are
parameters, or (b) one of them is basic.

In case (a) $v$ must take the value $\alpha$
in the current solution since both $\delta_v^+$ and 
$\delta_v^-$ take the value $0$ and 
$$v = \alpha + \delta_v^+ - \delta_v^-$$
Hence $\beta = \alpha$ and no changes need to be made.

In case (b) assume without loss of generality that $\delta_v^+$ is
basic.  In the original equation representing the stay constraint, the
coefficient for $\delta_v^+$ is the negative of the coefficient for
$\delta_v^-$.  Since these variables occur in no other constraints, this
relation between the coefficients will continue to hold as we perform
pivots.  In other words, $\delta_v^+$ and $\delta_v^-$ come in pairs: any
equation that contains $\delta_v^+$ will also contain $\delta_v^-$ and vice
versa.  Since $\delta_v^+$ is assumed to be basic, it occurs exactly once
in an equation with constant $c$, and further this equation also contains
the only occurrence of $\delta_v^-$.  In the current solution 
$$\{v \mapsto \beta, \delta_v^+ \mapsto c, \delta_v^- \mapsto 0\}$$
and since the
equation 
$$v = \alpha + \delta_v^+ - \delta_v^-$$
holds, $\beta = \alpha + c$.  To replace the equation 
$$v = \alpha + \delta_v^+ - \delta_v^-$$
by  
$$v = \beta + \delta_v^+ - \delta_v^-$$
we simply need to replace the constant $c$
in the row for $\delta_v^+$ by $0$.  Since there are no other 
occurrences of $\delta_v^+$
and $\delta_v^-$ we have replaced the old equation with the new.

For our example, to update the tableau for the new values for the stay
constraints on $x_l$ and $x_r$ we simply set the constant of last equation
(the equation for $\delta_{x_r}^+$) to 0.

Now let us consider the edit constraints.  Suppose the previous edit value
for $v$ was $\alpha$, and the new edit value for $v$ is $\beta$.  The
current tableau contains the information that 
$$v = \alpha + \delta_v^+ - \delta_v^-$$
and again we need to modify this so that instead 
$$v = \beta + \delta_v^+ - \delta_v^-$$
To do so we must replace every occurrence of 
$$\delta_v^+ - \delta_v^-$$
by 
$$\beta - \alpha + \delta_v^+ - \delta_v^-$$
taking proper account of the coefficients of $\delta_v^+$ and $\delta_v^-$.
(Again, remember that $\delta_v^+$ and $\delta_v^-$ come in pairs.)

If either of $\delta_v^+$ and $\delta_v^-$ is basic, this simply involves
appropriately modifying the equation in which they are basic.  Otherwise, if
both are non-basic, then we need to change every equation of the form
$$
x_i = c_i + a'_v \delta_v^+ - a'_v \delta_v^- + e
$$
to
$$
x_i = c_i + a'_v (\beta - \alpha) + a'_v \delta_v^+ - a'_v \delta_v^- + e
$$
Hence modifying the tableau to reflect the new values of edit and stay
constraints involves only changing the constant values in some equations.
The modifications for stay constraints always result in a tableau in basic
feasible solved form, since it never makes a constant become negative.
In contrast the modifications for edit constraints may not.

To return to our example, suppose we pick up $x_m$ with the mouse and
move it to 60.  Then we have that $\alpha = 50$ and $\beta = 60$,
so we need to add 10 times the 
coefficient of $\delta_{x_m}^+$ to the constant part of every row.
The modified tableau, after the updates for both the stays and edits, is 
\begin{trivlist}\item
minimize $20 + 1002 \delta_{x_m}^+ + 998 \delta_{x_m}^-  + 2
\delta_{x_l}^- + 2\delta_{x_r}^- $ 
subject to 
$$
\begin{array}{rlrrrrrr} 
x_m & = &60 &  + \delta_{x_m}^+ & - \delta_{x_m}^- \\
x_r & = &90 & + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                - \delta_{x_l}^+ & + \delta_{x_l}^- \\ \hline
x_l & = & 30  & & & + \delta_{x_l}^+ & - \delta_{x_l}^- \\
s_1 & = &50 &  + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                -2 \delta_{x_l}^+ & +2 \delta_{x_l}^- \\
s_2 & = &10 &   - 2 \delta_{x_m}^+ &+2\delta_{x_m}^- &
                + \delta_{x_l}^+ & - \delta_{x_l}^- \\
\delta_{x_r}^+ & = & 20 & + 2 \delta_{x_m}^+  & - 2\delta_{x_m}^- &
        - \delta_{x_l}^+ & + \delta_{x_l}^- & +\delta_{x_r}^-
\end{array}
$$
\end{trivlist}
Clearly it is feasible and already in optimal form, and so we have
incrementally resolved the problem by simply modifying constants in the
tableaux. The new tableaux give the solution
$\{x_m \mapsto 60, x_l \mapsto 30, x_r \mapsto 90\}$.
So sliding the midpoint rightwards 
has caused the right point to slide rightwards as well, but twice as far.
The resulting diagram is shown at the top of Figure~\ref{fig:quasi}.

\begin{figure}[htb]
\begin{center}
\input quasi.eepic
\end{center}
\caption{Resolving the constraints\label{fig:quasi}}
\end{figure}

Suppose we now move $x_m$ from 60 to 90.  
The modified tableau is 
\begin{trivlist}\item
minimize $60 + 1002 \delta_{x_m}^+ + 998 \delta_{x_m}^-  + 2
\delta_{x_l}^- + 2\delta_{x_r}^- $ 
subject to 
$$
\begin{array}{rlrrrrrr} 
x_m & = &90 &  + \delta_{x_m}^+ & - \delta_{x_m}^- \\
x_r & = &150 & + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                - \delta_{x_l}^+ & + \delta_{x_l}^- \\ \hline
x_l & = & 30  & & & + \delta_{x_l}^+ & - \delta_{x_l}^- \\
s_1 & = &110 &  + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                -2 \delta_{x_l}^+ & +2 \delta_{x_l}^- \\
s_2 & = &-50 &   - 2 \delta_{x_m}^+ &+2\delta_{x_m}^- &
                + \delta_{x_l}^+ & - \delta_{x_l}^- \\
\delta_{x_r}^+ & = & 60 & + 2 \delta_{x_m}^+  & - 2\delta_{x_m}^- &
        - \delta_{x_l}^+ & + \delta_{x_l}^- & +\delta_{x_r}^-
\end{array}
$$
\end{trivlist}
The tableau is no longer in basic feasible solved form,
since the constant of the row 
for $s_2$ is negative, even though $s_2$ is supposed to be non-negative.

Thus, in general, after updating the constants for the edit constraints,
the simplex tableau $C_S$ may no longer be in basic feasible solved form,
since some of the constants may be negative.  However, the tableau is still
in basic form, so we can still read a solution directly from it as before.
And since no coefficient has changed, in particular in the optimization
function, the resulting tableau reflects an optimal but not feasible
solution.

We need to find a feasible and optimal solution.  We could do so by adding
artificial variables (as we did when adding a constraint), optimizing the
sum of the artificial variables to find an initial feasible solution, and
then reoptimizing the original problem.

But we can do much better.  The process of moving from an optimal and
infeasible solution to an optimal and feasible solution is exactly the dual
of normal simplex algorithm, where we progress from a feasible and
non-optimal solution to feasible and optimal solution.  Hence we can use
the \emph{dual simplex algorithm} to find a feasible solution while staying
optimal.

Solving the dual optimization problem starts from
an infeasible optimal tableau of the form
\begin{quote}\vspace*{-1ex}
minimize $e + \Sigma_{j=1}^m d_j y_j$ subject to
$$\begin{array}{rcl}
\bigwedge_{i=1}^{n} x_i = c_i + \Sigma_{j=i}^m a_{ij} y_j
\end{array}$$
\end{quote}\vspace{-0.9ex}
where some $c_i$ may be negative 
for rows with non-negative basic variables (infeasibility) 
and each $d_j$ is non-negative (optimality).

The dual simplex algorithm selects an exit variable
by finding a row $I$ with non-negative basic variable
$x_I$ and negative constant $c_I$\@.  
The entry variable is the variable $y_J$
such that
the ratio $d_J/a_{IJ}$ is the minimum of all $d_j/a_{Ij}$
where $a_{Ij}$ is positive. This ensures that when pivoting we stay at an
optimal solution.
The pivot replaces $y_j$ by
$$-1/a_{Ij} (-x_I + c_I + \Sigma_{j=1, j\neq J }^m a_{Ij} y_j)$$
and is performed as in the (primal) simplex algorithm.
The algorithm is shown in Figure~\ref{fig:simplex-dual}.

\begin{figure}[tb]
\begin{center}
\fbox{
\begin{minipage}{10cm}
\begin{tabbing}
xx \= xx \= xx \= xx \= \kill
%Boolean
\textsf{re\_opt}($C_S$,$f$) \\
\> \textbf{foreach} $stay:v \in C$ \\
\> \> \textbf{if} $\delta_v^+$ or $\delta_v^-$ is basic in row $i$ \textbf{then} $c_i$ :=
0 \textbf{endif} \\
\>  \textbf{endfor} \\
\> \textbf{foreach} $edit:v \in C$ \\
\> \> \textbf{let} $\alpha$ and $\beta$ be the previous and current edit
values for $v$\\
\> \> \textbf{let} $\delta_v^+$ be $y_j$ \\
\> \> \textbf{foreach} $i \in \{1, \ldots ,n\}$ \\
\> \> \>  $c_i$ := $c_i + a_{ij} (\beta - \alpha)$ \\
\> \> \textbf{endfor} \\
\> \textbf{endfor} \\
\> \textbf{repeat} \\
%\> \> \textbf{if} for each $i \in \{ 1, \ldots , n\}$ 
%       $c_i \ge 0$ or $x_I \geq 0 \not\in C_I$  \textbf{then} \\
\> \> \% Choose variable $x_I$ to become non-basic\\
\> \> \textbf{choose} $I$ where $c_I < 0$ \\
\> \> \textbf{if} there is no such $I$ \\
\> \> \> \textbf{return} $true$  \\
\> \> \textbf{endif} \\

\> \> \% Choose variable $y_J$ to become basic\\
\> \> \textbf{if} for each $j \in \{ 1, \ldots , m\}$ $a_{Ij} \leq 0$ \textbf{then} \\
\> \> \> \textbf{return} $false$\\
\> \> \textbf{endif} \\
\> \> \textbf{choose} $J \in \{1, \ldots , m\}$ such that\\
\> \> \> $d_J / a_{IJ} =
    \min_{j \in \{1, \ldots , m\}} \{ d_j / a_{Ij} \mid a_{Ij} > 0 \}$\\
\> \> $e$ := $( x_I - c_I - \sum_{j=1, j\ne J}^m a_{Ij} y_j)/ a_{IJ}$\\
\> \> replace $y_J$ by $e$ in $f$\\
\> \> \textbf{for} each $i \in \{1, \ldots ,n\}$\\
\> \> \> \textbf{if} $i \ne I$ \textbf{then}
       replace $y_J$ by $e$ in row $i$ \textbf{endif}\\
\> \> \textbf{endfor}\\
\> \> replace the $I^{th}$ row by $y_J = e$ \\
\> \textbf{until} $false$ \\
\end{tabbing}
\end{minipage}
}
\end{center}
\caption{Dual Simplex Re-optimization\label{fig:simplex-dual}}
\end{figure}

Continuing the example above
we select the exit variable $s_2$, 
the only non-negative basic variable for a row with negative constant.  
We find that $\delta_{x_l}^+$
has the minimum ratio since its coefficient in the optimization function
is 0, so it will be the entry variable.
Replacing $\delta_{x_l}^+$ everywhere by 
$50 + s_2 + 2 \delta_{x_m}^+ - 2 \delta_{x_m}^- + \delta_{x_l}^+$
we obtain the tableau

\begin{trivlist}\item
minimize $30060 + 1002 \delta_{x_m}^+ + 998 \delta_{x_m}^-  + 2
\delta_{x_l}^- + 2\delta_{x_r}^- $ 
subject to 
$$
\begin{array}{rlrrrrrr} 
x_m & = &90 & & + \delta_{x_m}^+ & - \delta_{x_m}^- \\
x_r & = &100 & - s_2 \\ \hline
x_l & = & 80 & + s_2 & + 2 \delta_{x_m}^+ & - 2 \delta_{x_m}^- \\
s_1 & = &110 & - 2 s_2 & + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- \\
\delta_{x_l}^+ & = & 50 & + s_2 & + 2 \delta_{x_m}^+ & - 2\delta_{x_m}^- &
                + \delta_{x_l}^- \\
\delta_{x_r}^+ & = & 40 & - s_2 &&&&  +\delta_{x_r}^-
\end{array}
$$
\end{trivlist}
The tableau is feasible (and of course still
optimal) and represents the solution
$\{x_m \mapsto 90, x_r \mapsto 100, x_l \mapsto 80\}$.
So by sliding the midpoint further right, the rightmost point hits the wall
and the left point slides right to satisfy the constraints.
The resulting diagram is shown at the bottom of Figure~\ref{fig:quasi}.

To summarize, incrementally finding a new solution for new input variables
involves updating the constants in the tableaux to reflect the updated stay
constraints, then updating the constants to reflect the updated edit
constraints, and finally reoptimizing if needed.  In an interactive
graphical application, when using the dual optimization method typically a
pivot is only required when one part first hits a barrier, or first moves
away from a barrier.  The intuition behind this is that
when a constraint
first becomes unsatisfied, the value of one of its error variables will
become non-zero, and hence the variable will have to enter the basis;
when a constraint first becomes satisfied,
we can move one of its error variables out of the basis.

In the example, pivoting occurred when the right point $x_r$ came up against a
barrier.  Thus, if we picked up the midpoint $x_m$ with the mouse and
smoothly slid it rightwards, 1 pixel every screen refresh, only one pivot
would be required in moving from 50 to 95.  This illustrates why the dual
optimization is well suited to this problem and leads to efficient
resolving of the hierarchical constraints.

\section{Cassowary Details}
\label{cassowary-details}

This section is only in the technical report (not the UIST paper) and
includes details on the Cassowary implementations.  (The current
implementations are in Smalltalk, C++, and Java.)  There is also a
subsection on some fine points regarding the comparator.

\subsection{Solver Protocol}
\label{solver-protocol}

The solver itself is represented as an instance of {\sf ClSimplexSolver}.
The public message protocol is as follows.

\begin{description}

\item[{\sf addConstraint(ClConstraint cn)}] \ \\
Incrementally add the linear constraint {\sf cn} to the tableau.  The
constraint object contains its strength.

\item[{\sf removeConstraint(ClConstraint cn)}] \ \\
Remove the constraint {\sf cn} from the tableau.  Also remove 
any error variables associated with {\sf cn} from the objective function.


\item[{\sf addEditVar(ClVariable v, ClStrength s)}] \ \\
      Add an edit constraint of strength {\sf s} on variable {\sf v} to
      the tableau so that {\sf suggestValue} (see below) can be used on
      that variable after a {\sf beginEdit()}.
      

\item[{\sf removeEditVar(ClVariable v)}] \ \\
      Remove the previously added edit constraint on variable {\sf v}.
      The {\sf endEdit} message automatically removes all the edit
      variables as part of terminating an edit manipulation.
      

\item[{\sf beginEdit()}] \ \\
      Prepare the tableau for new values to be given to the
      currently-edited variables.  The {\sf addEditVar} message should
      be used before calling {\sf beginEdit}, and {\sf suggestValue} and
      {\sf resolve} should be used only after {\sf beginEdit} has been
      invoked (but before the matching {\sf endEdit}.
      

\item[{\sf suggestValue(ClVariable v, double n)}] \ \\
      Specify a new desired value, {\sf n}, for the variable {\sf v}.
      Before this call, {\sf v} needs to have been added as a variable
      of an edit constraint (either by {\sf addConstraint} of a
      hand-built {\sf EditConstraint} object or more simply using {\sf
        addEditVar}).
      

\item[{\sf endEdit()}] \ \\
      Denote the end of an edit manipulation, thus removing all edit
      constraints from the tableau.  Each {\sf beginEdit} call must be
      matched with a corresponding {\sf endEdit} invocation.

\item[{\sf resolve()}] \ \\
Try to re-solve the tableau given the newly specified desired values.
Calls to resolve should be sandwiched between a {\sf beginEdit()} and a
{\sf endEdit()}, and should occur after new values for edit variables
are set using {\sf suggestValue}.

\item[{\sf addPointStays(Vector points)}] \ \\
This is kind of a kludge, and addresses the desire to satisfy the stays
on both the {\sf x} and {\sf y} components of a given point rather than
on the {\sf x} component of one point and the {\sf y} component of another.
{\sf points} is an array of points, whose {\sf x} and {\sf y} components
are constrainable variables.  Add a weak stay constraint to the {\sf x}
and {\sf y} variables of each point.  The weights for the {\sf x} and
{\sf y} components of a given point are the same.  However, the weights
for successive points are each smaller than those for the previous
point (1/2 of the previous weight).  The effect of this is to encourage the
solver to satisfy the stays on both the {\sf x} and {\sf y} of a given
point rather than the {\sf x} stay on one point and the {\sf y} stay on
another.  See Subsection \ref{comparator-details} for more on this issue.

\item[{\sf reset}] \ \\
Re-initialize the solver from the original constraints,
thus getting rid of any accumulated numerical problems.  (It is not clear
how often such problems arise, but here is the method anyway.)

\end{description}

\subsubsection{Possible Revisions to Solver Protocol}

One thing that might be worth changing is the way that stay constraints
are handled.  Currently, each variable that is to stay at an old value
needs an explicit stay constraint.  These stay constraints need to be
added before any other constraints, since otherwise the variable's value
is likely to be changed inappropriately to satisfy the other
constraints while initially building the tableau.

Instead, stay constraints could be implicit for each variable, and thus
be in effect before any other constraints.  See Subsection
\ref{constraint-revisions}.

\subsection{Principal Classes}

Here is a listing of the principal classes.  (In the current implementation
all the classes start with ``{\sf Cl}''.)  All of the classes are of course
direct or indirect sublclasses of {\sf Object}.

\begin{tabbing}
XXX\=XXX\=XXX\=XXX\=XXX\=XXX\=   \kill
\> {\sf Object} \\
\> \> {\sf ClAbstractVariable} \\
\> \> \> {\sf ClDummyVariable} \\
\> \> \> {\sf ClObjectiveVariable} \\
\> \> \> {\sf ClSlackVariable} \\
\> \> \> {\sf ClVariable} \\
\> \> {\sf ClConstraint} \\
\> \> \> {\sf ClEditOrStayConstraint} \\
\> \> \> \> {\sf ClEditConstraint} \\
\> \> \> \> {\sf ClStayConstraint} \\
\> \> \> {\sf ClLinearConstraint} \\
\> \> \> \> {\sf ClLinearEqualityConstraint} \\
\> \> \> \> {\sf ClLinearInequalityConstraint} \\
\> \> {\sf ClLinearExpression} \\
\> \> {\sf ClTableau} \\
\> \> \> {\sf ClSimplexSolver} \\
\> \> {\sf ClStrength} \\
\> \> {\sf ClSymbolicWeight}
\end{tabbing}

Following is a description of the classes.  Some of these classes make
use of the Dictionary Abstract Data Type: Dictionaries have keys and
values and permit efficiently finding the value
for a given key, and adding or deleting key/value pairs.  One can also
iterate through all keys, all values, or all key/value pairs.

The solver itself is represented as an instance of {\sf ClSimplexSolver},
with public message protocol as described above.  There is more on the
implementation of this class in Subsection 
\ref{cl-simplex-solver-implementation}.

\subsubsection{Variables}

{\sf ClAbstractVariable} and its subclasses represent various kinds of
constrained variables.  {\sf ClAbstractVariable} is an abstract class, that
is, it is just used as a superclass of other classes; one does't make
instances of {\sf ClAbstractVariable} itself.  {\sf ClAbstractVariable}
defines the message protocol for constrainable variables.  Its only
instance variable is {\sf name}, which is a string name for the variable.
(This is used for debugging and constraint understanding tasks.)

Instances of the concrete {\sf ClVariable} sub-class of {\sf
  ClAbstractVariable} are what the user of the solver sees (hence
it was given a nicer class name).  This class has an instance variable
{\sf value} that holds the value of this variable.  Users of the solver can
send one of these variables the message {\sf value} to get its value.

The other subclasses of {\sf ClAbstractVariable} are used only within the
solver.  They do not hold their own values --- rather, the value is just
given by the current tableau.  None of them have any additional instance
variables.

Instances of {\sf ClSlackVariable} are restricted to be non-negative.  They
are used as the slack variable when converting an inequality constraint to
an equation, and for the error variables to represent non-required constraints.

Instances of {\sf ClDummyVariable} is used as a marker variable to allow
required equality constraints to be deleted.  (For inequalities or
non-required constraints, the slack or error variable is used as the
marker.)  These dummy variables are never pivoted into the basis.

An instance of {\sf ClObjectiveVariable} is used to index the objective row
in the tableau.  (Conventionally this variable is named $z$.)  This kind of
variable is just for convenience --- the tableau is represented as a
dictionary (with some additional cross-references).  Each row is
represented as an entry in the dictionary; the key is a basic variable and
the value is an expression.  So an instance of {\sf ClObjectiveVariable} is
the key for the objective row.  The objective row is unique in that the
coefficients of its expression are {\sf ClSymbolicWeight}s not just
ordinary real numbers. (The current C++ and Java implementations convert {\sf
  ClSymbolicWeight}s to real numbers to avoid dealing with {\sf
  ClLinearExpression}s parameterized on the type of the coefficient.
See section \ref{symweights} for more details.)

All variables understand the following messages: {\sf isDummy}, {\sf
isExternal}, {\sf isPivotable}, and {\sf isRestricted}.  They also
understand messages to get and set the variable's name.

\begin{figure}[htb]
\begin{center}
\begin{tabular}{|l|c|c|c|c|} \hline
Class               & isDummy & isExternal & isPivotable & isRestricted \\\hline\hline
ClDummyVariable     & True    & False      & False       & True \\\hline
ClVariable          & False   & True       & False       & False \\\hline
ClSlackVariable     & False   & False      & True        & True \\\hline
ClObjectiveVariable & False   & False      & False       & False \\\hline
\end{tabular}
\end{center}
\caption{Subclasses of {\sf ClAbstractVariable}\label{fig:absVarSubclasses}}
\end{figure}

For {\sf isDummy}, instances of {\sf ClDummyVariable} return true and
everone else returns false.  The solver uses this message to test for dummy
variables.  It will not choose a dummy variable as the subject for a new
equation, unless all the variables in the equation are dummy variables.
(The solver also will not pivot on dummy variables, but this is handled by the
{\sf isPivotable} message.)

For {\sf isExternal}, instances of {\sf ClVariable} return true and
everyone else returns false.  If a variable responds true to this message,
it means that it is known outside the solver, and so the solver needs to
give it a value after solving is complete.

For {\sf isPivotable}, instances of {\sf ClSlackVariable} returns true
and everyone else returns false.  The solver uses this message to decide
whether it can pivot on a variable.

For {\sf isRestricted}, instances of {\sf ClSlackVariable} and
of {\sf ClDummyVariable} return true, and instances of
{\sf ClVariable} and {\sf ClObjectiveVariable} return false.  Returning
true means that this variable is restricted to being non-negative.


So variables do not hold state, except for a name for debugging, and a value
for instances of {\sf ClVariable} --- mostly their significance is just
their identity.  The only other messages that variables understand are some
messages to {\sf ClVariable} for creating constraints --- see Subsection
\ref{constraint-creation}.

\subsubsection{Linear Expressions}

Instances of the class {\sf ClLinearExpression} hold a linear expression,
and are used in building and representing
constraints, and in representing the tableau.  A linear expression holds a
dictionary of variables and coefficients (the keys are variables and the
values are the corresponding coefficients).  Only variables
with non-zero coefficients are included in the dictionary; 
if a variable is not in this
dictionary its coefficient is assumed to be zero.  The other instance
variable is a constant.  So to represent the linear expression 
$a_1 x_1 + \cdots + a_n x_n + c$, the dictionary would hold the key $x_1$
with value $a_1$, etc., and the constant $c$.  This representation was
convenient in Smalltalk given the built-in class dictionary, and allows one
to find the coefficient for a given variable without searching.  It has
some space overhead for the dictionary.  An alternative representation
would be to use a linked list for the coefficients and variables --- with
this representation one would need to search the list for a given variable,
but the representation would be more compact.  If expressions typically had
only a small number of non-zero coefficients this representation may be
preferable.

Linear expressions understand  a large number of messages.  Some of these
are for constraint creation (see Section \ref{constraint-creation}).  The
others are to substitute an expression for a variable in the constraint, to
add an expression, to find the coefficient for a variable, and so forth.

\subsubsection{Constraints}
\label{constraint-classes}

There is an abstract class {\sf ClConstraint} that serves as the superclass
for other concrete classes.  It defines two instance variables: {\sf
strength} and {\sf weight}.  The variable {\sf strength} is the strength of
this constraint in the constraint hierarchy (and should be an instance of
{\sf ClStrength}), while {\sf weight} is a float indicating the weight of
the constraint, or nil if it does not have a weight.  (Weights are only
relevant for the weighted-sum-better comparator, not for
locally-error-better.) 

Constraints understand various message that return true or false regarding
some aspect of the constraint, such as {\sf isRequired}, {\sf
isEditConstraint}, {\sf isStayConstraint}, and {\sf isInequality}.

{\sf ClLinearConstraint} is an abstract subclass of {\sf ClConstraint}.  It
adds an instance variable {\sf expression}, which holds an instance of
{\sf ClLinearExpression}.  It has two concrete subclasses.  An instance of
{\sf ClLinearEquation} represents the linear equality constraint  \\
\hspace*{1cm} {\sf expression = 0}. \\
An instance of {\sf  ClLinearInequality} represents
the constraint \\
\hspace*{1cm} {\sf expression} $\geq$ 0.

The other part of the hierarchy is for edit and stay constraints (both of
which are represented explicitly in the current implementation).  {\sf
ClEditOrStayConstraint} has an instance variable {\sf variable}, which is
the variable with the edit or stay.  Otherwise all they do is respond
appropriately to the messages {\sf isEditConstraint} and {\sf
isStayConstraint}. 

\subsubsection{Possible Revisions to Constraint Representation}
\label{constraint-revisions}

In contrast to the current Cassowary implementation described above, the
QOCA implementation does not represent edits or stays explicitly.  Rather,
each variable has a preferred value and a weight, in addition to a current
value.  Given the special treatment of edit constraints in Cassowary (and
since there are typically only one or two of them) it is probably worth
continuing to represent them explicitly.  However, since many variables
have a stay it may be worth representing stay constraints implicitly.  Then
every constrainable variable would automatically be given an implicit stay
constraint of a given strength, which could be a special null strength if
no stay were desired.  This strength would be stored as an instance
variable of {\sf ClVariable}.  This would only allow at most one stay per
variable, which is the typical situation --- if multiple stays were needed
for some reason this could be simulated using additional variables and
equality constraints.

This hierarchy is also intended to allow extension to include local
propagation constraints (which would be another subclass of {\sf
ClConstraint}) -- otherwise we could have made everything be a linear
constraint. 

\subsubsection{Constraint Creation}
\label{constraint-creation}

This subsection describes a mechanism to allow constraints to be defined
easily by programmers.  The convenience afforded by Cassowary varies
among languages.  Smalltalk's dynamic nature makes it the most
expressive.  C++'s operator overloading still permits using natural
infix notation.  Java, however, requires using regular methods, and
leaves us with the single option of prefix expressions when building
constraints.

In Smalltalk, the messages +, -, *, and / are defined for {\sf
  ClVariable} and {\sf ClLinearExpression} to allow convenient creation
of constraints by programmers.  Also, {\sf ClVariable} and {\sf
  ClLinearExpression}, as well as {\sf Number}, define {\sf cnEqual:},
{\sf cnGEQ:}, and {\sf cnLEQ:} to return linear equality or inequality
constraints.  Thus, the Smalltalk expression

\hspace*{1cm} {\sf 3*x+5 cnLEQ: y}

returns an instance of {\sf ClLinearEquality} representing the
constraint $3 x + 5 \leq y$.  This works as follows.  The number 3 gets
the message {\sf * x}.  Since {\sf x} is not a number, 3 sends the
message {\sf * 3} to {\sf x}.  {\sf x} is an instance of {\sf
  ClVariable}, which understands * to return a new linear expression
with a single term, namely itself times the argument.  (If the argument
is not a number it raises an exception that the expression is
non-linear.)  The linear expression representing $3x$ gets the message +
with the argument 5, and returns a new linear expression representing $3
x + 5$.  This linear expression gets the message {\sf cnLEQ:} with the
argument {\sf y}.  It computes a new linear expression representing $3 x
+ 5 - y$, and then returns an instance of {\sf ClLinearInequality} with
this expression.

(It is tempting to make this nicer by using the $=$, $<=$, and $>=$
messages, so that one could write

\hspace*{1 cm} {\sf 3*x+5 $<=$ y}

instead but since the rest of Smalltalk expects $=$, $<=$, and $>=$ to
perform a test and return a boolean, rather than to return a constraint,
this would not be a good idea.)

Similarly, in C++ the arithmetic operators are overloaded to build {\sf
  ClLinearExpression}s from {\sf ClVariable}s and other {\sf
  ClLinearExpression}s.  Actual constraints are built using various
constructors for {\sf ClLinearEquation} or {\sf ClLinearInequality}.  An
enumeration defines {\sf cnLEQ} and {\sf cnGEQ} to approximate the
Smalltalk interface.  For example:

\hspace*{1 cm}{\sf ClLinearInequality cn(3*x+5, cnLEQ,y); ~ ~ ~ // C++}

build the constraint {\sf cn} representing $3 x + 5 \leq y$.

In Java, the same constraint would be built like so:

\hspace*{1 cm}{\sf ClLinearInequality cn = new
  ClLinearInequality(CL.Plus(CL.Times(x,3),5), CL.LEQ, y);}

Though the Java implementation makes it difficult to express hard-coded
constraints, use of the implementation in conjunction with a
user-interface for specifying the constraints has shown that the
inconvenience is relatively unimportant.

\subsubsection{Symbolic Weights and Strengths}
\label{symweights}

The constraint hierarchy theory allows an arbitrary (although finite)
number of strengths of constraint.  In practice, however, programmers use a
small number of strengths in a stylized way.  The current implementation
therefore includes a small number of pre-defined strengths, and the maximum
number of strengths is defined as a constant.  (This constant can be
changed --- see below --- but we would not expect to do so frequently.)

The strengths are currently defined as follows.

\begin{description}

\item[{\sf required}]  Required constraints must be satisfied.  This
strength is used for most programmer-defined constraints.

\item[{\sf strong}] This strength is used for edit constraints.

\item[{\sf medium}] Currently unused.  

\item[{\sf weak}] This strength is used for stay constraints.

\end{description}

These are represented as four instances of {\sf ClStrength}.

The other relevant class is {\sf ClSymbolicWeight}.  As mentioned in
Section \ref{non-requireds}, the objective function is formed as the
weighted sum of the positive and negative errors for the non-required
constraints.  The weights should be such that the stronger constraints
totally dominate the weaker ones.  In general to pick a real number for the
weight we need to know how big the values of the variables can be.  To
avoid this problem altogether, rather than real numbers as weights we use
symbolic weights and a lexicographic ordering, which ensures that strong
constraints are always satisfied in preference to weak ones.  

Instances of {\sf ClSymbolicWeight} are used to represent these symbolic
weights.  These instances have an array of floating point numbers, whose
length is the number of non-required strengths (so 3 at the moment).  Each
element of the array represents the value at that strength, so $(1.0, 0.0,
10.0)$ represents a weight of 1.0 {\sf strong}, 0.0 {\sf medium}, and 10.0
{\sf weak}.  (In Smalltalk {\sf ClSymbolicWeight} is a variable length
subclass; we could have had an instance variable with an array of length 3
instead.)   Symbolic weights understand various arithmetic messages, as
follows (in C++, these are implemented using operator overloading):

\begin{description}

\item[{\sf + w}]  \ \\
{\sf w} is also a symbolic weight.
Return the result of adding {\sf self} to {\sf w}.  

\item[{\sf -- w}] \ \\
{\sf w} is also a symbolic weight.
Return the result of subtracting {\sf w} from {\sf self}

\item[{\sf * n}] \ \\
{\sf n} is a number.
Return the result of multiplying {\sf self} by {\sf n}.  

\item[{\sf / n}] \ \\
{\sf n} is a number.
Return the result of dividing {\sf self} by {\sf n}.  

\item[$<=$ {\sf  n}, ~ $>=$ {\sf n}, ~ $<$ {\sf n}, ~ $>$ {\sf n}, ~ $=$ {\sf n}] \ \\
{\sf w} is a symbolic weight.
Return true if {\sf self} is related to {\sf n} as the operator normally
queries.

\item[{\sf negative}] \ \\
Return true if this symbolic weight is negative
(i.e.\ it does not consist of all zeros and the first non-zero number is
negative). 

\end{description}

Instances of {\sf ClStrength} represent a strength in the constraint
hierarchy.  The instance variables are {\sf name} (for printing
purposes) and {\sf symbolicWeight}, which is the unit symbolic weight
for this strength.  Thus, with the 3 strengths as above, {\sf strong} is
$(1.0, 0.0, 0.0)$, {\sf medium} is $(0.0, 1.0, 0.0)$, and {\sf weak} is
$(0.0, 0.0, 1.0)$.

The above arithmetic messages let the Smalltalk implementation of the
solver use symbolic weights just like numbers in expressions.  This is
important because the objective row in the tableau has coefficients
which are {\sf ClSymbolicWeight}s but are subject to the same
manipulation as the other tableau rows whose expressions have
coefficients which are just real numbers.

In both C++ and Java, an additional message {\sf asDouble()} is understood
by {\sf ClSymbolicWeight}s.  This converts the representation to a real
number that approximates the total ordering suggested by the more
general vector of real numbers.  It is these real numbers that are used
as the coefficients in the objective row of the tableau (in lieu of {\sf
  ClSymbolicWeight}s, which the coefficients conceptually are).  This
avoids the obvious complexities such genericity introduces to the static
type systems of C++ and Java. (An improved C++ implementation using
templates is underway).

Also, since Java lacks operator overloading, the above operations are
invoked using suggestive alphabetic method names such as {\sf add}, {\sf
  subtract}, {\sf times}, {\sf lessThan}, etc.


\subsection{{\sf ClSimplexSolver} Implementation}
\label{cl-simplex-solver-implementation}

Here are the instance variables of {\sf ClSimplexSolver} (some fields
are inherited from {\sf ClTableau}, the base class of {\sf
  ClSimplexSolver} which provides the basic sparse-matrix interface ---
see section \ref{cl-tableau-implementation}).

\begin{description}

\item[{\sf rows}] \ \\
A dictionary with keys {\sf ClAbstractVariable} and
values {\sf ClLinearExpression}.  This holds the tableau.  Note that the
keys can be either restricted or unrestricted variables, i.e.\ both $C_U$
and $C_S$ are actually merged into one tableau.  This simplified the code
considerably, since many operations are applied to both restricted and
unrestricted rows.

\item[{\sf columns}] \ \\
A dictionary with keys {\sf ClAbstractVariable} and
values {\sf Set of ClAbstractVariable}.  These are the column
cross-indices.  Each parametric variable {\sf p} should be a key in this
dictionary.  The corresponding set should include exactly those basic
variables whose linear expression includes {\sf p}
({\sf p} will of course have a non-zero coefficient).  The keys can be 
either unrestricted or restricted variables.

\item[{\sf objective}] \ \\
Return an instance of {\sf ClObjectiveVariable}
(named {\sf z}) that is the key for the objective row in the tableau.

\item[{\sf infeasibleRows}] \ \\
Return a set of basic variables that have
infeasible rows.  (This is used when re-optimizing with the dual simplex
method.)  

\item[{\sf prevEditConstants}] \ \\
An array of constants (floats) for the edit
constraints on the previous iteration.  The elements in this array must be
in the same order as {\sf editPlusErrorVars} and {\sf editMinusErrorVars},
and the argument to the public {\sf resolve:} message.

\item[{\sf stayPlusErrorVars}] \ \\
An array of plus error variables (instances
of {\sf ClSlackVariable}) for the stay constraints.  The corresponding
negative error variable must have the same index in {\sf stayMinusErrorVars}.

\item[{\sf stayMinusErrorVars}] \ \\
See {\sf stayPlusErrorVars}.

\item[{\sf editPlusErrorVars}] \ \\
An array of plus error variables (instances
of {\sf ClSlackVariable}) for the edit constraints.  The corresponding
negative error variable must have the same index in {\sf editMinusErrorVars}.

\item[{\sf editMinusErrorVars}] \ \\
See {\sf editPlusErrorVars}.

\item[{\sf markerVars}] \ \\
A dictionary whose keys are constraints and whose
values are instances of a subclass of {\sf ClAbstractVariable}.  This
dictionary is used to find the marker variable for a constraint when
deleting that constraint.  A secondary use is that iterating through the
keys will give all of the original constraints (useful for {\sf reset}).

\item[{\sf errorVars}] \ \\
A dictionary whose keys are constraints and whose
values are arrays of {\sf ClSlackVariable}.  This dictionary gives the
error variable (or variables) for a given non-required constraint.  We need
this if the constraint is deleted, since the corresponding error variables
must be deleted from the objective function.

\item[{\sf slackCounter}] \ \\
Used for debugging.  An integer used to generate
names for slack variables, which are useful when printing out expressions.
(Thus we get slack variables named {\sf s1}, {\sf s2}, etc.)

\item[{\sf artificialCounter}] \ \\
Similar to {\sf slackCounter} but for
artificial variables.

\item[{\sf dummyCounter}]  \ \\
Similar to {\sf slackCounter} but for
dummy variables (ie.\ marker variables for required equality constraints).

\end{description}

\subsubsection{{\sf ClTableau} (Sparse Matrix) Operations }
\label{cl-tableau-implementation}

The basic requirements for the tableau representation are that one should
be able to perform the following operations efficiently:

\begin{itemize}

\item determine whether a variable is basic

\item determine whether a variable is parametric

\item find the corresponding expression for a basic variable

\item iterate through all the parametric variables with non-zero
coefficients in a given row

\item find all the rows that contain a given parametric variable with a
non-zero coefficient

\item add a row

\item remove a row

\item remove a parametric variable

\item substitute out a variable (i.e.\ replace all occurrences of a
variable with an expression, updating the tableau as appropriate).

\end{itemize}

The representation of the tableau as a dictionary of rows, with column
cross-indices, supports these operations.  Keeping the cross indices
up-to-date is a bit tricky, and so the solver actually accesses the rows
and columns only via the below interface of {\sf ClTableau}, to avoid
getting the two representations out of sync.

\begin{description}
  

\item[{\sf addRow(ClAbstractVariable var, ClLinearExpression expr)}]  \ \\
      Add the constraint {\sf var=expr} to the tableau.  {\sf var} will
      become a basic variable.  Update the column cross indices.
      

\item[{\sf noteAddedVariable(ClAbstractVariable var, ClAbstractVariable subject)}]  \ \\
      Variable {\sf var} has been added to the linear expression for
      {\sf subject}.  Update the column cross indices.
      

\item[{\sf noteRemovedVariable(ClAbstractVariable var, ClAbstractVariable subject)}]  \ \\
      Variable {\sf var} has been removed from the linear expression for
      {\sf subject}.  Update the column cross indices.
      

\item[{\sf removeColumn(ClAbstractVariable var)}] \ \\
      Remove the parametric variable {\sf var} from the tableau.  This
      involves removing the column cross index for {\sf var} and
      removing {\sf var} from every expression in {\sf rows} in which it
      occurs.
      

\item[{\sf removeRow(ClAbstractVariable var)}] \ \\
      Remove the basic variable {\sf var} from the tableau.  Since {\sf
        var} is basic, there should be a row {\sf var=expr}.  Remove
      this row, and also update the column cross indices.
      

\item[{\sf substituteOut(ClAbstractVariable var, ClLinearExpression expr)}] \ \\
      Replace all occurences of {\sf var} with {\sf expr} and update the
      column cross indices.

\end{description}

\subsubsection{Adding a Constraint}
\label{cassowary-adding-constraints}

Section \ref{adding-constraints} discussed how to add constraints
incrementally.  For efficiency we should avoid using an artificial
variable if possible.  We can avoid using an artificial variable if we can
choose a subject for the equation from among its current variables.
Here are the rules for choosing a subject.  (These are to be used after
replacing any basic variables with their defining expressions.)  

We start with an expression {\sf expr} (which is an instance of {\sf
ClLinearExpression}).  If necessary, normalize {\sf expr} by multiplying by
$-1$ so that its constant part is non-negative.  We are adding the
constraint {\sf expr=0} to the tableau.  To do this we want to pick a
variable in {\sf expr} to be the subject of an equation, so that we can add
the row {\sf var=expr2}, where {\sf expr2} is the result of solving {\sf
expr=0} for {\sf var}.

\begin{itemize}

\item If {\sf expr} contains any unrestricted variables, we must choose an
unrestricted variable as the subject.

\item If the subject is new to the solver, we will not have to do any
substitutions, so we prefer new variables to ones that are currently noted
as parametric.  

\item If {\sf expr} contains only restricted variables, if there is a
(restricted) variable in {\sf expr} that has a negative coefficient and
that is new to the solver, we can pick that variable as the subject.

\item Otherwise use an artificial variable.

\end{itemize}

A consequence of these rules is that we can always add a non-required
constraint to the tableau without using an artificial variable, since the
equation will contain a positive and a negative error 
or slack variable, both of
which are new to the solver, and which occur with opposite signs.  
(Constraints that are originally equations will have a positive and
a negative error variable, while constraints that are originally inequalities
will have one error variable and one slack variable, with opposite signs.)  This is
good because a common operation is adding a non-required edit.

\subsubsection{Removing a Constraint}

Here are a few additional remarks in addition to the material presented in
Section \ref{removing-constraints}.  

First, before we remove the constraint, there may be some stay constraints
that were unsatisfied previously --- if we just removed the constraint
these could come into play.  Instead, reset all of the stays so that all
variables are constrained to stay at their current values.

Also, if the constraint being removed is not required we need to remove the
error variables for it from the objective function.  To do this we add the
following to the expression for the objective function:

$$ -1 \times e \times s \times w $$

where $e$ is the error variable if it is parametric, or else $e$ is its
defining expression if it is basic, $s$ is the unit symbolic weight for the
constraint's strength, and $w$ is its weight.  ($s$ is an instance of 
{\sf ClSymbolicWeight} and $w$ is a float.)

If we allow non-required constraints other than stays and edits, we also
need to re-optimize after deleting a constraint, since a non-required
constraint might have become satisfiable (or more nearly satisfiable).

\subsection{Omissions}

The solver should implement Bland's anti-cycling rule, but it does not at the
moment.  Adding this should be straightforward.

%% ASKALAN Ref for Bland's anti-cycling rule

\subsection{Comparator Details}  
\label{comparator-details}  

Our implementation of Cassowary favors solutions that satisfies some of the
constraints completely, rather than ones that partially satisfy e.g.\ each
of two conflicting equalities.  These are still legitimate
locally-error-better solutions.  Cassowary's behaviour is analogous to that
of the simplex algorithm, which always finds solutions at a vertex of the
polytope even if all the solutions on an edge or face are equally good.
(And of course Cassowary behaves this way because simplex does.)

Such solutions are also produced by greedy constraint satisfaction
algorithms, such as local propagation algorithms like DeltaBlue and Indigo,
since these algorithms try to satisfy constraints one at a time, and in
effect the constraints considered first are given a stronger strength than
those considered later.

However, there is an issue regarding comparators and Cassowary, which has
not yet been resolved in an entirely clean way.  One of the public methods
for Cassowary is {\sf addPointStays: points}, as discussed in Subsection
\ref{solver-protocol}.  This method addresses the desire to satisfy the
stays on both the {\sf x} and {\sf y} components of a given point rather
than on the {\sf x} component of one point and the {\sf y} component of
another.

As an example of why this is useful, consider a line with endpoints 
{\sf p1} and {\sf p2} and a midpoint {\sf m}.  There are constraints 
{\sf (p1.x+p2.x)/2 = m.x} and {\sf (p1.y+p2.y)/2 = m.y}.  Suppose we are
editing {\sf m}.  It would look strange to satisfy the stay constraints on
{\sf p1.x} and  {\sf p2.y}, rather than both stays on {\sf p1} or both
stays on {\sf p2}.  (This claim has been verified
empirically --- in earlier implementations of Cassowary this
happened, and indeed it looked strange.)

The current implementation of {\sf addPointStays: points} uses different
weights for the stay constraints for successive elements of {\sf points},
which is a kludge but which seems to work well in practice.

We had some trouble coming up with an example where it would give a bad
answer --- here is a kind of contrived one.  Suppose we have a line with
endpoints {\sf p1} and {\sf p2} and a midpoint {\sf m}.  Suppose also we
have constraints {\sf p2.x = 2*p3.x} and {\sf p2.y = 2*p3.y}.  (This is a
bit strange since here we are using {\sf p3} as a distance from the origin
rather than as a location --- otherwise multiplying it by 2 is
problematic.)  If we give these points to {\sf addPointStays:} in the order
{\sf p1}, {\sf p2}, and {\sf p3}, then the stays on {\sf p1} will have
weight 1, those on {\sf p2} will have weight 0.5, and those on {\sf p3}
will have weight 0.25.  Then, a one legitimate WSB solution would satisfy
the stays on {\sf p1.x} and {\sf p1.y}, but another legitimate WSB solution
would satisfy the stays on {\sf p1.x}, {\sf p2.y}, and {\sf p3.y}.

%% ASKALAN: Better fix -- use another dimension of ClSymbolicWeight?

Here is a cleaner way to handle this situation.  We first introduce a new
comparator with the dubious name of {\em tilted-locally-error-better}.  The
set of TLEB solutions can be defined by taking a given hierarchy, forming
all possible hierarchies by breaking strength ties in all possible ways to
form a totally ordered set of constraints, and taking the union of the sets
of solutions to each of these totally ordered hierarchies.

For example, consider the two constraints {\strength weak} $x=0$ and
{\strength weak} $x=10$.  The set of LEB solutions is the infinite set
of mappings from $x$ to each number in $\left[ 0, 10 \right]$.  Assuming 
equal weights on the constraints, the (single)
least-squares solution is $\left\{ x \mapsto 5 \right\}$.  The TLEB 
solutions are defined by
producing all the totally ordered hierarchies and taking the union of their
solutions.  In this case the two possible total orderings are:

\hspace*{1cm} {\strength weak} $x=0$, {\strength slightly\_weaker} $x=10$ \\
\hspace*{1cm} {\strength slightly\_weaker} $x=0$, {\strength weak} $x=10$

These have solutions $\left\{ x \mapsto 0 \right\}$ and 
$\left\{ x \mapsto 10 \right\}$ respectively, so the set of TLEB 
solutions to the original hierarchy is 
$\left\{ \left\{ x \mapsto 0 \right\} , 
    \left\{ x \mapsto 10 \right\} \right\}$.  

As an aside, we hypothesize that the only psychologically plausible
solutions to the example are $\left\{ x \mapsto 0\right\}$, 
$\left\{ x \mapsto 5\right\} $, and $\left\{ x \mapsto 10\right\}$, 
but not e.g.\ $\left\{ x \mapsto 3.8\right\} $ --- although this hypothesis
has not been tested.  Another relevant question is whether users prefer any
of these solutions over others (for a given application domain).

Next, we introduce a notion of a \emph{compound constraint}, a conjunction
of primitive constraints, in this case linear equalities or inequalities.
For compound constraints, when we break the strength ties in defining the
set of tilted-locally-error-better solutions, we insist on mapping each
linear equality or inequality in a compound constraint to an adjacent
strength.  (We have been a bit imprecise in the use of the term
``constraint'' in this paper, sometimes using it to denote a primitive
constraint and sometimes to denote a conjunction of primitive constraints.
For the present definition, however, we need to distinguish compound
constraints that have been specificially identified as such by the user
from conjunctions of primitive constraints more generally, such as the
constraints $C_S$ and $C_U$ discussed in Section \ref{augmented-simplex-form}.)

Now, to define {\sf addPointStays:} in a more clean way, we could make each
point stay a compound constraint.  To illustrate why this works, consider
the midpoint example again.  We have two endpoints {\sf p1} and {\sf p2},
and a midpoint {\sf m}.  There are constraints {\sf (p1.x+p2.x)/2 = m.x}
and {\sf (p1.y+p2.y)/2 = m.y}, and we are editing {\sf m}.  Then the stays
on {\sf p1} and {\sf p2} will each be compound constraints:

\hspace*{1cm} {\strength weak} ($p1.x$ {\em stay} \& $p1.y$ {\em stay}) \\
\hspace*{1cm} {\strength weak} ($p2.x$ {\em stay} \& $p2.y$ {\em stay})

In defining the set of tilted-locally-error-better solutions, the total
orderings of these constraints that we will consider have the stays on
$p1.x$ and $p1.y$ both stronger than those on $p2.x$ and $p2.y$, or both
weaker.  This produces the desired result.

Note that it is not sufficient just to define a notion of ``compound
constraint'' without adding the notion of tilting --- otherwise if we were
using locally-error-better, we would just sum the errors of the 
primitive constraints, which would allow us to trade off the errors
arbitrarily and hence satisfy the stay on the $x$ component of one point
and the $y$ component of another.

Note also that all of this is not a problem for QOCA --- its
least-squares-better comparator distributes the error to the $x$ and $y$
components of all the points with stays of the same strength.

\section{QOCA: Quadratic Optimization}
\label{quadratic}

Another useful way of comparing solutions to constraint hierarchies is
least-squares-better, in which case we are interested in
solving optimization problems of the form, referred to as $QP$:
\begin{quote}
minimize $f$ subject to $C$  \\
where $f = \sum_{i=1}^n w_i (x_i - d_i)^2$
\end{quote}\vspace{-0.9ex}
The variables are $x_1, \ldots , x_n$, and $C$  is the set of
required constraints.  The desired value for variable
$x_i$ is $d_i$, and the ``weight'' associated with that desire (which
should reflect the hierarchy) is $w_i$.

This problem is a type of {\em quadratic programming} in which a quadratic
optimization function is minimized with respect to a set of linear
arithmetic equality and inequality constraints.  In particular, since the
optimization function is a sum of squares, the problem is an example
of {\em convex} quadratic programming, meaning that the local minimum is
also the global minimum.  This is fortunate, since convex quadratic
programming has been well-studied and efficient methods for solving these
problems are well-known in the operations research community.
Here we will present two methods.
The first is a variant of the simplex algorithm introduced earlier,
while the second, based on ``active sets,''
is the method of choice for medium scale problems consisting of up to
1000 variables and constraints.

\subsection{Linear Complementary Pivoting}
\label{linear-complementary-pivoting}

Arguably the simplest approach to solving convex quadratic problems
is a simple modification of the simplex algorithm
that finds the local optimum of a quadratic problem, which since
the problem is convex, is the global optimimum.

Now, a solution is a local minimum if in every direction either the
optimization value increases or the region becomes infeasible.
The information about infeasibility is captured by the constraints
in the original problem (called the {\em primal problem}).
Information about how the optimization function
decreases is captured in the so-called {\em dual  problem} which
is obtained by looking at the derivative of the optimization function.
The idea is therefore to combine the primal and dual problems
and solve these together. Any solution to their combination will
be a feasible optimal solution for the original problem.
The point about a quadratic problem is that the derivative
of a quadratic optimization function is linear. Thus both the
dual and the primal problem consist of linear arithmetic constraints
and so a variant of the simplex can be used to solve their conjunction.
Let the constraints in the primal problem be in basic feasible
solved form:
\[
PP: \qquad \bigwedge_{i=1}^n x_i = b_i - \sum_{j=1}^m a_{ij} y_j
\]
where $x_1,...,x_n$ are the basic variables and $y_1,...,y_m$ are
the parameters and let the function to be minimized be $O$ and
assume that basic variables have been eliminated from $O$.
Then the dual problem is
\[
DP: \qquad \bigwedge_{j=1}^m t_j =
\frac{ \partial O}{\partial y_j} +  \sum_{i=1}^n a_{ij} z_i
\]
where $z_1,...,z_n \ge 0$ are the dual variables
(one for each equation in the primal problem)
and $t_1,...,t_m \ge 0$ are the dual slack variables.

The combined problem $CP$ is the conjunction of the dual and
primal problem plus the constraints that for all $i$ and $j$,
$x_i \times z_i = 0$ and $y_j \times t_j = 0$.
Note that the last constraints mean that in the combined problem
every variable has a {\em complementary} variable which is not allowed
to be positive if it is.

For example, imagine that our primal problem is the constraints from before:
\[
\begin{array}{rlrrr}
x_m & = &50 & + \frac{1}{2} x_l & - \frac{1}{2} s_4 \\
s_1 & = &90 & - x_l &  - s_4 \\
s_2 & = &100 & - x_l \\
d_3 & = &95 & - \frac{1}{2} x_l & + \frac{1}{2} s_4 \\
x_r & = &100 &  & - s_4
\end{array}
\]
and that we have the weak stay
constraints  $x_l = 10$, $x_m= 50$ and $x_r = 90$.
The optimization problem is therefore to minimize
\[
(x_l - 10)^2 + (x_m - 50)^2 + (x_r - 90)^2.
\]
Eliminating basic variables from this function gives
\[
O = (x_l - 10)^2 + (\frac{1}{2} x_l  - \frac{1}{2} s_4)^2
 + (10 -s_4)^2.
\]
Now
\[
\frac{ \partial O}{\partial x_l} = 2 (x_l - 10) +
 (2 \times \frac{1}{2}) (\frac{1}{2} x_l  - \frac{1}{2} s_4)
= \frac{5}{2} x_l  - \frac{1}{2} s_4 - 20
\]
and
\[
\frac{ \partial O}{\partial s_4} =
 (2 \times - \frac{1}{2}) (\frac{1}{2} x_l  - \frac{1}{2} s_4)
 - 2 (10 -s_4)
= - \frac{1}{2} x_l  + \frac{5}{2} s_4 - 20.
\]
The dual problem is therefore
\[
\begin{array}{rlrrrrrrrr}
t_1 & = &-20 & + \frac{5}{2} x_l & - \frac{1}{2} s_4 &
        + \frac{1}{2} z_1 & - z_2 & - z_3 & - \frac{1}{2} z_4 & \\
t_2 & = &-20 & + - \frac{1}{2} x_l & + \frac{5}{2} s_4 &
        - \frac{1}{2} z_1 & - z_2 &  & + \frac{1}{2} z_4 & -z_5  \\
\end{array}
\]

Putting the primal and dual together we obtain the problem:
\[
\begin{array}{rlrrrrrrrr}
x_m & = &50 & + \frac{1}{2} x_l & - \frac{1}{2} s_4 \\
s_1 & = &90 & - x_l &  - s_4 \\
s_2 & = &100 & - x_l \\
d_3 & = &95 & - \frac{1}{2} x_l & + \frac{1}{2} s_4 \\
x_r & = &100 &  & - s_4\\
t_1 & = &-20 & + \frac{5}{2} x_l & - \frac{1}{2} s_4 &
        + \frac{1}{2} z_1 & - z_2 & - z_3 & - \frac{1}{2} z_4 & \\
t_2 & = &-20 & + - \frac{1}{2} x_l & + \frac{5}{2} s_4 &
        - \frac{1}{2} z_1 & - z_2 &  & + \frac{1}{2} z_4 & -z_5  \\
\end{array}
\]
where the variables in the problem and their complements are given by:
\[
\begin{array}{cc}
x_l \leftrightarrow t_1 & x_m \leftrightarrow z_1 \\
x_r \leftrightarrow z_5 & s_1 \leftrightarrow z_2 \\
s_2 \leftrightarrow z_3 & d_3 \leftrightarrow z_4 \\
s_4 \leftrightarrow t_2 &  \\
\end{array}
\]

A consequence of the way the dual is constructed is that a variable
and its complement cannot both be basic, meaning that the solution
read from the basic feasible
solved form automatically satisfies the complementary condition.

Unfortunately the complete problem
is not in basic feasible form. To transform it
into basic feasible form we add an artificial variable $v$
to every equation giving the equations:
\[
\begin{array}{rlrrrrrrrrr}
x_m & = &50 & + v & + \frac{1}{2} x_l & - \frac{1}{2} s_4 \\
s_1 & = &90  & + v & - x_l &  - s_4 \\
s_2 & = &100  & + v & - x_l \\
d_3 & = &95  & + v & - \frac{1}{2} x_l & + \frac{1}{2} s_4 \\
x_r & = &100  & + v &  & - s_4\\
t_1 & = &-20  & + v & + \frac{5}{2} x_l & - \frac{1}{2} s_4 &
        + \frac{1}{2} z_1 & - z_2 & - z_3 & - \frac{1}{2} z_4 & \\
t_2 & = &-20  & + v & - \frac{1}{2} x_l & + \frac{5}{2} s_4 &
        - \frac{1}{2} z_1 & - z_2 &  & + \frac{1}{2} z_4 & -z_5  \\
\end{array}
\]
We now pivot on the row with the largest negative constant. In this
case either the row with basic variable $t_1$ or that with $t_2$.
 Say we arbitrarily choose $t_2$
then we obtain:
\[
\begin{array}{rlrrrrrrrrr}
x_m & = & 70 & + t_2 &  + x_l & -3 s_4 &
        + \frac{1}{2} z_1 & + z_2 & &  -  \frac{1}{2} z_4 & + z_5 \\
s_1 & = & 110  & + t_2 & - \frac{1}{2} x_l &  + \frac{3}{2} s_4 &
        + \frac{1}{2} z_1 & + z_2 & &  -  \frac{1}{2} z_4 & + z_5 \\
s_2 & = &120  & + t_2 & - \frac{1}{2} x_l & - \frac{5}{2} s_4 &
        + \frac{1}{2} z_1 & + z_2 & &  -  \frac{1}{2} z_4 & + z_5 \\
d_3 & = & 115  & + t_2 &  & - 2 s_4 &
        + \frac{1}{2} z_1 & + z_2 & &  -  \frac{1}{2} z_4 & + z_5 \\
x_r & = &120  & + t_2 & + \frac{1}{2} x_l  & - \frac{7}{2}s_4 &
        + \frac{1}{2} z_1 & + z_2 & &  -  \frac{1}{2} z_4 & + z_5 \\
t_1 & = & 0  & + t_2 & + 3 x_l & -3 s_4 &
        + z_1 &  & - z_3 & - z_4 & + z_5\\
v & = &-20  & + t_2 & +  \frac{1}{2} x_l & - \frac{5}{2} s_4 &
        + \frac{1}{2} z_1 & + z_2 &  & -  \frac{1}{2} z_4 &  +z_5  \\
\end{array}
\]

We have now obtained a solved form which is basic feasible and which
satisfies the complementary conditions. The only problem is that the
artificial variable $v$ is in the basis. We continue pivoting until $v$
leaves the basis. At each pivot we choose to make basic the
variable which is complementary to the variable which just left the basis.
Thus in the next stage we would choose to move $s_4$ into the basis since
this is the complement of $t_2$. We use the standard simplex row selection
rule to determine which variable to move out of the basis. This ensures
that feasibility is maintained. Because we only move a variable into the basis
once its complementary variable has been moved out of the basis this means that
the solution corresponding to any of the solved forms will satisfy
the complementary conditions.

In analogy with the incremental simplex algorithm
we can modify this algorithm so that it is incremental for resolving.
Changing the desired variable values only changes the constants
in the solved form. There are two possibilities. If the solved form
remains feasible, then we just read the new solution directly from the
solved form. Otherwise, the solved form is now infeasible. In this
case we proceed as above, first introducing an artificial variable
$v$ and making the solved form feasible, then pivoting until $v$
leaves the basis.

We are currently exploring an implementation based on complementary pivoting.

\subsection{Active Set Method}
\label{active-sets}

Our current implementation of QOCA uses the {\em active set method}
\cite{fletcher-book} to solve the convex quadratic programming problem.
This method is an iterative technique for solving constrained optimization
problems with inequality constraints.  It is reasonably robust and quite
fast, and is the method of choice for medium scale problems consisting of
up to 1000 variables and constraints.

The key idea behind the algorithm is to solve a sequence of constrained
optimization problems $O_0$, ..., $O_t \ (t \geq 0)$.  
Each problem minimizes $f$ with
respect to a set of equality constraints, ${\cal A}$, called the {\em
active set}.  The active set consists of the original equality constraints
plus those inequality constraints that are ``tight,'' in other words, those
inequalities that are currently required to be satisfied as
equalities. The other inequalities are ignored for the moment.

Essentially, each optimization problem $O_i$ can be treated as an unconstrained
quadratic optimization problem, denoted by $U_i$. To obtain $U_i$, we
rewrite the equality constraints in $O_i$ in basic feasible solved
form, and then eliminate all basic variables in the objective function $f$.
The optimal solution is the point at which all of the partial derivatives of 
$f$ equal zero. The problem $U_i$ can be solved easily, since we are 
dealing with a convex quadratic function $f$ and so 
its derivatives are linear. 
As a result, to solve $U_i$ we need only solve a system of
linear equations over unrestricted variables. 

In more detail,
in the active set method, we assume at each stage
that a feasible initial guess 
$\vx_0 = (x_1, \cdots x_n)^T$ is available, as well as the corresponding
active set ${\cal A}$\@. 
Assume that we have just solved the optimization problem $O_0$, and let its 
solution be $\vx^\ast_0$. We  face the following two possibilities 
when determining the new approximate solution $\vx_1$. 
\bl{\arabic{bean}.}
\item $\vx^\ast_0$ is feasible with respect
 to the constraints in $O_0$ but it
violates some inequality constraints in $QP$ that are not in the current
active set ${\cal A}$. In this case, a scalar $\alpha \in [0,1]$ is 
selected, such that it is as large as possible and the point $\vx_0 + \alpha
(\vx^\ast_0 - \vx_0) $ is feasible. This point is taken as the new approximate
solution $\vx_1$, and the violated constraints are added to the active
set, giving rise to a new optimization problem $O_1$. 
\item $\vx^\ast_0$ is feasible with respect to the original problem $QP$\@.
It is directly taken as the new 
approximate solution $\vx_1$ and we test to see it is  also
optimal $QP$. This requires us to 
check if there exists a direction $\vs$ at $\vx_1$, such that a feasible
incremental step along $\vs$ reduces $f$. If such direction $\vs$ exists,
then one constraint is taken out of the active set ${\cal A}$ 
to generate the direction $\vs$, which results in a new 
optimization problem $O_1$. If no such direction exists we are finished
since $\vx_1$ is both feasible and optimal.
\el
%
If the active set is modified, the whole process is repeated until 
the optimal solution is reached.




Note that the active set method is closely related to the simplex method. Those
inequalities whose slack variables are not basic are in the active set,
while those whose slack variables are basic are not.  Pivoting corresponds
to moving one inequality out of the active set and replacing it by another.

%
%*** example ***
%

Consider our working example with the weak constraints that
$x_m=50$, $x_l=30$ and $x_r=70$\@. This gives rise to the
minimization problem $QP_1:$
\begin{quote}
minimize $f_1 = (x_m - 50)^2 + (x_l - 30)^2 
+ (x_r - 70)^2 $ subject to
$$
\begin{array}{rrrrcl}
(1)  & 2 x_m & -x_l & -x_r & = & 0 \\
(2)      &       & -x_l & +x_r & \geq & 10 \\
(3)      &       &      & -x_r & \geq & -100 \\
(4)      &       & x_l  &      & \geq & 0 
\end{array}
$$
\end{quote}
Although it is obvious that $x_m = 50, x_l = 30, x_r = 70 $
or $ \vx^\ast = (50,30,70)^T$ is the optimal solution, it is still
instructive to see how the active set method computes this. 
The initial guess and active set are read from the augmented simplex form
tableux. We start with
an initial guess $x_m = 50, x_l = 0, x_r = 100$,
i.e.\ $\vx_0=(50,0,100)^T$,
and constraints 1, 3 and 4 are active. Thus 
${\cal A}^{(1)}_0 = \{ 1,3,4 \}$ is the initial active set. The equality
constrained optimization problem $O^{(1)}_0$ is therefore
\begin{quote}
minimize $f_1$ subject to
$$
\begin{array}{rrrrrr}
          & 2x_m & -x_l & -x_r & = & 0 \\
          &      &      & -x_r & = & -100 \\
          &      & x_l  &      & = & 0
\end{array}
$$
\end{quote}
The problem $O^{(1)}_0$ has only one feasible solution $x_m = 50, x_l = 0,
x_r = 100$, so it is also the optimal solution, denoted by $\vx^\ast_0$\@.
Next we check if $\vx^\ast_0$ is the optimal solution to the problem
$QP_1$\@. Constraint 4 forces $x_l$ to take the value 0
in $\vx^\ast_0$\@. However, the value of the
objective function $f_1$ can be reduced if $x_l$ 
is increased. 
Thus the 4th constraint $\vx_l \geq 0 $ can be moved out of the active
set in order to further reduce the value of $f_1$\@. This gives
$\vx_1 = \vx^\ast_0$ as the new approximate
solution, ${\cal A}^{(1)}_1 = \{1,3\}$ as the active set and the optimization
problem $O^{(1)}_1$ as 
\begin{quote}
minimize $f_1$ subject to
$$\begin{array}{rrrrrr} 
          & 2x_m & -x_l & -x_r & = & 0 \\
          &      &      & -x_r & = & -100
\end{array}
$$
\end{quote}
To solve $O^{(1)}_1$, we rewrite the constraints in $O^{(1)}_1$ to a basic
feasible solved form $x_r = 100 \wedge x_l = 2x_m - 100$, and then eliminate 
basic variables in the function $f_1$\@. This results in the following 
unconstrained optimization problem 
\begin{quote}
minimize $(x_m - 50)^2 + (2x_m - 100 -30)^2 + (100 - 70)^2$
\end{quote}
Setting the derivative to be zero we obtain 
$$ 2(x_m -50) + 2 \times 2 (2x_m - 130) = 0.$$
Solving this together with the constraint in $O^{(1)}_1$, the optimal solution
of $O^{(1)}_1$ is found to be $\vx^\ast_1 = (62,24,100)^T$\@. It is
easy to verify that $\vx^\ast_1$ is still feasible. Similarly to the 
case for $\vx^\ast_0$, in $\vx^\ast_1$
$x_r$ is forced to take the value 100 because of the
3rd constraint, yet the function value $f_1$ can be
reduced if $x_r$ is decreased. 
So the 3rd constraint $-x_r \geq -100$ is moved out of the active
set. We now have the new approximate solution $\vx_2 = \vx^\ast_1$, 
the active set 
${\cal A}^{(1)}_2 = \{ 1 \}$ and the optimization problem $O^{(1)}_2$:

\mbox{minimize $f_1$ subject to $2 x_m - x_l - x_r = 0.$}

To solve this problem, we repeat the same procedure as for solving $O^{(1)}_1$\@.
%Rewrite the constraint as $x_l = 2x_m - x_r$ and substitute it into the 
%function $f_1$, then we have the unconstrained problem \\
%\begin{quote}
%\hspace*{0.5 cm} $ \min \ q = (x_m-50)^2 + (x_l-30)^2 + (2x_m-x_l-70)^2$ \\
%\end{quote}\vspace{-0.9ex}
The solution to this problem satisfies the equations:
\bel{Q1_2}
\begin{array}{lclcl}
2(x_m-50) &+& 2 \times 2(2x_m - x_l -70)&  = & 0  \\
2(x_l-30) &+& 2(2x_m - x_l -70) & = & 0  
\end{array}  
\eel
%which is obtained by setting all partial derivatives of $q$ to be zero.  
These together with the constraint in $O^{(1)}_2$ have the solution
$\vx^\ast= (50,30,70)^T$\@. This is the optimal solution to $O^{(1)}_2$ and
is also the optimal solution to the original problem $QP_1$\@. 


\begin{figure}[htb]
\begin{center}
\input qoca.eepic
\end{center}
\caption{Resolving the constraints using QOCA\label{fig:qoca}}
\end{figure}


Now imagine that we have started to manipulate the diagram.
We have the weak constraints that $x_l = 30$ and $x_r = 70$ and
the strong constraint that $x_m = 60$\@.
Reflecting this, we change the first term in the function $f_1$ to be
$1000(x_m-60)^2$, denote it as $f_2$ and the corresponding optimization
problem as $QP_2$\@. 
Starting from $\vx_0 = (50,30,70)^T$, which is the optimal solution
to $QP_1$, an equality constrained problem $O^{(2)}_0$ is formed. $O^{(2)}_0$
is the same as $O^{(1)}_2$, except that they have different objective 
functions. The solution to $O^{(2)}_0$ satisfies similar linear equations
to those of (\ref{Q1_2}). These can be obtained by replacing the term $2(x_m-50)$
in the first equation of (\ref{Q1_2}) by $1000 (x_m-60)$
reflecting the change in the objective function. A solved form 
for these equations is 
\bel{Q2_0}
\begin{array}{lclcl}
x_m & = & \frac{500}{501} \times 60 & + & \frac{50}{501} \\
x_l & = & x_m & - & 20 
\end{array}
\eel
which leads to the optimal solution for both $O^{(2)}_0$ and $QP_2$ 
as $x_m = 59.98, x_l = 39.98, x_r = 79.98 $\@. 

Note that the exact least-squares-better
solution is actually $x_m = 60, x_l = 40, x_r = 80$\@.  With
quadratic optimization the strong constraints do not completely dominate the
weak ones in the computed solution.  However, by choosing a suitably large
constant we found a solution that {\em is} least-squares-better to under
a one-pixel resolution, so that the deviation from a least-squares-better
solution would not be visible in an interactive system. 

In practice, this 
appears to work well. However, in principle it is possible to solve the
problem iteratively by computing the solution to a sequence of 
quadratic optimization problems in which the 
constants grow by an order of magnitude
and stopping when the solution is sufficiently close to the limit.
It might also be possible to solve the problem by using symbolic values,
however, this is more difficult than in the Cassowary algorithm because
of more complex manipulation.

To modify the active set method so that it is incremental for
resolving, we observe that 
changing the desired variable values only changes the
optimization function $f$\@. Thus we can reuse the active set from the last
resolve and reoptimize with respect to this.  In most cases the active set does
not change, and so we are done.  Otherwise we proceed as above.

For example, if we now move $x_m$ from 60 to 90,
we change the objective function again, but need only change the 
desired values and can keep the weights the same as they are in $f_2$,
e.g.\ in the new objective function $f_3$, the variable $x_m$ has 
a new desired value 90. The corresponding optimization problem
is referred to as $QP_3$\@. To solve this problem, the {\em resolve}
procedure makes use of the information from the
previous solve $QP_2$, while applying the active set method to $QP_3$\@. 
When resolving, it is important to notice that, if we start from 
the solution for the previous problem $QP_2$, i.e.\ $\vx_0 = (59.98,
39.98,79.98)^T$, then the solution to the corresponding equality
constrained problem $O^{(3)}_0$,
\begin{quote}
minimize $f_3 $ subject to $2x_m -x_l -x_r = 0$,
\end{quote}
can be easily obtained. In fact, one can just replace the desired
value 60 for $x_m$ in (\ref{Q2_0}) by its new desired value 90, which 
leads to the optimal solution to $O^{(0)}_3$ as 
$\vx^\ast_0 = (89.9202, 69.9202,109.9202)^T$\@. If the desired value 
does not change too much, it is quite likely that $\vx^\ast_0$ is also 
optimal for $QP_3$\@. Unfortunately, this is not the case for this example,
since $\vx^\ast_0$ violates the 3rd constraint $-x_r \geq -100$\@.
Choosing $\alpha \in [ 0, 1]$ to be as big as possible while still ensuring
that
$\vx_1 = \vx_0 + \alpha(\vx^\ast_0 - \vx_0)$ is feasible, 
we have $\alpha = 0.6687$
and $\vx_1 = \vx_0 + \alpha(\vx^\ast_0 - \vx_0)$ as the new approximate 
solution, at which the 3rd constraint becomes active. By
solving the corresponding
equality constrained problem $Q^{(3)}_1$,
\begin{quote}
minimize $f_3 $ subject to $2x_m - x_l -x_r = 0, 
\ -x_r = -100 $,
\end{quote}
the optimal solution to $QP_3$ is found to be 
$x_m = 89.9003$, $x_l = 79.8007$,
$x_r = 100$. 

Figure~\ref{fig:qoca} shows the effect of moving the horizontal line
with the least squares comparator. With this comparator the line
moves right maintaining the same length until it hits the right boundary,
at which point it starts to compress. This contrasts with the behaviour
of the locally-error-better comparator
in which the line grew until it bumped against the side.

The actual implementation of QOCA is rather more complex than this example 
suggests and the reader is referred to \cite{fletcher-book} for more details.

\section{Empirical Evaluation}
\label{empirical-evaluation}

Both algorithms have been implemented and tested.

Our algorithms for incremental addition and deletion of equality and inequality
constraints and for solving and resolving for the least-square comparator
using the QOCA algorithm have been implemented as part of the QOCA C++
constraint solving toolkit. 
The results are very satisfactory.
For a test problem with 300 constraints and
300 variables, adding a constraint takes
on average  $1.5$~msec, deleting a constraint $1.6$~msec, the initial
solve  $12$~msec, and subsequent resolving as the point moves $4.5$~msec.
For a larger problem with 900 constraints and variables,
adding a constraint takes
on average  $9.7$~msec, deleting a constraint $17$~msec, the initial
solve  $120$~msec, and subsequent resolving as the point moves $67$~msec.
These tests were run a sun4m sparc, running SunOS~5.4.

Cassowary has been implemented in Smalltalk, C++, and Java.
Running the Smalltalk implementation of Cassowary on the same problems, 
for the 300 constraint problem,
adding a constraint takes on average $38$~msec (including the initial
solve), deleting a constraint $46$~msec, and resolving as the point moves
$15$~msec.  (Stay and edit constraints are represented explicitly in this
implementation, so there were also stay constraints on each variable, plus
two edit constraints, for a total of 602 constraints.)
For the 900 constraint problem, adding a constraint takes on
average $98$~msec (again including the initial solve), deleting a
constraint $151$~msec, and resolving as the point moves $45$~msec.  These
tests were run using an implementation in OTI Smalltalk Version 4.0 running
on a IBM Thinkpad 760EL laptop computer.

For the C++ implementation of Cassowary on the problem with 900
constraints and variables, adding a constraint takes $40$~msec, deleting 
a constraint $8$~msec, and resolving as the point moves $8$~msec.  By
default for Cassowary the solver optimizes after each constraint
addition or removal, so there is no timing for the initial ``solve''
because it is implicit with each change to the tableau.  The Java
implementation under the basic Sun JDK 1.1 (no JIT compiler) is about
6 to 10 times slower than the C++ implementation.  These tests were run
on a Pentium 200 running Linux 2.0.29.

As these measurements are for implementations in different languages,
running on different machines, they should not be viewed as any kind of
head-to-head comparison.  Nevertheless, they indicate that both
algorithms are eminently practical for use with interactive graphical
applications.

The QOCA toolkit has been employed in a number of applications.
The first application is part of an intelligent pen and paper
interface that contains a parser to incrementally parse diagrams drawn by
the user using a stylus, and that has a diagram editor that respects
the semantics of the diagram by preserving the constraints recognized
in the parsing process.  QOCA is used for both error correction in
parsing and for diagram manipulation in the editor~\cite{chok-marriott95}.
A second QOCA application is for layout of trees and graphs in the
presence of arbitrary linear arithmetic constraints and with
suggested placements for some nodes~\cite{he-marriott96}.

The various implementations of Cassowary are also being used actively.
One author is currently embedding the C++ implementation in a X11 system
window manager based on a Scheme configuration language. A demonstration
Constraint Drawing Application using the Java implementation is
available from the authors.  A third Cassowary application currently
being developed using a different Java implementation is a web authoring
tool \cite{borning-multimedia-97}, in which the appearance of a page is
determined by the combination of constraints from both the web author
and the viewer.

\subsection*{Acknowledgments}

This project has been funded in part by the National Science Foundation
under Grants \mbox{IRI-9302249} and \mbox{CCR-9402551} and in part by Object
Technology International.  Alan Borning's visit to Monash University and
the University of Melbourne was sponsored in part by the
Australian-American Educational Foundation (Fulbright Commission).

Additionally, Greg Badros is supported by a National Science Foundation
Graduate Research Fellowship.  Parts of this material are based upon
work supported under that fellowship.  Any opinions, findings,
conclusions, or recommendations expressed in this publication are those
of the author, and do not necessarily reflect the views of the National
Science Foundation.

\bibliography{constraints}
\bibliographystyle{plain}

\end{document}
